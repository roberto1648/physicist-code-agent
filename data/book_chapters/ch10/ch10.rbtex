
<%
  require "../eruby_util.rb"
%>

<%
  chapter(
    '10',
    %q{Fields},
    'ch:efield',
    '',
    {'opener'=>''}
  )
%>

<% marg(0) %>
<%
  fig(
    'pulp',
    '',
    {
      'anonymous'=>true
    }
  )
%>
<% end_marg %>

\formatlikecaption{%
``Okay. Your duties are as follows: Get Breen. I don't care
how you get him, but get him soon. That faker! He posed for twenty
years as a scientist without ever being apprehended. Well, I'm going
to do some apprehending that'll make all previous apprehending look
like no apprehension at all. You with me?'' 

``Yes,'' said
Battle, very much confused. ``What's that thing you have?''

``Piggy-back heat-ray. You transpose the air in its path into an
unstable isotope which tends to carry all energy as heat. Then you
shoot your juice light, or whatever along the isotopic path and you
burn whatever's on the receiving end. You want a few?''

``No,'' said Battle. ``I have my gats. What else have you
got for offense and defense?'' Underbottam opened a cabinet and
proudly waved an arm. ``Everything,'' he said.

``Disintegraters, heat-rays, bombs of every type. And impenetrable
shields of energy, massive and portable. What more do I need?''

        From THE REVERSIBLE REVOLUTIONS by Cecil Corwin, Cosmic Stories, March
        1941. Art by Morey, Bok, Kyle, Hunt, Forte. Copyright expired.
        }
\normalsize\normalfont

<% begin_sec("Fields of force",0) %>
\index{force!fields of}\index{fields of force}

Cutting-edge science readily infiltrates popular culture,
though sometimes in garbled form. The Newtonian imagination
populated the universe mostly with that nice solid stuff
called matter, which was made of little hard balls called
atoms. In the early twentieth century, consumers of pulp
fiction and popularized science began to hear of a new image
of the universe, full of x-rays, N-rays, and Hertzian waves.
What they were beginning to soak up through their skins was
a drastic revision of Newton's concept of a universe made of
chunks of matter which happened to interact via forces. In
the newly emerging picture, the universe was \emph{made} of
force, or, to be more technically accurate, of ripples in
universal fields of force. Unlike the average reader of
Cosmic Stories in 1941, you now possess enough technical
background to understand what a ``force field'' really is.

<% begin_sec("Why fields?") %>

<% begin_sec("Time delays in forces exerted at a distance") %>

What convinced physicists that they needed this new concept
of a field of force? Although we have been dealing mostly
with electrical forces, let's start with a magnetic example.
(In fact the main reason I've delayed a detailed discussion
of magnetism for so long is that mathematical calculations
of magnetic effects are handled much more easily with the
concept of a field of force.) First a little background
leading up to our example. A bar magnet, \figref{baratoms}, has an axis
about which many of the electrons' orbits are oriented. The
earth itself is also a magnet, although not a bar-shaped
one. The interaction between the earth-magnet and the bar
magnet, \figref{barhang}, makes them want to line up their axes in
opposing directions (in other words such that their
electrons rotate in parallel planes, but with one set
rotating clockwise and the other counterclockwise as seen
looking along the axes). On a smaller scale, any two bar
magnets placed near each other will try to align themselves head-to-tail, \figref{barns}.

<% marg(94) %>

%

<%
  fig(
    'baratoms',
    %q{A bar magnet's atoms are (partially) aligned.}
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'barhang',
    %q{A bar magnet interacts with our magnetic planet.}
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'barns',
    %q{Magnets aligned north-south.}
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'barnn',
    %q{The second magnet is reversed.}
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'barsn',
    %q{Both magnets are reversed.}
  )
%>
<% end_marg %>

Now we get to the relevant example. It is clear that two
people separated by a paper-thin wall could use a pair of
bar magnets to signal to each other. Each person would feel
her own magnet trying to twist around in response to any
rotation performed by the other person's magnet. The
practical range of communication would be very short for
this setup, but a sensitive electrical apparatus could pick
up magnetic signals from much farther away. In fact, this is
not so different from what a radio does: the electrons
racing up and down the transmitting antenna create forces on
the electrons in the distant receiving antenna. (Both
magnetic and electric forces are involved in real radio
signals, but we don't need to worry about that yet.)

A question now naturally arises as to whether there is any
time delay in this kind of communication via magnetic (and
electric) forces. Newton would have thought not, since he
conceived of physics in terms of instantaneous action at a
distance. We now know, however, that there is such a time
delay. If you make a long-distance phone call that is routed
through a communications satellite, you should easily be
able to detect a delay of about half a second over the
signal's round trip of 50,000 miles. Modern measurements
have shown that electric, magnetic, and gravitational forces
all travel at the speed of light, $3\times10^8$  m/s. (In
fact, we will soon discuss how light itself is made of
electricity and magnetism.)

If it takes some time for forces to be transmitted through
space, then apparently there is some \emph{thing} that
travels \emph{through} space. The fact that the phenomenon
travels outward at the same speed in all directions strongly
evokes wave metaphors such as ripples on a pond.

<% end_sec() %>

<% begin_sec("More evidence that fields of force are real: they carry energy.") %>
\label{fieldenergy}

The smoking-gun argument for this strange notion of
traveling force ripples comes from the fact that they carry energy.

First suppose that the person holding the bar magnet on the
right decides to reverse hers, resulting in configuration
\figref{barnn}. She had to do mechanical work to twist it, and if she
releases the magnet, energy will be released as it flips
back to \figref{barns}. She has apparently stored energy by going from
\figref{barns} to \figref{barnn}. So far everything is easily explained without
the concept of a field of force.

But now imagine that the two people start in position \figref{barns}
and then simultaneously flip their magnets extremely quickly
to position \figref{barsn}, keeping them lined up with each other the
whole time. Imagine, for the sake of argument, that they can
do this so quickly that each magnet is reversed while the
force signal from the other is still in transit. (For a more
realistic example, we'd have to have two radio antennas, not
two magnets, but the magnets are easier to visualize.)
During the flipping, each magnet is still feeling the forces
arising from the way the other magnet \emph{used} to be
oriented. Even though the two magnets stay aligned during
the flip, the time delay causes each person to feel
resistance as she twists her magnet around. How can this be?
Both of them are apparently doing mechanical work, so they
must be storing magnetic energy somehow. But in the
traditional Newtonian conception of matter interacting via
instantaneous forces at a distance, interaction energy arises
from the relative positions of objects that are interacting
via forces. If the magnets never changed their orientations
relative to each other, how can any magnetic energy have been stored?

The only possible answer is that the energy must have gone
into the magnetic force ripples crisscrossing the space
between the magnets. Fields of force apparently carry energy
across space, which is strong evidence that they are real things.

This is perhaps not as radical an idea to us as it was to
our ancestors. We are used to the idea that a radio
transmitting antenna consumes a great deal of power, and
somehow spews it out into the universe. A person working
around such an antenna needs to be careful not to get too
close to it, since all that energy can easily cook flesh (a
painful phenomenon known as an ``RF burn'').

<% end_sec() %>

<% end_sec() %>

<% begin_sec("The gravitational field") %>
\index{gravitational field}\index{field!gravitational}

<% marg(50) %>
<%
  fig(
    'seaofarrows',
    %q{%
      The wind patterns in a certain area of the ocean could be
              charted in a ``sea of arrows'' representation like this. Each
              arrow represents both the wind's strength and its direction at a
              certain location.
    }
  )
%>
<% end_marg %>

Given that fields of force are real, how do we define,
measure, and calculate them? A fruitful metaphor will be the
wind patterns experienced by a sailing ship. Wherever the
ship goes, it will feel a certain amount of force from the
wind, and that force will be in a certain direction. The
weather is ever-changing, of course, but for now let's just
imagine steady wind patterns. Definitions in physics are
operational, i.e., they describe how to measure the thing
being defined. The ship's captain can measure the wind's
``field of force'' by going to the location of interest and
determining both the direction of the wind and the strength
with which it is blowing. Charting all these measurements on
a map leads to a depiction of the field of wind force like
the one shown in the figure. This is known as the ``sea of
arrows'' method of visualizing a field.

Now let's see how these concepts are applied to the
fundamental force fields of the universe. We'll start with
the gravitational field, which is the easiest to understand.
As with the wind patterns, we'll start by imagining gravity
as a static field, even though the existence of the tides
proves that there are continual changes in the gravity field
in our region of space. 
When the gravitational field was introduced in chapter
\ref{ch:2}, I avoided discussing its direction explicitly, but
defining it is easy enough: we simply go to the
location of interest and measure the direction of the
gravitational force on an object, such as a weight tied to
the end of a string.\index{gravitational field}\index{field!gravitational}

In chapter \ref{ch:2}, I defined the gravitational field in
terms of the energy required to raise a unit mass through a unit distance.
However, I'm going to give a different definition now, using an
approach that will be more easily adapted to electric and magnetic fields. 
This approach is based on force rather than energy.
We couldn't carry out the energy-based definition without dividing by
the mass of the object involved, and the same is true for the force-based definition.
For example, gravitational forces are weaker on the moon than on
the earth, but we cannot specify the strength of gravity
simply by giving a certain number of newtons. The number of
newtons of gravitational force depends not just on the
strength of the local gravitational field but also on the
mass of the object on which we're testing gravity, our
``test mass.'' A boulder on the moon feels a stronger
gravitational force than a pebble on the earth. We can get
around this problem by defining the strength of the
gravitational field as the force acting on an object,
\emph{divided by the object's mass:}

The gravitational field vector, $\vc{g}$, at any location in space is
found by placing a test mass $m_t$ at that point. The field vector
is then given by $\vc{g}=\vc{F}/m_t$, where $\vc{F}$ is the gravitational
force on the test mass.

We now have three ways of representing a gravitational field. The
 magnitude of the gravitational field near the surface of
the earth, for instance, could be written as 9.8 N/kg, 9.8 $\zu{J}/\zu{kg}\cdot\zu{m}$,
or 9.8 $\zu{m}/\zu{s}^2$. If we already had two names for it, why invent
a third? The main reason is that
it prepares us with the right approach for defining other fields.

The most subtle point about all this is that the gravitational
field tells us about what forces \emph{would} be exerted on
a test mass by the earth, sun, moon, and the rest of the
universe, \emph{if} we inserted a test mass at the point in
question. The field still exists at all the places where
we didn't measure it.

\begin{eg}{Gravitational field of the earth}\label{eg:gravfieldofsphere}
\egquestion
What is the magnitude of the earth's gravitational
field, in terms of its mass, $M$, and the distance
$r$ from its center?

\eganswer
        Substituting $|\vc{F}|= GMm_{t}/ r^2$ into the definition of the
gravitational field, we find $|\vc{g}|= GM/ r^2$. This expression
could be used for the field of any spherically symmetric mass distribution, since
the equation we assumed for the gravitational force would apply in any such
case.

\end{eg}

<% begin_sec("Sources and sinks") %>
\index{sinks in fields}\index{sources of fields}

<% marg(48) %>

%

<%
  fig(
    'sink',
    %q{%
      The gravitational field surrounding a clump of mass such as
              the earth.
    }
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'earthmoonfield',
    %q{%
      The gravitational fields of the earth and moon superpose.
              Note how the fields cancel at one point, and how there is no boundary
              between the interpenetrating fields surrounding the two bodies.
    }
  )
%>
<% end_marg %>

If we make a \index{sea-of-arrows representation}sea-of-arrows
picture of the gravitational fields surrounding the earth,
\figref{sink}, the result is evocative of water going down a drain.
For this reason, anything that creates an inward-pointing
field around itself is called a sink. The earth is a
gravitational sink. The term ``source'' can refer specifically
to things that make outward fields, or it can be used as a
more general term for both ``outies'' and ``innies.''
However confusing the terminology, we know that gravitational
fields are only attractive, so we will never find a region
of space with an outward-pointing field pattern.

Knowledge of the field is interchangeable with knowledge of
its sources (at least in the case of a static, unchanging
field). If aliens saw the earth's gravitational field
pattern they could immediately infer the existence of the
planet, and conversely if they knew the mass of the earth
they could predict its influence on the surrounding
gravitational field.

<% end_sec() %>

<% begin_sec("Superposition of fields") %>
\index{superposition of fields}\index{fields!superposition of}

A very important fact about all fields of force is that when
there is more than one source (or sink), the fields add
according to the rules of vector addition. The gravitational
field certainly will have this property, since it is defined
in terms of the force on a test mass, and forces add like
vectors. Superposition is an important characteristic of
waves, so the superposition property of fields is consistent
with the idea that disturbances can propagate outward
as waves in a field.

\pagebreak[4]

\begin{eg}{Reduction in gravity on Io due to Jupiter's gravity}
\egquestion
The average gravitational field on Jupiter's moon
Io is 1.81 N/kg. By how much is this reduced when Jupiter is
directly overhead? Io's orbit has a radius of $ 4.22\times10^8$ m,
and Jupiter's mass is $ 1.899\times10^{27}$  kg.

\eganswer
                By the shell theorem, we can treat the Jupiter as if its
mass was all concentrated at its center, and likewise for
Io. If we visit Io and land at the point where Jupiter is
overhead, we are on the same line as these two centers, so
the whole problem can be treated one-dimensionally, and
vector addition is just like scalar addition. Let's use
positive numbers for downward fields (toward the center of
Io) and negative for upward ones. Plugging the appropriate
data into the expression  derived in example \ref{eg:gravfieldofsphere},
we find that the Jupiter's contribution to the field is
$- 0.71$ N/kg. Superposition says that we can find the actual
gravitational field by adding up the fields created by Io
and Jupiter: $1.81-0.71$ N/kg = 1.1 N/kg.  
        You might think that this reduction would create some
spectacular effects, and make Io an exciting tourist
destination. Actually you would not detect any difference if
you flew from one side of Io to the other. This is because
your body and Io both experience Jupiter's gravity, so you
follow the same orbital curve through the space around Jupiter.                

\end{eg}

%

<%
  fig(
    'ligo',
    %q{%
      The part of the LIGO gravity wave detector at 
      Hanford Nuclear Reservation, near Richland, Washington. The other half of the
      detector is in Louisiana.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true,
      'sidepos'=>'b'
    }
  )
%>

<% end_sec() %>

<% begin_sec("Gravitational waves") %>

\index{waves!gravitational}\index{gravitational waves}

A source that sits still will create a static field pattern,
like a steel ball sitting peacefully on a sheet of rubber. A
moving source will create a spreading wave pattern in the
field, like a bug thrashing on the surface of a pond.
Although we have started with the gravitational field as the
simplest example of a static field, stars and planets do
more stately gliding than thrashing, so gravitational waves
are not easy to detect. Newton's theory of gravity does not
describe gravitational waves, but they are predicted by
Einstein's general theory of relativity.

A Caltech-MIT collaboration has built
        a pair of gravitational wave detectors
        called LIGO\index{LIGO} to search for direct evidence of gravitational
        waves. Since they are essentially the most sensitive
        vibration detectors ever made, they are located in quiet
        rural areas, and signals are compared between them to
        make sure that  they were not due to passing trucks. 
        The signature of a gravitational wave is if the same wiggle is seen in both detectors within
        a short time. The detectors are able to
        sense a vibration that causes a change of $10^{-18}$ m in the distance between
        the mirrors at the ends of the 4-km vacuum tunnels. This is a thousand times
        less than the size of an atomic nucleus!
        In 2016, the collaboration announced the first detection of a gravitational wave, which is believed
        to have originated from the collision of two black holes. Propagation of gravitational waves
        at $c$ was verified through multiple methods both by study of the 2016 event and through
        an event in 2017, interpreted as a collision of two neutron stars, in which both gravitational
        waves and electromagnetic waves were detected simultaneously.

<% end_sec() %>

<% end_sec() %>

<% begin_sec("The electric field",3) %>
\index{electric field}\index{field!electric}

<% begin_sec("Definition") %>

The definition of the electric field is directly analogous
to, and has the same motivation as, the definition of the
gravitational field:

The electric field vector, $\vc{E}$, at any location in space is
found by placing a test charge $q_t$ at that point. The electric
field vector is then given by $\vc{E}=\vc{F}/q_t$, where
$\vc{F}$ is the electric force on the test charge.                

Charges are what create electric fields. Unlike gravity,
which is always attractive, electricity displays both
attraction and repulsion. A positive charge is a source of
electric fields, and a negative one is a sink.

The most difficult point about the definition of the
electric field is that the force on a negative charge is in
the opposite direction compared to the field. This follows
from the definition, since dividing a vector by a negative
number reverses its direction. It's as though we had some
objects that fell upward instead of down.

<% self_check('pointchargefield',<<-'SELF_CHECK'
Find an equation for the magnitude of the field of a
        single point charge $Q$.
  SELF_CHECK
  ) %>

\begin{eg}{Superposition of electric fields }\label{eg:square}
\egquestion
Charges $q$ and $- q$ are at a distance $b$ from
each other, as shown in the figure. What is the electric
field at the point P, which lies at a third corner of the square?

\eganswer
        The field at P is the vector sum of the fields
that would have been created by the two charges independently.
Let positive $x$ be to the right and let positive $y$ be up.

Negative charges have fields that point at them, so the
charge $-q$ makes a field that points to the right, i.e., has
a positive $x$ component. Using the answer to the self-check, we have
\begin{align*}
                 E_{-q,x}         &=    \frac{ kq}{ b^2}  \\
                 E_{-q,y}         &=    0\eqquad.  
\end{align*}
Note that if we had blindly ignored the absolute value signs
and plugged in $- q$ to the equation, we would have
incorrectly concluded that the field went to the left.

                By the Pythagorean theorem, the positive charge is at a
distance $\sqrt{2} b$ from P, so the magnitude of its contribution to
the field is  $E=  kq/2 b^2$. Positive charges have
fields that point away from them, so the field vector is at
an angle of 135\degunit counterclockwise from the $x$ axis.  \\
\begin{align*}
                 E_{q,x}         &=    \frac{ kq}{2 b^2} \zu{cos}\  135\degunit  \\
                         &=     -\frac{ kq}{2^\zu{3/2} b^2}   \\
                 E_{q,y}         &=    \frac{ kq}{2 b^2} \zu{sin}\  135\degunit  \\
                         &=      \frac{ kq}{2^\zu{3/2} b^2}
\end{align*}

The total field is
\begin{align*}
                 E_\zu{x}         &=   \left(1-2^{-\zu{3/2}}\right)\frac{ kq}{ b^2}   \\
                 E_{y}         &=   \frac{ kq}{2^\zu{3/2} b^2}
\end{align*}
\end{eg}

<% end_sec() %>

<% begin_sec("Dipoles") %>
\index{dipole!electric}\index{electric dipole}

<% marg(180) %>

%

<%
  fig(
    'eg-square',
    %q{Example \ref{eg:square}.}
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'dipolefield',
    %q{%
      A dipole field. Electric fields diverge from a positive
              charge and converge on a negative charge.
    }
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'watermolecule',
    %q{A water molecule is a dipole.}
  )
%>
<% end_marg %>

The simplest set of sources that can occur with electricity
but not with gravity is the \index{dipole!electric}\emph{dipole},
consisting of a positive charge and a negative charge
with equal magnitudes. More generally, an electric dipole can
be any object with an imbalance of positive charge on one
side and negative on the other. A water molecule, \figref{watermolecule}, is a
dipole because the electrons tend to shift away from the
hydrogen atoms and onto the oxygen atom.

        Your microwave oven acts on water molecules with electric
        fields. Let us imagine what happens if we start with a
        uniform electric field, \figref{dipoleinfield}/1, made by some external charges,
        and then insert a dipole, \figref{dipoleinfield}/2, consisting of two charges
        connected by a rigid rod. The dipole disturbs the field
        pattern, but more important for our present purposes is that
        it experiences a torque. In this example, the positive
        charge feels an upward force, but the negative charge is
        pulled down. The result is that the dipole wants to align
        itself with the field, \figref{dipoleinfield}/3. The microwave oven heats food
        with electrical (and magnetic) waves. The alternation of the
        torque causes the molecules to wiggle and increase the
        amount of random motion. The slightly vague definition of a
        dipole given above can be improved by saying that a dipole
        is any object that experiences a torque in an electric field.
%

<%
  fig(
    'dipoleinfield',
    %q{%
      1. A uniform electric field created by some charges 
               ``off-stage.'' 2. A dipole is placed in the field. 3. The dipole aligns with the field.
    },
    {
      'width'=>'wide',
      'floatpos'=>'t'
    }
  )
%>

What determines the torque on a dipole placed in an
externally created field? Torque depends on the force, the
distance from the axis at which the force is applied, and
the angle between the force and the line from the axis to
the point of application. Let a dipole consisting of charges
$+q$ and $-q$ separated by a distance $\ell$ be placed in an
external field of magnitude $|\vc{E}|$, at an angle $\theta$ with
respect to the field. The total torque on the dipole is
\begin{align*}
                \tau          &=  \frac{\ell}{2}q|\vc{E}|\sin \theta+\frac{\ell}{2}q|\vc{E}|\sin \theta    \\
                         &=     \ell q|\vc{E}|\sin \theta\eqquad.  
\end{align*}
(Note that even though the two forces are in opposite
directions, the torques do not cancel, because they are both
trying to twist the dipole in the same direction.) The
quantity is called the dipole \index{moment!dipole}\index{dipole
moment}moment, notated $D$. (More complex dipoles can also
be assigned a dipole moment --- they are defined as having
the same dipole moment as the two-charge dipole that would
experience the same torque.)

Employing a little more mathematical elegance, we can define
a dipole moment \emph{vector},
\begin{equation*}
        \vc{D} = \sum q_i \vc{r}_i\eqquad,
\end{equation*}
where $\vc{r}_i$ is the position vector of the charge labeled by the
index $i$. We can then write the torque in terms of a vector
cross product (page \pageref{vectorcrossproductdef}),
\begin{equation*}
        \btau = \vc{D}\times\vc{E}\eqquad.
\end{equation*}

No matter how we notate it, the definition of the dipole moment requires
that we choose a point from which we measure all the position vectors
of the charges. However, in the commonly encountered special case where
 the total charge of the object is zero, 
the dipole moment is the same regardless of this choice.

\begin{eg}{Dipole moment of a molecule of NaCl gas}
\egquestion
In a molecule of NaCl gas, the center-to-center
distance between the two atoms is about 0.24 nm. Assuming
that the chlorine completely steals one of the sodium's
electrons, compute the magnitude of this molecule's dipole moment.

        \eganswer
        The total charge is zero, so it doesn't matter where we choose the
        origin of our coordinate system. For convenience, let's choose
        it to be at one of the atoms, so that the charge on that atom
        doesn't contribute to the dipole moment.
         The magnitude of the dipole moment is then
        \begin{align*}
                         D         &=    (2.4\times10^{-10}\ \zu{m})( e)  \\
                                 &=    (2.4\times10^{-10}\ \zu{m})( 1.6\times10^{-19}\ \zu{C})  \\
                                 &\approx    4\times10^{-29}\ \zu{C}\cdot\zu{m}
        \end{align*}
        The experimentally measured value is $3.0\times10^{-29}\ \zu{C}\cdot\zu{m}$, which shows
        that the electron is not completely ``stolen.''
% McCaffrey, https://www.era.lib.ed.ac.uk/bitstream/1842/2601/2/McCaffrey%20PD%20thesis%2007.pdf
% someone else's measurement, quoted on p. 167 of pdf
% 8.97 Debye units; 1 D=10^-18 statC.cm
% calc -e "d=3.3 10^-30 C.m; 8.97d"
\end{eg}

<% marg(-10) %>

<% fig('eg-dipole-vector','Example \ref{eg:dipole-vector}.')
%>
<% end_marg %>

\begin{eg}{Dipole moments as vectors}\label{eg:dipole-vector}
\egquestion The horizontal and vertical spacing between the charges in the figure is $b$. Find the dipole
moment.

\eganswer Let the origin of the coordinate system be at the leftmost charge.
\begin{align*}
  \vc{D} &= \sum q_i \vc{r}_i \\
         &= (q)(\vc{0})+(-q)(b\hat{\vc{x}})+(q)(b\hat{\vc{x}}+b\hat{\vc{y}})+(-q)(2b\hat{\vc{x}}) \\
         &= -2bq\hat{\vc{x}}+bq\hat{\vc{y}}
\end{align*}
The dipole moment points up and to the left, which makes sense because the positive charges are
predominantly above and to the left of the negative ones.
\end{eg}

\begin{eg}{Molecules with zero and nonzero dipole moments}\label{eg:molecular-dipoles}
It can be useful to know whether or not a molecule is polar, i.e., has a nonzero dipole moment.
A polar molecule such as water is readily heated in a microwave oven, while a nonpolar one is not.
Polar molecules are attracted to one another, so polar substances dissolve in other polar substances,
but not in nonpolar substances, i.e., ``like dissolves like.'' 

In a symmetric molecule such as carbon disulfide, figure
\subfigref{molecular-dipoles}{1}, the dipole moment vanishes by
symmetry. For if we rotate the molecule by 180 degrees about any one
of the three coordinate axes defined in the caption of the figure, the molecule is unchanged, which means
that its dipole moment is unchanged. A zero vector is the only
vector that can stay the same under all these rotations.

<%
  fig(
    'molecular-dipoles',
    %q{%
      Example \ref{eg:molecular-dipoles}. The positive $x$ axis is to the right, $y$ is up,
      and $z$ is out of the page. Dark gray atoms are carbon, and the small light gray ones are hydrogen.
      Some other elements are labeled when their identity would otherwise not be clear.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true,
      'sidepos'=>'b'
    }
  )
%>

If we wish, we can think of this vanishing of the dipole moment
as arising from a cancellation of two dipole vectors, one for
each of the molecular bonds. Each chemical element has a certain affinity
for electrons, which is represented by the column in which it lies on the periodic table,
and is ultimately determined by quantum physics (section \ref{subsubsec:deriving-periodic-table},
p.~\pageref{subsubsec:deriving-periodic-table}). 
Sulfur ``likes'' electrons just slightly more than carbon, so that the electron cloud shifts outward
somewhat from the center. This gives rise to two dipole vectors, each of which points inward, toward
the positive charge. (Chemists use an opposite convention for the direction.) These two vectors add up
to zero.

Similar symmetry arguments show that sulfur hexafluoride, \subfigref{molecular-dipoles}{2}, and
benzene \subfigref{molecular-dipoles}{3}, have vanishing dipole moments.

The formaldehyde molecule, \subfigref{molecular-dipoles}{4}, does not have enough symmetry to guarantee
that its dipole moment must vanish, but it does have enough to dictate that the dipole vector
must lie along the $x$ axis. To determine the sign of $D_x$, we must use the fact that
oxygen has a much higher electron affinity than carbon, so this creates
a strong contribution to the dipole moment in the negative $x$ direction. Carbon in turn has a higher
electron affinity than hydrogen, so there is also a dipole moment associated with each CH bond,
pointing toward the hydrogen and therefore contributing a further negative $D_x$. The $y$ components
cancel. A similar analysis for chloroform, \subfigref{molecular-dipoles}{5},
shows that the dipole moment points in the positive $z$ direction.

From these arguments we can tell, for example, that
carbon disulfide will be soluble in benzene, but chloroform will not.
\end{eg}

<% end_sec() %>

\pagebreak

<% begin_sec("Alternative definition of the electric field") %>
\label{subsec:efielddipoledef}

The behavior of a dipole in an externally created field
leads us to an alternative definition of the electric field:

The electric field vector, $E$, at any location in space is
defined by observing the torque exerted on a test dipole
$D_t$ placed there. The direction of the field is the
direction in which the field tends to align a dipole (from
$-$ to +), and the field's magnitude is $|\vc{E}|=\tau/D_t\sin\theta$.
In other words, the field vector is the vector that satisfies the
equation $\btau = \vc{D}_t\times\vc{E}$ for any test dipole $\vc{D}_t$
placed at that point in space.

The main reason for introducing a second definition for the
same concept is that the magnetic field is most easily
defined using a similar approach.

<% end_sec() %>

<% begin_sec("Energy of a dipole in a field") %>

A dipole in an external field has a stable equilibrium orientation in which
$\vc{D}$ is parallel to $\vc{E}$. Since the vector cross product vanishes for parallel
vectors, the field's torque on the dipole is zero.
(There is also an unstable equilibrium with $\vc{D}$ and $\vc{E}$ antiparallel.)
This fact is probably more familiar from the magnetic context, where a magnetic dipole such
as a compass needle tends to align itself with an external magnetic field. We will encounter
magnetic fields and dipoles later, and they have many properties analogous to those of their
electric counterparts. Depending on its orientation, the dipole will have some interaction
energy $U$ when it interacts with the field. If the physical size of the dipole is small, the
electric field can be approximated as having a single value throughout.
Since energy is a scalar, and the dot product is
the only way (up to a multiplicative constant) to multiply two vectors to get a scalar
(sec.~\ref{subsec:dotproduct}, p.~\pageref{subsec:dotproduct}), we must
have $U\propto \vc{D}\cdot\vc{E}$. Because the energy is minimized when $\vc{D}$ and $\vc{E}$ are
parallel, the constant of proportionality must be negative, and one can easily show by considering
a concrete example that this constant equals $-1$. Therefore we have
\begin{equation*}
        U = -\vc{D}\cdot\vc{E}\eqquad. \qquad\text{[energy of a pointlike dipole in a field]}
\end{equation*}\index{dipole!electric!energy due to orientation}

\vfill

\begin{eg}{Force on a dipole in a nonuniform field}\label{eg:dipole-in-nonuniform-field-fancy}
If a dipole with zero total charge is placed in a uniform field, it may experience a torque, but
 it will not experience any \emph{force}. The situation changes if the field is nonuniform.
The force can be nonzero, and, perhaps more surprisingly, the force depends only on the dipole moment,
not on the details of the arrangement of the charges inside the dipole, provided that the dipole is
small enough. This can be shown either
by a brute-force calculation (problem \ref{hw:dipole-in-nonuniform-field}, \pageref{hw:dipole-in-nonuniform-field})
or by the following slightly slicker technique. The force in the $x$ direction is
\begin{equation*}
  F_x = -\frac{\partial U}{\partial x},
\end{equation*}
where the symbol $\partial$ indicates a derivative with respect to the $x$ coordinate only, holding $y$ and
$z$ constant. (This is referred to as a partial derivative.) We then have
\begin{equation*}
  F_x = -\frac{\partial }{\partial x}(\vc{D}\cdot\vc{E}) = \vc{D}\cdot\frac{\partial \vc{E}}{\partial x},
\end{equation*}
which depends on the dipole's properties only through $\vc{D}$. Similar expressions apply for the $y$ and $z$
components.

This principle can be used as a way of measuring the unknown dipole moments of a beam of particles, as
in figure \figref{dipole-spectrometer}. At a more pedestrian level, this is one way of explaining
the fact that we can use a charged object to pick up uncharged
scraps of paper (p.~\pageref{subsubsec:e-forces-on-neutral-objects}).
\end{eg}

<% marg(0) %>
<%
  fig(
    'dipole-spectrometer',
    %q{An electric dipole spectrometer. A beam of randomly oriented dipoles is shot through a ``croquet hoop''
       consisting of two fixed positive charges. Although the field along the central axis of symmetry equals zero,
       the field is nonuniform, and therefore the dipoles feel a nonvanishing force, and are sorted out according
       to their orientations. A magnetic version of this device, described on p.~\pageref{eg:sharp-magnet-poles},
       was used in the historic Stern-Gerlach experiment, sec.~\ref{sec:stern-gerlach}, p.~\pageref{sec:stern-gerlach}.}
  )
%>
<% end_marg %>

<% end_sec() %>

\pagebreak

\startdqs

\begin{dq}
        In the definition of the electric field, does the test
        charge need to be 1 coulomb? Does it need to be positive?
\end{dq}
 % 
\begin{dq}
        Does a charged particle such as an electron or proton
        feel a force from its own electric field?
\end{dq}
 % 
\begin{dq}
        Is there an electric field surrounding a wall socket that
        has nothing plugged into it, or a battery that is just sitting on a table?
\end{dq}
 % 
\begin{dq}
        In a flashlight powered by a battery, which way do the
        electric fields point? What would the fields be like inside
        the wires? Inside the filament of the bulb?
\end{dq}
 % 
\begin{dq}
        Criticize the following statement: ``An electric field
        can be represented by a sea of arrows showing how current is flowing.''
\end{dq}
 % 
\begin{dq}
        The field of a point charge, $|\vc{E}|=kQ/r^2$, was
        derived in a self-check. How would the field pattern
        of a uniformly charged sphere compare with the field of a point charge?
\end{dq}
 % 
\begin{dq}
        The interior of a perfect electrical conductor in
        equilibrium must have zero electric field, since otherwise
        the free charges within it would be drifting in response to
        the field, and it would not be in equilibrium. What about
        the field right at the surface of a perfect conductor?
        Consider the possibility of a field perpendicular to the
        surface or parallel to it.
\end{dq}

<% marg(30) %>
<%
  fig(
    'dq-dipole-moments',
    %q{Discussion question \ref{dq:dipole-moments}.}
  )
%>
<% end_marg %>

\begin{dq}\label{dq:dipole-moments}
Compare the dipole moments of the molecules and molecular
ions shown in the figure.
\end{dq}
 % 
\begin{dq}
        Small pieces of paper that have not been electrically
        prepared in any way can be picked up with a charged object
        such as a charged piece of tape. In our new terminology, we
        could describe the tape's charge as inducing a dipole moment
        in the paper. Can a similar technique be used to induce not
        just a dipole moment but a charge?
\end{dq}

 % ------------------------------------------------------------------------------------ 

<% end_sec() %>

<% end_sec %>

<% begin_sec("Potential related to field") %>
%
        \index{electric field!related to voltage}\index{voltage!related to electric field}

<% begin_sec("One dimension") %>

Electrical potential (voltage) is electrical energy per unit charge, and electric
field is force per unit charge. For a particle moving in one
dimension, along the $x$ axis, we can therefore relate
potential and field if we start from the relationship between
interaction energy and force,
\begin{equation*}
                \der U    =    -F_x\der x\eqquad,   
\end{equation*}
and divide by charge,
\begin{equation*}
                \frac{\der U}{q}                =    -\frac{F_x}{q}\der x\eqquad,
\end{equation*}
giving
\begin{equation*}
                        \der V    =    -E_x \der x\eqquad,
\end{equation*}                                                
or
\begin{equation*}
                        \frac{\der V}{\der x}    =    -E_x\eqquad.
\end{equation*}                                                
The interpretation is that a strong
electric field occurs in a region of space where the potential is
rapidly changing. By analogy, a steep hillside is a place on
the map where the altitude is rapidly changing.

\begin{eg}{Field generated by an electric eel}
\egquestion
Suppose an electric eel is 1 m long, and
generates a voltage difference of 1000 volts between its
head and tail. What is the electric field in the water around it?

\eganswer
        We are only calculating the amount of field, not
its direction, so we ignore positive and negative signs.
Subject to the possibly inaccurate assumption of a constant
field parallel to the eel's body, we have
\begin{align*}
                        |\vc{E}| &=    \frac{\der V}{\der  x}  \\
                                        &\approx \frac{\Delta V}{\Delta  x} \qquad \text{[assumption of constant field]} \\
                                         &=    1000\ \zu{V/m}\eqquad.  
\end{align*}

\end{eg}

\enlargethispage{-6\baselineskip}

\begin{eg}{Relating the units of electric field and potential}
From our original definition of the electric field, we
expect it to have units of newtons per coulomb, N/C. The
example above, however, came out in volts per meter, V/m.
Are these inconsistent? Let's reassure ourselves that this
all works. In this kind of situation, the best strategy is
usually to simplify the more complex units so that they
involve only mks units and coulombs. Since potential is
defined as electrical energy per unit charge, it has units of J/C:
\begin{align*}
                \frac{\zu{V}}{\zu{m}}         &=   \frac{\zu{J/C}}{\zu{m}}   \\
                         &=  \frac{\zu{J}}{\zu{C}\cdot\zu{m}}\eqquad.  
\end{align*}
To connect joules to newtons, we recall that work equals
force times distance, so $\zu{J}=\zu{N}\cdot\zu{m}$, so
\begin{align*}
                \frac{\zu{V}}{\zu{m}} &=   \frac{\zu{N}\cdot\zu{m}}{\zu{C}\cdot\zu{m}}   \\
                         &=      \frac{\zu{N}}{\zu{C}}
\end{align*}
As with other such difficulties with electrical units, one
quickly begins to recognize frequently occurring combinations.
\end{eg}
\begin{eg}{Potential associated with a point charge}
\egquestion
What is the potential associated with a point charge?

\eganswer As derived previously in self-check \ref{sc:pointchargefield} on page \pageref{sc:pointchargefield}, the field is
        \begin{equation*}
                        |\vc{E}|         =    \frac{ kQ}{ r^2}     
        \end{equation*}
        The difference in potential between two points on the same radius line is
        \begin{align*}
                        \Delta  V         &=      -\int \der  V \\
                                                &= -\int  E_{x} \der  x
        \end{align*}
        In the general discussion above, $x$ was just a generic name
        for distance traveled along the line from one point to the
        other, so in this case $x$ really means $r$.
        \begin{align*}
                        \Delta V &= -\int_{ r_1}^{ r_2}  E_{r} \der  r \\
                                 &=   -\int_{ r_1}^{ r_2} \frac{ kQ}{ r^2} \der  r   \\
                                 &=   \left.\frac{ kQ}{ r}\right]_{ r_1}^{ r_2} \\  
                                 &= \frac{ kQ}{ r_2}-\frac{ kQ}{ r_1}\eqquad.
        \end{align*}
        The standard convention is to use $r_1=\infty$ as a reference
        point, so that the potential at any distance $r$ from the charge is
        \begin{equation*}
                         V         =   \frac{ kQ}{ r}\eqquad.  
        \end{equation*}
        The interpretation is that if you bring a positive test
        charge closer to a positive charge, its electrical energy is
        increased; if it was released, it would spring away,
        releasing this as kinetic energy.

\end{eg}

<% self_check('pointchargevtoe',<<-'SELF_CHECK'
Show that you can recover the expression for the field of a
        point charge by evaluating the derivative $E_{x}=-\der  V/\der  x$.
  SELF_CHECK
  ) %>

\begin{eg}{Weighing an electron}\label{eg:tolman-stewart}
J.J.~Thomson (p.~\pageref{subsubsec:thomson-experiment}) is considered to have discovered the electron because he measured its
charge-to-mass ratio $q/m$ and found it to be much larger than that of an ionized atom,
interpreting this as evidence that he was seeing a subatomic particle with a mass much small than an atom's.
But not only is the electron's $q/m$ relatively large compared to that of an atom, it is simply a huge number ($\sim-10^{11}\ \zu{C}/\kgunit$) when expressed
in SI units. SI units are designed for human scales of experience, so this suggests that in everyday life we should expect it to be very
difficult to detect any effect from the weight or inertia of an electron.

As an example, suppose that a metal rod of length $L$ is
oriented upright. The conduction electrons are free to move, so they would tend to drop to the bottom of the rod. Electrical forces will
however resist this segregation of positive and negative charges. To estimate how hard it would be to observe such an effect, let us
imagine connecting the probes of a voltmeter to the ends of the rod. In equilibrium, the electrical and gravitational fields must have effects
on an electron that cancel out. Setting the magnitudes of these forces equal to each other, we have $eE=mg$, and since $E=\Delta V/L$, we predict
a voltage difference $\Delta V=(m/q)gL$. For a one-meter rod, the predicted effect is $\sim10^{-10}\ \zu{V}$.

This is quite small, but not
impossible to measure, and the theoretical prediction was confirmed for a similar experiment by Tolman and Stewart in a 1916 experiment at Berkeley.
This was the first direct evidence that the charge carriers inside a metal wire are in fact electrons.
Similarly, we do expect mechanical side-effects in any electrical circuit, e.g., a slight twitching of a flashlight when we turn it on or off,
but these will be much too small to notice except with exceptionally delicate and sensitive tools.\index{Tolman-Stewart experiment}
% calc -x -e "m=9.1 10^-31 kg; g=9.8 m/s2; L=1 m; V=(m/e)gL->V"
%    V = 5.56617829287959*10^-11 V
It is surprising that we can get information about the microscopic structure of a metal merely by measuring
its properties in this way. Another, similar example along these lines is described in sec.~\ref{subsec:g-factor}, p.~\pageref{subsec:g-factor}.
\end{eg}

<% end_sec() %>

<% begin_sec("Two or three dimensions",nil,'evthreed') %>

The topographical map in figure \figref{topo} suggests a good
way to visualize the relationship between field and potential
in two dimensions. Each contour on the map is a line of
constant height; some of these are labeled with their
elevations in units of feet. Height is related to gravitational
 energy, so in a gravitational analogy, we can
think of height as representing potential. Where the contour
lines are far apart, as in the town, the slope is gentle.
Lines close together indicate a steep slope.

<% marg(70) %>
<%
  fig(
    'topo',
    %q{%
      A topographical map of Shelburne Falls,
              Mass. \photocredit{USGS}
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'voltageofpointcharge',
    %q{%
      The constant-potential curves surrounding a point charge.
              Near the charge, the curves are so closely spaced that they blend
              together on this drawing due to the finite width with which they were
              drawn. Some electric fields are shown as arrows.
    }
  )
%>
<% end_marg %>

If we walk along a straight line, say straight east from the
town, then height (potential) is a function of the east-west
coordinate $x$. Using the usual mathematical definition of
the slope, and writing $V$ for the height in order to remind
us of the electrical analogy, the slope along such a line is
$\der V/\der x$ (the rise over the run).

What if everything isn't confined to a straight line? Water
flows downhill. Notice how the streams on the map cut
perpendicularly through the lines of constant height.

It is possible to map potentials in the same way, as shown in
figure \figref{voltageofpointcharge}. The electric field is strongest where the
constant-potential curves are closest together, and the
electric field vectors always point perpendicular to the
constant-potential curves.

The one-dimensional relationship $E=-\der V/\der x$ generalizes
to three dimensions as follows: 
\begin{align*}
                E_x  &=  -\frac{\der V}{\der x}  \\
                E_y  &=  -\frac{\der V}{\der y}  \\
                E_z  &=  -\frac{\der V}{\der z}  
\end{align*}
This can be notated as a gradient (page \pageref{gradandlineintegral}),
\begin{equation*}
                \vc{E} = -\nabla V\eqquad,
\end{equation*}
and if we know the field and want to find the potential, we can
use a line integral,
\begin{equation*}
                \Delta V         =    -\int_C \vc{E}\cdot\der\vc{r}\eqquad,  
\end{equation*}
where the quantity inside the integral is a vector dot product.

<% self_check('topointerp',<<-'SELF_CHECK'
Imagine that figure \figref{topo} represents potential rather
        than height. (a) Consider the stream the starts near the
        center of the map. Determine the positive and negative signs
        of $\der V/\der x$ and $\der V/\der y$, and relate these to the direction of
        the force that is pushing the current forward against the
        resistance of friction. (b) If you wanted to find a lot of
        electric charge on this map, where would you look?
  SELF_CHECK
  ) %>

\enlargethispage{-2\baselineskip}

Figure \figref{twodvmaps} %on page \pageref{fig:twodvmaps} % convinced it to put on same page
shows some examples of ways
to visualize field and potential patterns.

<%
  fig(
    'twodvmaps',
    %q{%
      Two-dimensional field and potential patterns.
      Top: A uniformly charged rod. Bottom: A dipole.
      In each case, the diagram on the left shows the field vectors and constant-potential curves, while the one on the right shows
      the potential (up-down coordinate) as a function of x and y.
      Interpreting the field diagrams: Each arrow represents the field at the point where its tail has been positioned. For clarity,
      some of the arrows in regions of very strong field strength are not shown --- they would be too long to show.
      Interpreting the constant-potential curves: In regions of very strong fields, the curves are not shown because they would
      merge together to make solid black regions.
      Interpreting the perspective plots: Keep in mind that even though we're visualizing things in three dimensions, these are
      really two-dimensional potential patterns being represented. The third (up-down) dimension represents potential, not position.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Fields by superposition",4) %>

<% begin_sec("Electric field of a continuous charge distribution") %>

Charge really comes in discrete chunks, but often it is
mathematically convenient to treat a set of charges as if
they were like a continuous fluid spread throughout a region
of space. For example, a charged metal ball will have charge
spread nearly uniformly all over its surface, and for
most purposes it will make sense to ignore the fact that
this uniformity is broken at the atomic level. The electric
field made by such a continuous charge distribution is the
sum of the fields created by every part of it. If we let the
``parts'' become infinitesimally small, we have a sum of an
infinitely many infinitesimal numbers: an
integral. If it was a discrete sum, as in example \ref{eg:square} on
page \pageref{eg:square}, we would have a total
electric field in the $x$ direction that was the sum of all
the $x$ components of the individual fields, and similarly
we'd have sums for the $y$ and $z$ components. In the
continuous case, we have three integrals. Let's keep it simple
by starting with a one-dimensional example.

<% marg(0) %>
<%
  fig(
    'eg-charged-rod',
    %q{Example \ref{eg:chargedrod}.}
  )
%>
<% end_marg %>

\begin{eg}{Field of a uniformly charged rod}\label{eg:chargedrod}
\egquestion
A rod of length $L$ has charge $Q$ spread
uniformly along it. Find the electric field at a point a
distance $d$ from the center of the rod, along the rod's axis.

\eganswer
This is a one-dimensional
situation, so we really only need to do a single integral
representing the total field along the axis. We imagine
breaking the rod down into short pieces of length $\der  z$, each
with charge $\der  q$. Since charge is uniformly spread along the
rod, we have $\der  q=\lambda\der  z$, where $\lambda= Q/ L$ (Greek lambda) is the
charge per unit length, in units of coulombs per meter. Since the pieces are infinitesimally
short, we can treat them as point charges and use the
expression $k\der  q/ r^2$ for their contributions to the field,
where $r= d- z$ is the distance from the charge at $z$ to the
point in which we are interested.
\begin{align*}
                 E_{z}         &=  \int \frac{ k\der  q }{ r^2}   \\
                         &=  \int_{- L/2}^{+ L/2} \frac{ k\lambda\der  z }{ r^2}   \\
                         &=   k \lambda \int_{- L/2}^{+ L/2} \frac{\der  z}{( d- z)^2}
\end{align*}
The integral can be looked up in a table, or reduced to an
elementary form by substituting a new variable for
$d- z$. The result is
\begin{align*}
                 E_{z}         &=    k\lambda\left(\frac{1}{ d- z}\right)_{- L/2}^{+ L/2}   \\
                         &=   \frac{ kQ}{ L} \left(\frac{1}{ d- L/2}-\frac{1}{ d+ L/2}\right)\eqquad.  
\end{align*}
        For large values of $d$, this expression gets
smaller for two reasons: (1) the denominators of the
fractions become large, and (2) the two fractions become
nearly the same, and tend to cancel out. This makes sense,
since the field should get weaker as we get farther away from
the charge. In fact, the field at large distances must approach
$ kQ/ d^2$ (homework problem \ref{hw:distantrsquared}).

It's also interesting to note that the field becomes
infinite at the ends of the rod, but is not infinite on the
interior of the rod. Can you explain physically why this happens?
\end{eg}

Example \ref{eg:chargedrod} was one-dimensional. In the general three-dimensional
case, we might have to integrate all three components of the field. However,
there is a trick that lets us avoid this much complication. The potential is a scalar,
so we can find the potential by doing just a single integral, then use the potential
to find the field.

<% marg(0) %>
<%
  fig(
    'chargedrodside',
    %q{Example \ref{eg:chargedrodside}.}
  )
%>
<% end_marg %>

\begin{eg}{Potential, then field}\label{eg:chargedrodside}
\egquestion
A rod of length $L$ is uniformly charged with charge $Q$.
Find the field at a point lying in the midplane of the rod
at a distance $R$. 

\eganswer
By symmetry, the field has only a radial component, $E_R$, pointing
directly away from the rod (or toward it for $Q<0$). The brute-force approach,
then, would be to evaluate the integral $E=\int |\der\vc{E}|\zu{cos}\ \theta$,
where $\der\vc{E}$ is the contribution to the field from a charge $\der q$ at some
point along the rod, and $\theta$ is the angle $\der\vc{E}$ makes with the radial line.

It's easier, however, to find the potential first, and then find the field from the
potential. Since the potential is a scalar,
we simply integrate the contribution $\der V$ from each charge $\der q$, without
even worrying about angles and directions. Let $z$ be the coordinate that measures distance
up and down along the rod, with $z=0$ at the center of the rod. Then the distance
between a point $z$ on the rod and the point of interest is 
$r=\sqrt{ z^2+ R^2}$, and we have
\begin{align*}
         V        &= \int \frac{ k\der q}{ r} \\
                        &=  k\lambda \int_{- L/2}^{+ L/2}\frac{\der z}{ r} \\
                        &=  k\lambda \int_{- L/2}^{+ L/2}\frac{\der z}{\sqrt{ z^2+ R^2}} \\
\end{align*}
The integral can be looked up in a table, or evaluated using computer software:
\begin{align*}
         V        &= \left. k\lambda\: \zu{ln}\left( z+\sqrt{ z^2+ R^2}\right)\right|_{- L/2}^{+ L/2} \\
                &=  k\lambda\: \zu{ln}\left(%
                                        \frac{ L/2+\sqrt{ L^2/4+ R^2}}%
                                                {- L/2+\sqrt{ L^2/4+ R^2}}%
                                \right) \\
\end{align*}
The expression inside the parentheses can be simplified a little.
Leaving out some tedious algebra, the result is
\begin{equation*}
         V = 2 k\lambda\: \zu{ln}\left(%
                                        \frac{ L}{2 R}%
                                                +\sqrt{1+\frac{ L^2}{4 R^2}}%
                                \right)
\end{equation*}

This can readily be differentiated to find the field:
\begin{align*}
         E_{R}        &= -\frac{\der V}{\der R} \\
                                        &= (-2 k\lambda)%
                                                \frac{%
                                                        - L/2 R^2
                                                        +(1/2)(1+ L^2/4 R^2)^{-1/2}(- L^2/2 R^3)
                                                }{%
                                                         L/2 R+(1+ L^2/4 R^2)^{1/2}
                                                }\eqquad, \qquad \\
\intertext{or, after some simplification,}
                 E_{R} &= \frac{ k\lambda L}{ R^2\sqrt{1+ L^2/4 R^2}}
\end{align*}

For large values of $R$, the square root approaches one, and we have
simply $E_{R}\approx k\lambda L/ R^2= k Q/ R^2$.
In other words, the field very far away is the same regardless of whether the charge
is a point charge or some other shape like a rod. This is intuitively appealing, and
doing this kind of check also helps to reassure one that the final result is correct.
\end{eg}
The preceding example, although it involved some messy algebra, required only straightforward
calculus, and no vector operations at all, because we only had to integrate a scalar
function to find the potential. The next example is one in which
we can integrate either the field or the potential without too much complication.

<% marg(0) %>
<%
  fig(
    'ring',
    %q{Example \ref{eg:ring}.}
  )
%>
<% end_marg %>

\begin{eg}{On-axis field of a ring of charge}\label{eg:ring}
        \egquestion
        Find the potential and field along the axis of a uniformly charged ring.

        \eganswer
        Integrating the potential is straightforward.
        \begin{align*}
                 V        &= \int \frac{ k\der q}{ r} \\
                                &=  k \int \frac{\der q}{\sqrt{ b^2+ z^2}} \\
                                &= \frac{ k}{\sqrt{ b^2+ z^2}} \int \der q \\
                                &= \frac{ kQ}{\sqrt{ b^2+ z^2}}\eqquad,
        \end{align*}
        where $Q$ is the total charge of the ring.
        This result could have been derived without calculus, since the distance
        $r$ is the same for every point around the ring, i.e., the integrand is a constant.
        It would also be straightforward to find the field by differentiating this expression
        with respect to $z$ (homework problem \ref{hw:ringve}).

        Instead, let's see how to find the field by direct integration. By symmetry, the
        field at the point of interest can have only a component along the axis of
        symmetry, the $z$ axis:
        \begin{align*}
                 E_{x}        &= 0 \\
                 E_y        &= 0 
        \end{align*}
        To find the field in the $z$ direction, we integrate the $z$ components
        contributed to the field by each infinitesimal part of the ring.
        \begin{align*}
                 E_{z}        &= \int \der E_z \\
                                                &= \int |\der\vc{E}|\:\zu{cos}\:\theta\eqquad,
        \end{align*}
        where $\theta$ is the angle shown in the figure.
        \begin{align*}
                 E_{z}        &= \int \frac{ k\der q}{ r^2}\:\zu{cos}\:\theta \\
                                                &=  k \int \frac{\der q}{ b^2+ z^2}\:\zu{cos}\:\theta 
        \end{align*}
        Everything inside the integral is a constant, so we have
        \begin{align*}
                 E_{z}        &= \frac{ k}{ b^2+ z^2}\:\zu{cos}\:\theta \int \der q \\
                                                &= \frac{ kQ}{ b^2+ z^2}\:\zu{cos}\:\theta \\
                                                &= \frac{ kQ}{ b^2+ z^2}\:\frac{ z}{ r} \\
                                                &= \frac{ kQz}{\left( b^2+ z^2\right)^\zu{3/2}} 
        \end{align*}
\end{eg}

In all the examples presented so far, the charge has been confined to a one-dimensional
line or curve. Although it is possible, for example, to put charge on a piece of wire,
it is more common to encounter practical devices in which the charge is distributed
over a two-dimensional surface, as in the flat metal plates used in Thomson's experiments.
Mathematically, we can approach this type of calculation with the divide-and-conquer
technique: slice the surface into lines or curves whose fields we know how to calculate,
and then add up the contributions to the field from all these slices. In the limit where the
slices are imagined to be infinitesimally thin, we have an integral.

\begin{eg}{Field of a uniformly charged disk}\label{eg:diskofcharge}
\egquestion
A circular disk is uniformly charged. (The disk must be an insulator; if it was a conductor,
then the repulsion of all the charge would cause it to collect more densely near the
edge.) Find the field at a point on the axis, at a distance $z$ from the plane of the disk.

<% marg(70) %>
<%
  fig(
    'disk',
    %q{Example \ref{eg:diskofcharge}: geometry.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'diskfield',
    %q{Example \ref{eg:diskofcharge}: the field on both sides (for $\sigma>0$).}
  )
%>

\spacebetweenfigs

<%
  fig(
    'diskcap',
    %q{A capacitor consisting of two disks with opposite charges.}
  )
%>
<% end_marg %>

\eganswer
We're given that every part of the disk has the same charge per unit area, so rather
than working with $Q$, the total charge, it will be easier to use the charge
per unit area, conventionally notated $\sigma$ (Greek sigma), $\sigma= Q/\pi b^2$.

Since we already know the field due to a ring of charge, we can solve the problem
by slicing the disk into rings, with each ring extending from $r$ to $r+\der r$.
The area of such a ring equals its circumference multiplied by its width,
i.e., $2\pi r\der r$, so its charge is $\der q=2\pi\sigma r\der r$,
and from the result of example \ref{eg:ring}, its contribution to the field is
\begin{align*}
        \der E_{z}        &= \frac{ kz\der q}{\left( r^2+ z^2\right)^\zu{3/2}} \\
                                        &= \frac{2\pi\sigma kzr\der r}{\left( r^2+ z^2\right)^\zu{3/2}} \\
\end{align*}
The total field is
\begin{align*}
         E_{z}        &= \int \der E_{z} \\
                                        &= 2\pi\sigma kz
                                                \int_0^{b} 
                                                \frac{ r\der r}{\left( r^2+ z^2\right)^\zu{3/2}} \\
                &= 2\pi\sigma kz \left. %
                                \frac{-1}{\sqrt{ r^2+ z^2}}
                        \right|_{ r=0}^{ r=\zu{b}} \\
                &= 2\pi\sigma k\left(1-\frac{ z}{\sqrt{ b^2+ z^2}}\right)
\end{align*}
\end{eg}

The result of example \ref{eg:diskofcharge} has some interesting properties.
First, we note that it was derived on the unspoken assumption
of $z>0$. By symmetry, the field on the other side of the disk must be equally
strong, but in the opposite direction, as shown  in
figures \figref{diskfield} and \figref{diskfieldgraph}. Thus there is a discontinuity in the field
at $z=0$. In reality, the disk will have some finite thickness, and the switching
over of the field will be rapid, but not discontinuous.

At large values of $z$, i.e., $z\gg b$, the field rapidly approaches
the $1/r^2$ variation that we expect when we are so far from the disk that the disk's
size and shape cannot matter (homework problem \ref{hw:distantrsquared}).

%

<%
  fig(
    'diskfieldgraph',
    %q{Example \ref{eg:diskofcharge}: variation of the field ($\sigma>0$).},
    {
      'width'=>'wide'
    }
  )
%>

A practical application is the case of a capacitor, \figref{diskcap}, having
two parallel circular plates very close together. In normal operation, the charges
on the plates are opposite, so one plate has fields pointing into it and the other
one has fields pointing out. In a real capacitor, the plates are a metal conductor,
not an insulator, so the charge will tend to arrange itself more densely near the edges,
rather than spreading itself uniformly on each plate. Furthermore, we have only calculated
the \emph{on-axis} field in example \ref{eg:diskofcharge}; in the off-axis region,
each disk's contribution
to the field will be weaker, and it will also point away from the axis a little. But if we are
willing to ignore these complications for the sake of a rough analysis, then the fields
superimpose as shown in figure \figref{diskcap}: the fields cancel the outside of the
capacitor, but between the plates its value is double that contributed by a single
plate. This cancellation on the outside is a very useful property for a practical
capacitor. For instance, if you look at the printed circuit board in a typical piece
of consumer electronics, there are many capacitors, often placed fairly close together.
If their exterior fields didn't cancel out nicely, then each capacitor would interact
with its neighbors in a complicated way, and the behavior of the circuit would depend
on the exact physical layout, since the interaction would be stronger or weaker depending
on distance. In reality, a capacitor does create weak external electric fields, but their
effects are often negligible, and we can then use the \emph{lumped-circuit approximation},
\index{lumped-circuit approximation!for capacitors}\label{lumped-circuit-approx}
which states that each component's behavior depends only on the currents that flow in
and out of it, not on the interaction of its fields with the other components.



<% end_sec() %>

<% begin_sec("The field near a charged surface",nil,'surfacefield') %>

From a theoretical point of view, there is something even
more intriguing about example \ref{eg:diskofcharge}:
the magnitude of the field for small values
of $z$ ($z\ll b$) is $E=2\pi k\sigma$, which doesn't depend on $b$ at all for a fixed value
of $\sigma$. If we made a disk with twice the radius, and covered
it with the same number of coulombs per square meter
 (resulting in a total charge four times as great), the field close to the disk would
 be unchanged! That is, a flea living near the center of the disk, \figref{fleathinking}, would have no way
 of determining the size of her flat ``planet'' by measuring the local field
 and charge density. (Only by leaping off the surface into outer space would she
 be able to measure fields that were dependent on $b$. If she traveled very
 far, to $z\gg b$, she would be in the region where the field is well approximated by
 $|\vc{E}|\approx kQ/z^2=k\pi b^2\sigma/z^2$, which she could solve for $b$.)

<% marg(80) %>
<%
  fig(
    'fleathinking',
    %q{%
      Close to the surface, the relationship between $E$ and $\sigma$ is a fixed one, regardless
              of the geometry. The flea can't determine the size or shape of her world by
              comparing $E$ and $\sigma$.
    }
  )
%>
<% end_marg %>

What is the reason for this surprisingly simple behavior of the field? Is it a
piece of mathematical trivia, true only in this particular case? What if the shape
was a square rather than a circle? In other words, the flea gets no information about
the \emph{size} of the disk from measuring $E$, since $E=2\pi k\sigma$, independent
of $b$, but what if she didn't know the \emph{shape}, either? If the result for a square
had some other geometrical factor in front instead of $2\pi$, then she could tell which
shape it was by measuring $E$. The surprising mathematical fact, however, is that the
result for a square, indeed for any shape whatsoever, is $E=2\pi\sigma k$. It doesn't
even matter whether the surface is flat or warped, or whether the density of charge
is different at parts of the surface which are far away compared to the flea's distance
above the surface.

<% marg(0) %>
<%
  fig(
    'fleavectors',
    %q{%
      Fields contributed by nearby parts of the surface, P, Q, and R, contribute
              to $E_\perp$. Fields due to distant charges, S, and T, have very small contributions to
              $E_\perp$ because of their shallow angles.
    }
  )
%>
<% end_marg %>

This universal $E_\perp=2\pi k\sigma$ field perpendicular to
 a charged surface can be proved mathematically based
on Gauss's law\footnote{rhymes with ``mouse''} (section \ref{sec:gauss}), but we can understand
what's happening on qualitative grounds. Suppose on night, while the flea is asleep,
someone adds more surface area, also positively charged, around the outside edge of
her disk-shaped world, doubling its radius. The added charge, however, has very little
effect on the field in her environment, as long as she stays at low altitudes above
the surface. As shown in figure \figref{fleavectors}, the new charge
to her west contributes a field, T, that is almost purely ``horizontal'' (i.e., parallel
to the surface) and to the east. It has 
 a negligible upward component, since the angle is so shallow. This new eastward 
contribution to the field
is exactly canceled out by the westward field, S, created by the new charge to her east.
There is likewise almost perfect cancellation between any other pair of opposite
compass directions. 

A similar argument can be made as to the shape-independence of the result, as long as
the shape is symmetric.
For example, suppose that the next night, the tricky real estate developers
decide to add corners to the disk and transform it into a square. Each corner's
contribution to the field measured at the center is canceled by the field due to the 
corner diagonally across from it.

What if the flea goes on a trip away from the center of the disk? The perfect cancellation
of the ``horizontal'' fields contributed by distant charges will no longer occur, but the
``vertical'' field (i.e., the field perpendicular to the surface)
will still be $E_\perp=2\pi k\sigma$, where $\sigma$ is the local charge density,
since the distant charges can't contribute to the vertical field. The same result applies
if the shape of the surface is asymmetric, and doesn't even have any well-defined
geometric center: the component perpendicular to the surface is $E_\perp=2\pi k\sigma$,
but we may have $E_\parallel\neq0$. All of the above arguments can be made more rigorous
by discussing mathematical limits rather than using words like ``very small.'' There is not
much point in giving a rigorous proof here, however, since we will be able to demonstrate
this fact as a corollary of Gauss' Law in section \ref{sec:gauss}. The result
is as follows:

At a point lying a distance $z$ from a charged surface, the component of the electric field
perpendicular to the surface obeys
\begin{equation*}
        \lim_{z\rightarrow 0} E_\perp = 2\pi k\sigma\eqquad,
\end{equation*}
where $\sigma$ is the charge per unit area. This is true regardless of the shape or
size of the surface.

<% marg(0) %>
<%
  fig(
    'pointlinesurface',
    %q{Example \ref{eg:pointlinesurface}.}
  )
%>
<% end_marg %>

\begin{eg}{The field near a point, line, or surface charge}\label{eg:pointlinesurface}
        \egquestion
        Compare the variation of the electric field with distance, $d$, for small
        values of $d$ in the case of a point charge, an infinite line of charge,
        and an infinite charged surface.

        \eganswer
        For a point charge, we have already found $E\propto  d^{-2}$ for the
        magnitude of the field, where we are now using $d$ for the quantity we would
        ordinarily notate as $r$. This is true for all values of $d$, not just for small
        $d$ --- it has to be that way, because the point charge has no size, so if $E$
        behaved differently for small and large $d$, there would be no way to decide what
        $d$ had to be small or large relative to.

        For a line of charge, the result of example \ref{eg:chargedrodside} is
        \begin{equation*}
         E = \frac{ k\lambda L}%
                { d^2\sqrt{1+ L^2/4 d^2}}\eqquad.
        \end{equation*}
        In the limit of $d\ll L$, the quantity inside the square root is dominated
        by the second term, and we have $E\propto  d^{-1}$.

%\enlargethispage{-2\baselineskip}

        Finally, in the case of a charged surface, the result is simply
        $E=2\pi\sigma k$, or $E\propto  d^{0}$.

        Notice the lovely simplicity of the pattern, as shown in figure 
        \figref{pointlinesurface}. A point is zero-dimensional: it has no length,
        width, or breadth. A line
        is one-dimensional, and a surface is two-dimensional.
        As the dimensionality of the charged object changes from 0 to 1, and then to 2,
        the exponent in the near-field expression goes from 2 to 1 to 0.
\end{eg}

\vfill

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Energy in fields",nil,'fieldenergy') %>

<% begin_sec("Electric field energy") %>

Fields possess energy, as argued on page \pageref{fieldenergy}, but how much energy?
The answer can be found using the following elegant approach. We assume that the electric energy
contained in an infinitesimal volume of space $\der v$ is given by
$\der U_e=f(\vc{E})\der v$, where $f$ is some function, which we wish to determine,
of the field \vc{E}.
It might seem that we would have no easy way to determine the function $f$, but many of
the functions we could cook up would violate the symmetry of space. For instance,
we could imagine $f(\vc{E})=aE_y$, where $a$ is some constant with the appropriate units.
However, this would violate the symmetry of space, because it would give the $y$ axis
a different status from $x$ and $z$. As discussed on page \pageref{subsec:dotproduct}, 
if we wish to calculate a scalar based on some vectors, the dot product is the only way
to do it that has the correct symmetry properties. If all we have is one vector, \vc{E}, then
the only scalar we can form is $\vc{E}\cdot\vc{E}$, which is the square of the magnitude
of the electric field vector.

In principle, the energy function we are seeking could
be proportional to $\vc{E}\cdot\vc{E}$, or to any function computed from it, such as
$\sqrt{\vc{E}\cdot\vc{E}}$ or $(\vc{E}\cdot\vc{E})^7$. On physical grounds, however,
the only possibility that works is $\vc{E}\cdot\vc{E}$. Suppose, for instance, that 
we pull apart two oppositely charged capacitor plates, as shown in figure
\figref{pullplates}. We are doing work by pulling them apart against the force
of their electrical attraction, and this quantity of mechanical work equals the
increase in electrical energy, $U_e$.
 Using our previous approach to energy, we would have thought of
$U_e$ as a quantity which depended on the distance of the positive and negative charges
from each other, but now we're going to imagine $U_e$ as being stored within the electric field
that exists in the space between and around the charges. When the plates are
touching, their fields cancel everywhere, and there is zero electrical energy.
When they are separated, there is still approximately zero field on the outside, but the field
between the plates is nonzero, and holds some energy.

<% marg(0) %>
<%
  fig(
    'pullplates',
    %q{Two oppositely charged capacitor plates are pulled apart.}
  )
%>
<% end_marg %>

Now suppose we carry out the whole process, but with the plates carrying double
their previous charges. Since Coulomb's law involves the product $q_1q_2$ of two
charges, we have quadrupled the force between any given pair of charged particles,
and the total attractive force is therefore also four times greater than before.
This means that the work done in separating the plates is four times greater,
and so is the energy $U_e$ stored in the field.
The field, however, has merely been doubled at any given location:
the electric field $\vc{E}_+$ due to the positively charged plate
is doubled, and similarly for the contribution $\vc{E}_-$ from the negative one,
so the total electric field $\vc{E}_++\vc{E}_-$ is also doubled. Thus
doubling the field results in an electrical energy which is four times greater,
i.e., the energy density must be proportional to the square of the field, 
$\der U_e\propto(\vc{E}\cdot\vc{E})\der v$. For ease of notation, we write this as
$\der U_e\propto E^2\der v$, or $\der U_e=aE^2\der v$, where $a$ is a constant
of proportionality. 
Note that we never really made use of any of the details of the geometry of
figure \figref{pullplates}, so the reasoning is of general validity. 
In other words, not only is  $\der U_e=aE^2\der v$ the function that works in this
particular case, but there is every reason to believe that it would work
in other cases as well.

It now remains only to find $a$. Since the constant
must be the same in all situations, we only need to find one example in which we
can compute the field and the energy, and then we can determine $a$. 
The situation shown in figure \figref{pullplates} is just about the easiest example
to analyze. We let the square capacitor plates be uniformly covered with charge densities
$+\sigma$ and $-\sigma$, and we write $b$ for the lengths of their sides.
Let $h$ be the gap between the plates after they have been separated.
We choose $h\ll b$, so that the field experienced by the negative plate due
to the positive plate is $E_+=2\pi k\sigma$. The charge of the negative plate is
$-\sigma b^2$, so the magnitude of the force attracting it back toward the positive plate
is $(\text{force})=(\text{charge})(\text{field})=2\pi k\sigma^2 b^2$. The amount of work
done in separating the plates is
$(\text{work})=(\text{force})(\text{distance})=2\pi k\sigma^2 b^2h$.
This is the amount of energy that has been stored in the field between the two plates,
$U_e=2\pi k\sigma^2 b^2h=2\pi k\sigma^2 v$, where $v$ is the volume of the region
between the plates.

We want to equate this to $U_e=aE^2v$. (We can write $U_e$ and $v$ rather than 
$\der U_e$ and $\der v$, since the field is constant in the region between the plates.)
 The field between the plates
has contributions from both plates, $E=E_++E_-=4\pi k\sigma$. (We only used half this
value in the computation of the work done on the moving plate, since the moving plate
can't make a force on itself. Mathematically, each plate is in a region where its
own field is reversing directions, so we can think of its own contribution to the field
as being zero within itself.) We then have $aE^2v=a\cdot 16\pi^2k^2\sigma^2 \cdot v$, and setting
this equal to $U_e=2\pi k\sigma^2 v$ from the result of the work computation, we find
$a=1/8\pi k$. Our final result is as follows:

The electric energy possessed by an electric field \vc{E} occupying an infinitesimal
volume of space $\der v$ is given by
\begin{equation*}
        \der U_e = \frac{1}{8\pi k}E^2 \der v\eqquad,
\end{equation*}
where $E^2=\vc{E}\cdot\vc{E}$ is the square of the magnitude of the electric field.
\index{energy density!of electric field}\index{electric field!energy density of}

This is reminiscent of how waves behave: the energy content of a wave is
typically proportional to the square of its amplitude.

\pagebreak[4]

        
<% self_check('reverseex',<<-'SELF_CHECK'
We can think of the quantity $\der U_{e}/\der v$ as the
        \emph{energy density} due to the electric field, i.e., the number of joules
        per cubic meter needed in order to create that field. (a) How does this quantity
        depend on the components of the field vector, $E_x$, $E_y$, and $E_z$? (b) Suppose
        we have a field with $E_x\neq0$, $E_y$=0, and $E_z$=0. What would happen to the
        energy density if we reversed the sign of $E_x$?
  SELF_CHECK
  ) %>

\begin{eg}{A numerical example}\label{eg:numparplate}
\egquestion
A capacitor has plates whose areas are $10^{-4}\ \zu{m}^2$, separated
by a gap of $10^{-5}$ m. A 1.5-volt battery is connected across it.
How much energy is sucked out of the battery and stored in the electric field
between the plates? (A real capacitor typically has an insulating material between
the plates whose molecules interact electrically with the charge in the plates.
For this example, we'll assume that there is just a vacuum in between the plates. The
plates are also typically rolled up rather than flat.)

\eganswer
To connect this with our previous calculations, we need to find the charge
density on the plates in terms of the voltage we were given. Our previous
examples were based on the assumption that the gap between the plates
was small compared to the size of the plates. Is this valid here? Well, if the
plates were square, then the area of $10^{-4}\ \zu{m}^2$ would imply
that their sides were $10^{-2}$ m in length. This is indeed very large
compared to the gap of $10^{-5}$ m, so this assumption appears to be valid
(unless, perhaps, the plates have some very strange, long and skinny shape).

Based on this assumption, the field is relatively uniform in the whole volume
between the plates, so we can use a single symbol, $E$, to represent its magnitude,
and the relation $E=\der  V/\der  x$ is equivalent to
$E=\Delta V/\Delta x=(\text{1.5 V})/(\text{gap})= 1.5\times10^5\ \zu{V}/\zu{m}$.

Since the field is uniform, we can dispense with the calculus, and
replace $\der  U_{e} = (1/8\pi k) E^2 \der  v$
with $U_{e} = (1/8\pi k) E^2 v$.
The volume equals the area multiplied by the
gap, so we have
\begin{align*}
         U_{e} &= (1/8\pi k) E^2(\text{area})(\text{gap})\\
                                &= \frac{1}{8\pi\times9\ \times10^9\ %
                                                \zu{N}\unitdot\munit^2/\zu{C}^2}%
                                                        ( 1.5\times10^5\ \zu{V}/\zu{m})^2%
                                                        (10^{-4}\ \zu{m}^2)(10^{-5}\ \zu{m})\\
                                &= 1\times10^{-10}\ \zu{J}
\end{align*}
\end{eg}

        
<% self_check('numercapunits',<<-'SELF_CHECK'
Show that the units in the preceding example really
        do work out to be joules.
  SELF_CHECK
  ) %>

\begin{eg}{Why $k$ is on the bottom}
It may also seem strange that the constant $k$ 
is in the denominator of the equation $\der  U_{e} = (1/8\pi k) E^2 \der  v$.
The Coulomb constant $k$ tells us how strong electric
forces are, so shouldn't it be on top? No.
Consider, for instance, an alternative universe in which
electric forces are twice as strong as in ours. The numerical value
of $k$ is doubled. Because $k$ is doubled, all the
electric field strengths are doubled as well, which
quadruples the quantity $E^2$. In the expression $E^2/8\pi k$, we've
quadrupled something on top and doubled something on the
bottom, which makes the energy twice as big. That makes perfect sense.
\end{eg}

\begin{eg}{Potential energy of a pair of opposite charges}\label{eg:upointcharges}
        Imagine taking two opposite charges, \figref{upointcharges}, that were
initially far apart and allowing them to come together under
the influence of their electrical attraction.

<% marg(30) %>
<%
  fig(
    'upointcharges',
    %q{Example \ref{eg:upointcharges}.}
  )
%>
<% end_marg %>

        According to our old approach, electrical energy is lost
because the electric force did positive work as it brought
the charges together. (This makes sense because as they come
together and accelerate it is their electrical energy that is
being lost and converted to kinetic energy.)

        By the new method, we must ask how the energy stored in the
electric field has changed. In the region indicated
approximately by the shading in the figure, the superposing
fields of the two charges undergo partial cancellation
because they are in opposing directions. The energy in the
shaded region is reduced by this effect. In the unshaded
region, the fields reinforce, and the energy is increased.

        It would be quite a project to do an actual numerical
calculation of the energy gained and lost in the two regions
(this is a case where the old method of finding energy gives
greater ease of computation), but it is fairly easy to
convince oneself that the energy is less when the charges
are closer. This is because bringing the charges together
shrinks the high-energy unshaded region and enlarges the
low-energy shaded region.
\end{eg}

<% marg(0) %>
<%
  fig(
    'sphericalcap',
    %q{%
      Example \ref{eg:sphericalcap}. Part of the
              outside sphere has been drawn as if it is transparent, in order to
              show the inside sphere.
    }
  )
%>
<% end_marg %>

\begin{eg}{A spherical capacitor}\label{eg:sphericalcap}\index{capacitor!spherical}
\egquestion
A spherical capacitor, \figref{sphericalcap}, consists of two concentric spheres
of radii $a$ and $b$.
Find the energy required to charge up the capacitor
so that the plates hold charges $+ q$ and $- q$.

\eganswer
On page \pageref{shelltheoremsubsection}, I proved that for
\emph{gravitational} forces, the interaction of a spherical
shell of mass with other masses outside it is the same as
if the shell's mass was concentrated at its center. On the
interior of such a shell, the forces cancel out exactly.
Since gravity and the electric force both vary as $1/ r^2$,
the same proof carries over immediately to electrical forces. The magnitude of the outward
electric field contributed by the charge $+ q$ of the central sphere
is therefore
\begin{equation*}
        |\vc{E}_+| = \left\{
                                        \begin{array}{lr}
                                                0,                                                 &  r< a \\
                                                 kq/ r^2,  &  r> a
                                        \end{array}
                                \right.\eqquad,
\end{equation*}
where $r$ is the distance from the center. Similarly, the magnitude of the
\emph{inward} field contributed
by the outside sphere is
\begin{equation*}
        |\vc{E}_-| = \left\{
                                        \begin{array}{lr}
                                                0,                                                 &  r< b \\
                                                 kq/ r^2,  &  r> b
                                        \end{array}
                                \right.\eqquad.
\end{equation*}
In the region outside the whole capacitor, the two fields are equal in
magnitude, but opposite in direction, so they cancel. We then have for
the total field
\begin{equation*}
        |\vc{E}| = \left\{
                                        \begin{array}{lr}
                                                0,                                                 &  r< a \\
                                                 kq/ r^2,  &  a< r< b \\
                                                0,                                          &  r> b
                                        \end{array}
                                \right.\eqquad,
\end{equation*}
so to calculate the energy, we only need to worry about the
region $a< r< b$. The energy density in this region is
\begin{align*}
        \frac{\der  U_{e}}{\der  v} &= \frac{1}{8\pi  k} E^2  \\
                        &= \frac{ kq^2}{8\pi} r^{-4}\eqquad.
\end{align*}
This expression only depends on $r$, so the energy density is constant across
any sphere of radius $r$. We can slice the region $a< r< b$ into
concentric spherical layers, like an onion, and the energy within one such layer,
extending from $r$ to $r+\der r$ is
\begin{align*}
        \der  U_{e}        &= \frac{\der  U_{e}}{\der  v} \der v \\
                        &= \frac{\der  U_{e}}{\der  v} (\text{area of shell}) (\text{thickness of shell}) \\
                        &= (\frac{ kq^2}{8\pi} r^{-4}) (4\pi r^2) (\der r) \\
                        &= \frac{ kq^2}{2} r^{-2}\der r\eqquad.
\end{align*}
Integrating over all the layers to find the total energy, we have
\begin{align*}
         U_{e}        &= \int \der  U_{e} \\
                                        &= \int_{a}^{b} \frac{ kq^2}{2} r^{-2}\der r \\
                                        &= \left.-\frac{ kq^2}{2} r^{-1}\right|_{a}^{b} \\
                                        &= \frac{ kq^2}{2}\left(\frac{1}{a}-\frac{1}{b}\right) \\
\end{align*}
\end{eg}

\startdqs

\begin{dq}\label{eg:pointchargeincap}
        The figure shows a positive charge in the gap between two
        capacitor plates. Compare the energy of the electric fields in the
        two cases. Does this agree with what you would have expected based
        on your knowledge of electrical forces?
\end{dq}

\begin{dq}\label{eg:sphericalcap}
        The figure shows a spherical capacitor. In the text, the energy stored
        in its electric field is shown to be
        \begin{equation*}
                 U_{e}        = \frac{ kq^2}{2}\left(\frac{1}{a}-\frac{1}{b}\right)\eqquad. \\
        \end{equation*}
        What happens if the difference between $b$ and $a$ is very small? Does this
        make sense in terms of the mechanical work needed in order to separate the
        charges? Does it make sense in terms of the energy stored in the electric
        field? Should these two energies be added together?

        Similarly, discuss the cases of $b\rightarrow\infty$ and $a\rightarrow0$.
\end{dq}

<% marg(80) %>
<%
  fig(
    'pointchargeincap',
    %q{Discussion question \ref{eg:pointchargeincap}.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'sphericalcap',
    %q{Discussion question \ref{eg:sphericalcap}.},
    {'suffix'=>'2'}
  )
%>
<% end_marg %>

\begin{dq}
        Criticize the following statement: ``A solenoid makes a charge in
        the space surrounding it, which dissipates when you release the
        energy.''
\end{dq}
 % 
\begin{dq}
        In example \ref{eg:upointcharges} on page \pageref{eg:upointcharges},
        I argued that for the charges shown in the figure, the
        fields contain less
        energy when the charges are closer together, because the region
        of cancellation expanded, while the region of reinforcing fields
        shrank. Perhaps a simpler
        approach is to consider the two extreme possibilities: the case
        where the charges are infinitely far apart, and the one in which
        they are at zero distance from each other, i.e., right on top of each
        other. Carry out this reasoning for the case of (1) a positive
        charge and a negative charge of equal magnitude, (2) two positive
        charges of equal magnitude, (3) the gravitational energy of two
        equal masses.
\end{dq}

<% end_sec() %>

<% begin_sec("Gravitational field energy") %>

Example \ref{eg:sphericalcap} depended on the close analogy between electric and gravitational
forces. In fact, every argument, proof, and example discussed so far in this section is equally
valid as a gravitational example, provided we take into account one fact: only positive mass
exists, and the gravitational force between two  masses is attractive. This is the opposite of
what happens with electrical forces, which are repulsive in the case of two positive charges.
As a consequence of this, we need to assign a \emph{negative} energy density to the gravitational
field! For a gravitational field, we have
\begin{equation*}
        \der U_g = -\frac{1}{8\pi G}g^2 \der v\eqquad,
\end{equation*}
where $g^2=\vc{g}\cdot\vc{g}$ is the square of the magnitude of the gravitational field.
\index{energy density!of gravitational field}\index{gravitational field!energy density of}

<% end_sec() %>

<% begin_sec("Magnetic field energy") %>

So far we've only touched in passing on the topic of magnetic fields, which
will deal with in detail in chapter \ref{ch:em}. Magnetism is an interaction between
moving charge and moving charge, i.e., between currents and currents. Since a current
has a direction in space,\footnote{Current is a scalar, since the definition
$I=\der q/\der t$ is the derivative of a scalar. However, there is a closely related
quantity called the current \emph{density}, \vc{J}, which is a vector, and \vc{J} is
in fact the more fundamentally important quantity.\index{current density}}
while charge doesn't, we can anticipate that the mathematical rule connecting
a magnetic field to its source-currents will have to be completely different from
the one relating the electric field to its source-charges. However, if you look carefully
at the argument leading to the relation $\der U_e/\der v = E^2/8\pi k$, you'll see that
these mathematical details were only necessary to the part of the argument in which
we fixed the constant of proportionality. To establish $\der U_e/\der v \propto E^2$,
we only had to use three simple facts:
\begin{itemize}
        \item The field is proportional to the source.
        \item Forces are proportional to fields.
        \item Field contributed by multiple sources add like vectors.
\end{itemize}
All three of these statements are true for the magnetic field as well, so without
knowing anything more specific about magnetic fields --- not even what units are
used to measure them! --- we can state with certainty that the energy density in
the magnetic field is proportional to the square of the magnitude of the magnetic field.\label{b-field-energy-propto}
\index{energy density!of magnetic field}\index{magnetic field!energy density of}%
The constant of proportionality is given on p.~\pageref{benergy}.

\vfill
 % 
 % ------------------------------------------------------------------------------------- 
 % 

<% end_sec() %>

<% end_sec() %>

<% begin_sec("LRC circuits",4) %>

The long road leading from the light bulb to the computer
started with one very important step: the introduction of
feedback into electronic circuits. Although the principle of
feedback has been understood and and applied to mechanical
systems for centuries, and to electrical ones since the
early twentieth century, for most of us the word evokes an
image of Jimi Hendrix
intentionally creating earsplitting screeches, or of the
school principal doing the same inadvertently in the
auditorium. In the guitar example, the musician stands in
front of the amp and turns it up so high that the sound
waves coming from the speaker come back to the guitar string
and make it shake harder. This is an example of \emph{positive}
feedback: the harder the string vibrates, the stronger the
sound waves, and the stronger the sound waves, the harder
the string vibrates. The only limit is the power-handling
ability of the amplifier.

Negative feedback is equally important. Your thermostat, for
example, provides negative feedback by kicking the heater
off when the house gets warm enough, and by firing it up
again when it gets too cold. This causes the house's
temperature to oscillate back and forth within a certain
range. Just as out-of-control exponential freak-outs are a
characteristic behavior of positive-feedback systems,
oscillation is typical in cases of negative feedback. You
have already studied negative feedback extensively in section \ref{sec:resonance}
in the case of a mechanical system, although we didn't call it that.

<% begin_sec("Capacitance and inductance") %>
%
\index{capacitor}\index{capacitor!capacitance}\index{inductor}\index{inductor!inductance}
In a mechanical oscillation, energy is exchanged repetitively
between potential and kinetic forms, and may also be
siphoned off in the form of heat dissipated by friction. In
an electrical circuit, resistors are the circuit elements
that dissipate heat. What are the electrical analogs of
storing and releasing the potential and kinetic energy of a
vibrating object? When you think of energy storage in an
electrical circuit, you are likely to imagine a battery, but
even rechargeable batteries can only go through 10 or 100
cycles before they wear out. In addition, batteries are not
able to exchange energy on a short enough time scale for
most applications. The circuit in a musical synthesizer may
be called upon to oscillate thousands of times a second, and
your microwave oven operates at gigahertz frequencies.
Instead of batteries, we generally use
capacitors and
inductors to store energy in oscillating circuits. Capacitors,
which you've already encountered, store energy in electric fields.
An inductor does the same with magnetic fields.

<% begin_sec("Capacitors") %>

A capacitor's energy exists in its surrounding electric
fields. It is proportional to the square of the field
strength, which is proportional to the charges on the
plates. If we assume the plates carry charges that are the
same in magnitude, $+q$ and $-q$, then the energy stored in
the capacitor must be proportional to $q^2$. For historical
reasons, we write the constant of proportionality as $1/2C$, 
\begin{equation*}
                U_C    =   \frac{1}{2C}q^2\eqquad.
\end{equation*}
The constant $C$ is a geometrical property of the
capacitor, called its capacitance.

<% marg(50) %>
<%
  fig(
    'capsymbol',
    %q{The symbol for a capacitor.}
  )
%>

  \spacebetweenfigs

<%
  fig(
    'capacitors-photo',
    %q{Some capacitors.}
  )
%>
<% end_marg %>

Based on this definition, the units of capacitance must be
coulombs squared per joule, and this combination is more
conveniently abbreviated as the farad\index{farad!defined},
$1\ \zu{F}=1\ \zu{C}^2/\zu{J}$.
``Condenser'' is a less formal term
for a capacitor. Note that the labels printed on capacitors
often use MF to mean $\mu\zu{F}$, even though MF should really
be the symbol for megafarads, not microfarads. Confusion
doesn't result from this nonstandard notation, since
picofarad and microfarad values are the most common, and it
wasn't until the 1990's that even millifarad and farad
values became available in practical physical sizes. Figure
\figref{capsymbol} shows the symbol used in schematics
to represent a capacitor.

\begin{eg}{A parallel-plate capacitor}\label{eg:capparplate}
\egquestion
Suppose a capacitor consists of two parallel metal plates with
area $A$, and the gap between them is $h$. The gap is small
compared to the dimensions of the plates. What is the
capacitance?

\eganswer
Since the plates are metal, the charges on each plate are free to
move, and will tend to cluster themselves more densely near the edges due to the
mutual repulsion of the other charges in the same plate. However, it turns out
that if the gap is small, this is a small effect, so we can get away with
assuming uniform charge density on each plate. 
The result of example \ref{eg:numparplate} then applies, and
for the region between the plates, we have $E=4\pi k\sigma=4\pi kq/ A$ and
$U_{e} = (1/8\pi k) E^2 Ah$. Substituting the
first expression into the second, we find $U_{e}=2\pi kq^2 h/ A$.
Comparing this to the definition of capacitance, we end up with
$C= A/4\pi kh$.

\end{eg}

<% marg(10) %>
<%
  fig(
    'coilshapes',
    %q{%
      Two common geometries for inductors. The cylindrical
      shape on the left is called a solenoid.\index{solenoid}
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'indsymbol',
    %q{The symbol for an inductor.}
  )
%>

  \spacebetweenfigs

<%
  fig(
    'inductors-photo',
    %q{Some inductors.}
  )
%>
<% end_marg %>

<% end_sec() %>

<% begin_sec("Inductors") %>

Any current will
create a magnetic field, so in fact every current-carrying
wire in a circuit acts as an inductor! 
However, this type of ``stray'' inductance is typically
negligible, just as we can usually ignore the stray resistance of
our wires and only take into account the actual resistors.
To store any appreciable amount of magnetic energy, one
usually uses a coil of wire designed specifically to be an inductor.
All the loops' contribution to the magnetic field add together
to make a stronger field. Unlike
capacitors and resistors, practical inductors are easy to make by
hand. One can for instance spool some wire around a short wooden dowel.
An inductor like this, in the form cylindrical coil of wire, is called
a solenoid, \figref{coilshapes}, and a stylized solenoid, \figref{indsymbol}, is the
symbol used to represent an inductor in a circuit regardless
of its actual geometry.

How much energy does an inductor store?
The energy density is proportional
to the square of the magnetic field strength, which is in
turn proportional to the current flowing through the
coiled wire, so the energy stored in the inductor must be
proportional to $I^2$. We write $L/2$ for the constant of
proportionality, giving
\begin{equation*}
                U_L    =    \frac{L}{2}I^2\eqquad.  
\end{equation*}

As in the definition of capacitance, we have a
factor of 1/2, which is purely a matter of definition. The
quantity $L$ is called the \emph{inductance}\index{inductance!defined} of the
inductor, and we see that its units must be joules per
ampere squared. This clumsy combination of units is more
commonly abbreviated as the henry, 1 henry = 1 $\zu{J}/\zu{A}^2$.
Rather than memorizing this definition, it makes more sense
to derive it when needed from the definition of inductance.
Many people know inductors simply as ``coils,'' or
``chokes,'' and will not understand you if you refer to an
``inductor,'' but they will still refer to $L$ as the
``inductance,'' not the ``coilance''  or ``chokeance!''

There is a lumped circuit approximation for inductors, just like the one
for capacitors (p. \pageref{lumped-circuit-approx}).
For a capacitor, this means assuming that
the electric fields are completely internal, so
that components only interact via currents that flow through wires,
not due to the physical overlapping of their fields in space. Similarly
for an inductor, the lumped circuit approximation is the assumption
that the magnetic fields are completely internal.

<% marg(0) %>
<%
  fig(
    'indseries',
    %q{Inductances in series add.}
  )
%>
<% end_marg %>

\begin{eg}{Identical inductances in series}
If two inductors are placed in series, any current that
passes through the combined double inductor must pass
through both its parts.
If we assume the lumped circuit approximation, the two inductors' fields
don't interfere with each other, so the energy is doubled for a given current.
Thus by the
definition of inductance, the inductance is doubled as well.
In general, inductances in series add, just like resistances.
The same kind of reasoning also shows that the inductance of
a solenoid is approximately proportional to its length,
assuming the number of turns per unit length is kept
constant. (This is only approximately true, because putting
two solenoids end-to-end causes the fields just outside
their mouths to overlap and add together in a complicated manner.
In other words, the lumped-circuit approximation may not be very
good.)
\end{eg}

<% marg(5) %>
<%
  fig(
    'capparallel',
    %q{Capacitances in parallel add.}
  )
%>
<% end_marg %>

\begin{eg}{Identical capacitances in parallel}
When two identical capacitances are placed in parallel, any
charge deposited at the terminals of the combined double
capacitor will divide itself evenly between the two parts.
The electric fields surrounding each capacitor will be half
the intensity, and therefore store one quarter the energy.
Two capacitors, each storing one quarter the energy, give
half the total energy storage. Since capacitance is
inversely related to energy storage, this implies that
identical capacitances in parallel give double the
capacitance. In general, capacitances in parallel add. This
is unlike the behavior of inductors and resistors, for which
series configurations give addition.

This is consistent with the result of example \ref{eg:capparplate},
which had the capacitance of a single parallel-plate capacitor
proportional to the area of the plates.
If we have two
parallel-plate capacitors, and we combine them in parallel
and bring them very close together side by side, we have
produced a single capacitor with plates of double the area, and
it has approximately double the capacitance, subject to any
violation of the lumped-circuit approximation due to the interaction
of the fields where the edges of the capacitors are joined together.
\end{eg}

Inductances in parallel and capacitances in series are explored
in homework problems \ref{hw:parallelinductors} and
\ref{hw:seriescapacitors}.

<% marg(0) %>
<%
  fig(
    'variablecap',
    %q{A variable capacitor.}
  )
%>
<% end_marg %>

\begin{eg}{A variable capacitor}
Figure \figref{variablecap}/1 shows the construction of a
variable capacitor out of two parallel semicircles of metal.
One plate is fixed, while the other can be rotated about
their common axis with a knob. The opposite charges on the
two plates are attracted to one another, and therefore tend
to gather in the overlapping area. This overlapping area,
then, is the only area that effectively contributes to the
capacitance, and turning the knob changes the capacitance.
The simple design can only provide very small capacitance
values, so in practice one usually uses a bank of capacitors,
wired in parallel, with all the moving parts on the same shaft.
\end{eg}
 %  

\startdqs

\begin{dq}
Suppose that two parallel-plate capacitors are wired in parallel, and
are placed very close together, side by side, so that the lumped circuit
approximation is not very accurate. Will the resulting capacitance be
too small, or too big? Could you twist the circuit into a different shape
and make the effect be the other way around, or make the effect vanish?
How about the case of two inductors in series?
\end{dq}
 % 

<% marg(5) %>
<%
  fig(
    'dielectric',
    %q{Discussion question \ref{dq:dielectric}.}
  )
%>
<% end_marg %>

\begin{dq}\label{dq:dielectric}
Most practical capacitors do not have an air gap or vacuum gap between the plates; instead,
they have an insulating substance called a dielectric. We can think of the molecules in this substance
as dipoles that are free to rotate (at least a little), but that are not free to move
around, since it is a solid. The figure shows a highly stylized and unrealistic way of
visualizing this. We imagine that all the dipoles are intially turned sideways, (1),
and that as the capacitor is charged, they all respond by turning through a certain angle, (2).
(In reality, the scene might be much more random, and the alignment effect much weaker.)

For simplicity, imagine inserting just one electric dipole
into the vacuum gap. For a given amount of charge on
the plates, how does this affect the amount of
energy stored in the electric field? How does this affect the capacitance?

Now redo the analysis in terms of the mechanical work needed in order to
charge up the plates.
\end{dq}

 % ------------------------------------------ 

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Oscillations") %>

 % 
Figure \figref{lrc} shows the simplest possible oscillating circuit.
For any useful application it would actually need to include
more components. For example, if it was a radio tuner, it
would need to be connected to an antenna and an amplifier.
Nevertheless, all the essential physics is there.

<% marg(0) %>
<%
  fig(
    'lrc',
    %q{A series LRC circuit.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'lrcanalogy',
    %q{A mechanical analogy for the LRC circuit.}
  )
%>
<% end_marg %>

We can analyze it without any sweat or tears whatsoever,
simply by constructing an analogy with a mechanical system.
In a mechanical oscillator, \figref{lrcanalogy}, we have two forms of stored energy,
\begin{align*}
                U_{spring}         &=            \frac{1}{2}kx^2                &(1)  \\
                K                         &=     \frac{1}{2}mv^2\eqquad.        \qquad                &(2)  
\end{align*}

In the case of a mechanical oscillator, 
we have usually assumed a friction force of the form that
turns out to give the nicest mathematical results,
$F=-bv$. In the circuit, the dissipation of energy
into heat occurs via the resistor, with no mechanical force
involved, so in order to make the analogy, we need to restate
the role of the friction force in terms of energy. The power
dissipated by friction equals the mechanical work it does in
a time interval $\der t$, divided by $\der t$,
$P=W/\der t=F\der x/\der t=Fv=-bv^2$, so
\begin{equation*}
                \text{rate of heat dissipation}  =  -bv^2\eqquad.        \qquad        (3)  
\end{equation*}

<% self_check('lrcsigns',<<-'SELF_CHECK'
Equation (1) has $x$ squared, and equations (2) and (3) have
$v$ squared. Because they're squared, the results don't
depend on whether these variables are positive or negative.
Does this make physical sense?
  SELF_CHECK
  ) %>

In the circuit, the stored forms of energy are
\begin{align*}
                U_C    &=    \frac{1}{2C}q^2                        &(1')  \\
                U_L    &=    \frac{1}{2}LI^2\eqquad, \qquad                        &(2')  
\end{align*}
and the rate of heat dissipation in the resistor is
\begin{equation*}
                \text{rate of heat dissipation}  =  -RI^2\eqquad.        \qquad        (3')  
\end{equation*}
Comparing the two sets of equations, we first form analogies
between quantities that represent the state of the system at
some moment in time:
\begin{align*}
        x        &\leftrightarrow q\\
        v        &\leftrightarrow I\\
\end{align*}

<% self_check('xvqi',<<-'SELF_CHECK'
How is $v$ related mathematically to $x$? How
is $I$ connected to $q$? Are the two relationships analogous?
  SELF_CHECK
  ) %>

Next we relate the ones that describe the system's
permanent characteristics:
\begin{align*}
        k        &\leftrightarrow 1/C\\
        m        &\leftrightarrow L\\
        b        &\leftrightarrow R
\end{align*}

Since the mechanical
system naturally oscillates with a frequency\footnote{As in chapter \ref{ch:2},
we use the word ``frequency'' to mean either $f$ or $\omega=2\pi f$
when the context makes it clear which is being referred to.}
 $\omega\approx\sqrt{k/m}$ , we can immediately solve the
electrical version by analogy, giving
\begin{equation*}
          \omega \approx \frac{1}{\sqrt{LC}}\eqquad.
\end{equation*}

Since the resistance $R$ is analogous to $b$ in the mechanical case, we
find that the $Q$ (quality factor, not charge) of the
resonance is inversely proportional to $R$, and the width of
the resonance is directly proportional to $R$.

\begin{eg}{Tuning a radio receiver}
A radio receiver uses this kind of circuit to pick out the
desired station. Since the receiver resonates at a
particular frequency, stations whose frequencies are far off
will not excite any response in the circuit. The value of
$R$ has to be small enough so that only one station at a
time is picked up, but big enough so that the tuner isn't
too touchy. The resonant frequency can be tuned by adjusting
either $L$ or $C$, but variable capacitors are easier to
build than variable inductors.
\end{eg}
\begin{eg}{A numerical calculation}
        The phone company sends more than one conversation at a
time over the same wire, which is accomplished by shifting
each voice signal into different range of frequencies during
transmission. The number of signals per wire can be maximized by making
each range of frequencies (known as a bandwidth) as small as
possible. It turns out that only a relatively narrow range
of frequencies is necessary in order to make a human voice
intelligible, so the phone company filters out all the
extreme highs and lows. (This is why your phone voice sounds
different from your normal voice.)

\egquestion
If the filter consists of an LRC circuit with a
broad resonance centered around 1.0 kHz, and the capacitor
is 1 $\mu\zu{F}$ (microfarad), what inductance value must be used?

\eganswer
Solving for $L$, we have
\begin{align*}
                         L         &=   \frac{1}{ C\omega^2}   \\
                                 &=    \frac{1}{(10^{-6}\ \zu{F})(2\pi\times10^3\ \zu{s}^{-1})^2}   \\
                                 &=     2.5\times10^{-3}\ \zu{F}^{-1}\zu{s}^2  
\end{align*}
Checking that these really are the same units as henries is
a little tedious, but it builds character:
\begin{align*}
                        \zu{F}^{-1}\zu{s}^2         &=    (\zu{C}^2/\zu{J})^{-1}\zu{s}^2          \\
                                 &=    \zu{J}\cdot\zu{C}^{-2}\zu{s}^2          \\
                                 &=    \zu{J}/\zu{A}^2  \\
                                 &=    \zu{H}          
\end{align*}
The result is 25 mH (millihenries).

        This is actually quite a large inductance value, and would
require a big, heavy, expensive coil. In fact, there is a
trick for making this kind of circuit small and cheap. There
is a kind of silicon chip called an op-amp, which, among
other things, can be used to simulate the behavior of an
inductor. The main limitation of the op-amp is that it is
restricted to low-power applications.\index{op-amp}\index{operational amplifier (op-amp)}
\end{eg}
 %  

 % ------------------------------------------ 

<% end_sec() %>

<% begin_sec("Voltage and current") %>

 % 
What is physically happening in one of these oscillating
circuits? Let's first look at the mechanical case, and then
draw the analogy to the circuit. For simplicity, let's
ignore the existence of damping, so there is no friction in
the mechanical oscillator, and no resistance in the electrical one.

Suppose we take the mechanical oscillator and pull the mass
away from equilibrium, then release it. Since friction tends
to resist the spring's force, we might naively expect that
having zero friction would allow the mass to leap instantaneously
to the equilibrium position. This can't happen, however,
because the mass would have to have infinite velocity in
order to make such an instantaneous leap. Infinite velocity
would require infinite kinetic energy, but the only kind of
energy that is available for conversion to kinetic is the
 energy stored in the spring, and that is finite,
not infinite. At each step on its way back to equilibrium,
the mass's velocity is controlled exactly by the amount of
the spring's energy that has so far been converted into kinetic energy. After the mass
reaches equilibrium, it overshoots due to its own momentum.
It performs identical oscillations on both sides of
equilibrium, and it never loses amplitude because friction
is not available to convert mechanical energy into heat.

Now with the electrical oscillator, the analog of position
is charge. Pulling the mass away from equilibrium is like
depositing charges $+q$ and $-q$ on the plates of the
capacitor. Since resistance tends to resist the flow of
charge, we might imagine that with no friction present, the
charge would instantly flow through the inductor (which is,
after all, just a piece of wire), and the capacitor would
discharge instantly. However, such an instant discharge is
impossible, because it would require infinite current for
one instant. Infinite current would create infinite magnetic
fields surrounding the inductor, and these fields would have
infinite energy. Instead, the rate of flow of current is
controlled at each instant by the relationship between the
amount of energy stored in the magnetic field and the amount
of current that must exist in order to have that strong a
field. After the capacitor reaches $q=0$, it overshoots. The
circuit has its own kind of electrical ``inertia,'' because
if charge was to stop flowing, there would have to be zero
current through the inductor. But the current in the
inductor must be related to the amount of energy stored in
its magnetic fields. When the capacitor is at $q=0$, all the
circuit's energy is in the inductor, so it must therefore
have strong magnetic fields surrounding it and quite a bit
of current going through it.

The only thing that might seem spooky here is that we used
to speak as if the current in the inductor caused the
magnetic field, but now it sounds as if the field causes the
current. Actually this is symptomatic of the elusive nature
of cause and effect in physics. It's equally valid to think
of the cause and effect relationship in either way. This may
seem unsatisfying, however, and for example does not really
get at the question of what brings about a voltage
difference across the resistor (in the case where the
resistance is finite); there must be such a voltage
difference, because without one, Ohm's law would predict
zero current through the resistor.

Voltage, then, is what is really missing from our story so far.

Let's start by studying the voltage across a capacitor.
Voltage is electrical potential energy per unit charge, so
the voltage difference between the two plates of the
capacitor is related to the amount by which its energy would
increase if we increased the absolute values of the charges on the plates
 from $q$ to $q+\der q$:
\begin{align*}
                V_C         &=    (U_{q+\der q}-U_q)/\der q   \\
                        &=        \frac{\der U_C}{\der q} \\
                        &= \frac{\der}{\der q}\left(\frac{1}{2C}q^2\right) \\
                        &= \frac{q}{C} 
\end{align*}
Many books
use this as the definition of capacitance. This equation, by
the way, probably explains the historical reason why $C$ was
defined so that the energy was \emph{inversely} proportional
to $C$ for a given value of $q$: the people who invented the
definition were thinking of a capacitor as a device for
storing charge rather than energy, and the amount of charge
stored for a fixed voltage (the charge ``capacity'') is
proportional to $C$.

<% marg(0) %>
<%
  fig(
    'indv',
    %q{The inductor releases energy and gives it to the black box.}
  )
%>
<% end_marg %>

In the case of an inductor, we know that if there is a
steady, constant current flowing through it, then the
magnetic field is constant, and so is the amount of energy
stored; no energy is being exchanged between the inductor and
any other circuit element. But what if the current is changing? The magnetic
field is proportional to the current, so a change in one
implies a change in the other. For concreteness, let's
imagine that the magnetic field and the current are both
decreasing. The energy stored in the magnetic field is
therefore decreasing, and by conservation of energy, this
energy can't just go away --- some other circuit element must
be taking energy from the inductor. The simplest example,
shown in figure \figref{indv}, is a series circuit consisting
of the inductor plus one other circuit element. It doesn't matter
what this other circuit element is, so we just call it a black box,
but if you like, we can think of it as a resistor, in which case
the energy lost by the inductor is being turned into heat by
the resistor. 
The junction rule tells us that both circuit
elements have the same current through them, so $I$ could refer
to either one, and likewise the loop rule tells us
$V_{inductor}+V_{black\ box}=0$, so the two voltage drops
have the same absolute value, which we can refer to as $V$.
Whatever the black box is, the rate at which it
is taking energy from the inductor is given by $|P|=|IV|$, so
\begin{align*}
        |IV| &= \left|\frac{\der U_L}{\der t}\right| \\
                &= \left|\frac{\der}{\der t}\left( \frac{1}{2}LI^2\right) \right| \\
                &= \left|LI\frac{\der I}{\der t}\right|\eqquad,\\
\intertext{or}
        |V|        &= \left|L\frac{\der I}{\der t}\right|\eqquad, \\
\end{align*}
which in many books is taken to be the definition of inductance. The
direction of the voltage drop (plus or minus sign) is such
that the inductor resists the change in current.

There's one very intriguing thing about this result. Suppose, for
concreteness, that the black box in figure \figref{indv} is a resistor,
and that the inductor's energy is decreasing, and being converted into heat
in the resistor. The voltage drop across the resistor indicates that
it has an electric field across it, which is driving the current.
But where is this electric field coming from? There are no charges anywhere
that could be creating it! What we've discovered is one special case of
a more general principle, the principle of induction: a changing magnetic
field creates an electric field, which is in addition to any electric
field created by charges.\index{induction} (The reverse is also true:
any electric field that changes over time creates a magnetic field.)
Induction forms the basis for such technologies as the generator\index{generator}
and the transformer,\index{transformer} and ultimately it leads to the
existence of light, which is a wave pattern in the electric and magnetic fields.
These are all topics for chapter \ref{ch:em}, but it's truly remarkable
that we could come to this conclusion without yet having learned any details about
magnetism.\label{inductorinduction}

<%
  fig(
    'inductor-voltage',
    %q{%
      Electric fields made by charges, 1, and by
      changing magnetic fields, 2 and 3.
    },
    {
      'width'=>'fullpage'
    }
  )
%>

The cartoons in figure \figref{inductor-voltage} compares electric fields made by charges, 1,
to electric fields made by changing magnetic fields, 2-3. In \figref{inductor-voltage}/1, two
physicists are in a room whose ceiling is positively charged and whose floor is negatively
charged. The physicist on the bottom throws a positively charged bowling ball into the curved
pipe. The physicist at the top uses a radar gun to measure the speed of the ball as it comes
out of the pipe. They find that the ball has slowed down by the time it gets to the top. By measuring the change in the
ball's kinetic energy, the two physicists are acting just like a voltmeter. They conclude that the
top of the tube is at a higher voltage than the bottom of the pipe. A difference in voltage
indicates an electric field, and this field is clearly being caused by the charges in the
floor and ceiling.

In \figref{inductor-voltage}/2, there are no charges anywhere in the room except for the
charged bowling ball. Moving charges make magnetic fields, so there is a magnetic field
surrounding the helical pipe while the ball is moving through it. A magnetic field has been
created where there was none before, and that field has energy. Where could the energy have
come from? It can only have come from the ball itself, so the ball must be losing kinetic energy.
The two physicists working together are again acting as a voltmeter, and again they conclude
that there is a voltage difference between the top and bottom of the pipe. This indicates
an electric field, but this electric field can't have been created by any charges, because
there aren't any in the room. This electric field was created by the change in the magnetic field.

The bottom physicist keeps on throwing balls into the pipe, until the pipe is full of balls,
\figref{inductor-voltage}/3, and finally a steady current is established.
While the pipe was filling up with balls, the energy in the
magnetic field was steadily increasing, and that energy was being stolen from the
balls' kinetic energy. But once a steady current is established, the energy in the magnetic
field is no longer changing. The balls no longer have to give up energy in order to
build up the field, and the physicist at the top finds that the balls are exiting the
pipe at full speed again. There is no voltage difference any more. Although there is
a current, $\der I/\der t$ is zero.

<% marg(-5) %>

<%
  fig(
    'ballasts',
    'Ballasts for fluorescent lights. Top: a big, heavy inductor used as a ballast in an
     old-fashioned fluorescent bulb. Bottom: a small solid-state ballast, built into the base
     of a modern compact fluorescent bulb.'
  )
%>
<% end_marg %>

\begin{eg}{Ballasts}\label{eg:ballast}\index{ballast}\index{gas discharge tube}\index{fluorescent light}
In a gas discharge tube, such as
a neon sign, enough voltage is applied to a tube full of gas to ionize some of the atoms
in the gas. Once ions have been created, the voltage accelerates them, and they
strike other atoms, ionizing them as well and resulting in a chain reaction. This is a spark,
like a bolt of lightning. But once the spark starts up,
the device begins to act as though it has no resistance: more and more current flows, without
the need to apply any more voltage. The power, $P=IV$, would grow without limit, and the tube 
would burn itself out.

The simplest solution is to connect an inductor, known as the ``ballast,'' in series with the
tube, and run the whole thing on an AC voltage. During each cycle, as the voltage reaches the
point where the chain reaction begins, there is a surge of current, but the inductor resists
such a sudden change of current, and the energy that would otherwise have burned out the bulb
is instead channeled into building a magnetic field.

A common household fluorescent lightbulb consists of a gas discharge tube in which the 
glass is coated with a fluorescent material. The gas in the tube emits ultraviolet light,
which is absorbed by the coating, and the coating then glows in the visible spectrum.

Until recently, it was common for a fluroescent light's ballast to be a simple inductor,
and for the whole device to be operated at the 60 Hz frequency of the electrical power lines. This caused
the lights to flicker annoyingly at 120 Hz, and could also cause an audible hum, since
the magnetic field surrounding the inductor could exert mechanical forces on things.
Modern compact fluorescent bulbs have ballasts built into their bases that use
a frequency in the kilohertz range, eliminating the flicker and    
hum. 
\end{eg}

\startdq

\begin{dq}
What happens when the physicist at the bottom in figure \figref{inductor-voltage}/3 starts
getting tired, and decreases the current?
\end{dq}

<% end_sec() %>

<% begin_sec("Decay") %>

Up until now I've soft-pedaled the fact that by changing the
characteristics of an oscillator, it is possible to produce
non-oscillatory behavior. For example, imagine taking the
mass-on-a-spring system and making the spring weaker and
weaker. In the limit of small $k$, it's as though there was
no spring whatsoever, and the behavior of the system is that
if you kick the mass, it simply starts slowing down. For
friction proportional to $v$, as we've been assuming, the
result is that the velocity approaches zero, but never
actually reaches zero. This is unrealistic for the
mechanical oscillator, which will not have vanishing
friction at low velocities, but it is quite realistic in the
case of an electrical circuit, for which the voltage drop
across the resistor really does approach zero as the
current approaches zero.

<% marg(30) %>
<%
  fig(
    'rc',
    %q{An RC circuit.}
  )
%>
<% end_marg %>

We do not even have to reduce $k$ to exactly zero in order
to get non-oscillatory behavior. There is actually a finite,
critical value below which the behavior changes, so that the
mass never even makes it through one cycle. This is the case
of overdamping, discussed on page \pageref{overdamped}.\index{oscillations!overdamped!electrical}%
        \index{overdamped oscillations!electrical}\index{damped oscillations!overdamped!electrical}

Electrical circuits can exhibit all the same behavior. 
For simplicity we will analyze only the
cases where either the
capacitor or the inductor is completely absent, giving $Q=0$.

<% begin_sec("The RC circuit") %>
\index{RC circuit}
We first analyze the RC circuit, \figref{rc}. In reality one would
have to ``kick'' the circuit, for example by briefly
inserting a battery, in order to get any interesting
behavior. We start with Ohm's law and the equation for the
voltage across a capacitor:
\begin{align*}
                V_R         &=    IR  \\
                V_C         &=    q/C  
\end{align*}

The loop rule tells us
\begin{equation*}
                        V_R + V_C  =  0\eqquad,  
\end{equation*}
and combining the three equations results in a relationship between $q$ and $I$:
\begin{equation*}
                        I         =      -\frac{1}{RC}q
\end{equation*}
The negative sign tells us that the current tends to reduce
the charge on the capacitor, i.e., to discharge it. It makes
sense that the current is proportional to $q$\/:  if $q$ is
large, then the attractive forces between the $+q$ and $-q$
charges on the plates of the capacitor are large, and
charges will flow more quickly through the resistor in order
to reunite. If there was zero charge on the capacitor
plates, there would be no reason for current to flow. Since
amperes, the unit of current, are the same as coulombs per
second, it appears that the quantity $RC$ must have
units of seconds, and you can check for yourself that this
is correct. $RC$ is therefore referred to as the time
constant of the circuit.\index{RC time constant}\index{time constant!RC}

How exactly do $I$ and $q$ vary with time? Rewriting $I$ as
$\der q/\der t$, we have
\begin{equation*}
                \frac{\der q}{\der t} = -\frac{1}{RC}q\eqquad.
\end{equation*}
We need a function $q(t)$ whose derivative
equals itself, but multiplied by a negative constant.
A function of the form $ae^t$, where $e=2.718...$ is the base of natural logarithms,
is the only one that has its derivative equal to itself, and
$ae^{bt}$ has its derivative equal to itself multiplied by $b$. Thus
our solution is
\begin{equation*}
                q = q_\zu{o}\exp\left(-\frac{t}{RC}\right)\eqquad.
\end{equation*}

<% marg(0) %>
<%
  fig(
    'rcdecay',
    %q{%
      Over a time interval $RC$, the charge on the capacitor
      is reduced by a factor of $e$.
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'rl',
    %q{An RL circuit.}
  )
%>
<% end_marg %>

<% end_sec() %>

<% begin_sec("The RL circuit") %>
\index{RL circuit}
The RL circuit, \figref{rl}, can be attacked by similar methods, and
it can easily be shown that it gives
\begin{equation*}
                I = I_\zu{o}\exp\left(-\frac{R}{L}t\right)\eqquad.
\end{equation*}
The RL time constant equals $L/R$.

\begin{eg}{Death by solenoid; spark plugs}\index{spark plug}
When we suddenly break an RL circuit, what will happen?
It might seem that we're faced with a paradox, since we only
have two forms of energy, magnetic energy and heat, and if the
current stops suddenly, the magnetic field must collapse suddenly.
But where does the lost magnetic energy go? It can't go into
resistive heating of the resistor, because the circuit has now
been broken, and current can't flow!

The way out of this conundrum is to recognize that the open gap
in the circuit has a resistance which is large, but not infinite.
This large resistance causes the RL time constant $L/ R$ to be very
small. The current thus continues to flow for a very brief time, and
flows straight across the air gap where the circuit has been opened.
In other words, there is a spark!

We can determine based on several different lines of
reasoning that the voltage drop from one end of the spark to the other must be
very large. First, the air's resistance is large, so $V= IR$ requires a large
voltage. We can also reason that all the energy in the magnetic field is
being dissipated in a short time, so the power dissipated in the spark, $P= IV$,
is large, and this requires a large value of $V$. ($I$ isn't large --- it is
decreasing from its initial value.) Yet a third way to reach the same result
is to consider the equation $V_{L}=\der I/\der t$\/: since the time constant
is short, the time derivative $\der I/\der t$ is large.

This is exactly how a car's spark plugs work. Another application is to
electrical safety: it can be dangerous to break an inductive circuit
suddenly, because so much energy is released in a short time. There is
also no guarantee that the spark will discharge across the air gap; it
might go through your body instead, since your body might have a lower
resistance.
\end{eg}

<% marg(-10) %>

<%
  fig(
    'spark-gap-transmitter',
    'Example \ref{eg:spark-gap-transmitter}.'
  )
%>
<% end_marg %>

\begin{eg}{A spark-gap radio transmitter}\label{eg:spark-gap-transmitter}
Figure \figref{spark-gap-transmitter} shows a primitive type of radio transmitter, called
a spark gap transmitter, used to send Morse code around the turn of the twentieth century. The high voltage
source, V, is typically about 10,000 volts. When the telegraph
switch, S, is closed, the RC
circuit on the left starts charging up. An increasing voltage difference develops between the electrodes of the spark gap, G.
When this voltage difference gets large enough, the electric field in the air between the electrodes
causes a spark, partially discharging the RC circuit, but charging the LC circuit on the right.
The LC circuit then oscillates at its resonant frequency (typically about 1 MHz), but the energy of these oscillations is
rapidly radiated away by the antenna, A, which sends out radio waves (chapter \ref{ch:em}).
\end{eg}

\startdqs

\begin{dq}
A gopher gnaws through one of the wires in the DC lighting system in
your front yard, and the lights turn off. At the instant when the circuit becomes open, we can
consider the bare ends of the wire to be like the plates of a capacitor,
with an air gap (or gopher gap) between them. What kind of capacitance
value are we talking about here? What would this tell you about the
$RC$ time constant?
\end{dq}
 %  

 % ------------------------------------------ 

<% end_sec() %>

<% end_sec() %>

\pagebreak

<% begin_sec("Review of complex numbers",nil,'complex-numbers') %>
\index{complex numbers}
For a more detailed treatment of complex numbers, see ch. 3 of
James Nearing's free book at \url{physics.miami.edu/~nearing/mathmethods}.

<% marg(50) %>
<%
  fig(
    'complex-numbers',
    %q{Visualizing complex numbers as points in a plane.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'complex-addition',
    %q{%
      Addition of complex numbers is just like addition of vectors,
      although the real and imaginary axes don't actually represent directions in space.
    }
  )
%>
\spacebetweenfigs
<%
  fig(
    'complex-conjugate',
    %q{A complex number and its conjugate.}
  )
%>
<% end_marg %>

We assume there is a number, $i$, such that $i^2=-1$.
The square roots of $-1$ are then $i$ and $-i$. (In electrical engineering work,
where $i$ stands for current, $j$ is sometimes used instead.) This gives rise
to a number system, called the complex numbers, containing the real numbers as a subset.
Any complex number $z$ can be written in the form $z=a+bi$, where $a$ and $b$ are
real, and $a$ and $b$ are then referred to as the real and imaginary parts of $z$.
A number with a zero real part is called an imaginary number.
The complex numbers can be visualized as a plane, with the real number line placed
horizontally like the $x$ axis of the familiar $x-y$ plane, and the imaginary numbers running
along the $y$ axis. The complex numbers are complete in a way that the real numbers
aren't: every nonzero complex number has two square roots. For example, 1 is
a real number, so it is also a member of the complex numbers, and its square roots
are $-1$ and 1. Likewise, $-1$ has square roots $i$ and $-i$, and the number $i$
has square roots $1/\sqrt{2}+i/\sqrt{2}$ and $-1/\sqrt{2}-i/\sqrt{2}$.

Complex numbers can be added and subtracted by adding or subtracting their real
and imaginary parts. Geometrically, this is the same as vector addition.

The complex numbers $a+bi$ and $a-bi$, lying at equal distances above and below the
real axis, are called complex conjugates. The results of the quadratic formula
are either both real, or complex conjugates of each other.
The complex conjugate of a number $z$ is notated as $\bar{z}$ or
$z^*$.

The complex numbers obey all the same rules of arithmetic as the reals, except that
they can't be ordered along a single line. That is, it's not possible to say whether
one complex number is greater than another. We can compare them in terms of their
magnitudes (their distances from the origin), but two distinct complex numbers may
have the same magnitude, so, for example, we can't say whether $1$ is greater than
$i$ or $i$ is greater than $1$.

\begin{eg}{A square root of $i$}\label{eg:sqrt-i}
\egquestion Prove that $1/\sqrt{2}+i/\sqrt{2}$ is a square root of $i$.

\eganswer Our proof can use any ordinary rules of arithmetic, except for
ordering.
\begin{align*}
   (\frac{1}{\sqrt{2}}+\frac{i}{\sqrt{2}})^2 
    & = \frac{1}{\sqrt{2}}\cdot\frac{1}{\sqrt{2}}
       +\frac{1}{\sqrt{2}}\cdot\frac{i}{\sqrt{2}}
       +\frac{i}{\sqrt{2}}\cdot\frac{1}{\sqrt{2}}
       +\frac{i}{\sqrt{2}}\cdot\frac{i}{\sqrt{2}} \\
    &= \frac{1}{2}(1+i+i-1) \\
    &= i
\end{align*}
\end{eg}

Example \ref{eg:sqrt-i} showed one method of multiplying complex numbers.
However, there is another nice interpretation of complex multiplication.
We define the argument of a complex number as its angle in the complex plane, measured
counterclockwise from the positive real axis.
Multiplying two complex numbers then corresponds to multiplying their magnitudes,
and adding their arguments.

<% self_check('complex-square-root',<<-'SELF_CHECK'
Using this interpretation of multiplication, how could you find the
square roots of a complex number?
  SELF_CHECK
  ) %>

<% marg(50) %>
<%
  fig(
    'complex-polar',
    %q{%
      A complex number can be described in terms of its magnitude and
      argument.
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'complex-multiplication',
    %q{The argument of $uv$ is the sum of the arguments of $u$ and $v$.}
  )
%>
<% end_marg %>


\begin{eg}{An identity} 
The magnitude $|z|$ of a complex number $z$ obeys
the identity $|z|^2=z\bar{z}$. To prove this, we first note that $\bar{z}$
has the same magnitude as $z$, since flipping it to the other side of the
real axis doesn't change its distance from the origin. Multiplying $z$ by
$\bar{z}$ gives a result whose magnitude is found by multiplying their
magnitudes, so the magnitude of
$z\bar{z}$ must therefore equal  $|z|^2$. Now we just have to prove that
$z\bar{z}$ is a positive real number. But if, for example, $z$ lies counterclockwise
from the real axis, then $\bar{z}$ lies clockwise from it. If $z$ has a positive
argument, then $\bar{z}$ has a negative one, or vice-versa. The sum of their arguments is therefore
zero, so the result has an argument of zero, and is on the positive real axis.
\footnote{I cheated a little. If $z$'s argument
is 30 degrees, then we could say $\bar{z}$'s was -30, but we could also call it
330. That's OK, because 330+30 gives 360, and an argument of
360 is the same as an argument of zero.}
\end{eg}

This whole system was built up in order to make every number have square roots.
What about cube roots, fourth roots, and so on? Does it get even more weird when
you want to do those as well? No. The complex number system we've already discussed
is sufficient to handle all of them. The nicest way of thinking about it is in terms
of roots of polynomials. In the real number system, the polynomial $x^2-1$ has
two roots, i.e., two values of $x$ (plus and minus one) that we can plug in to the
polynomial and get zero. Because it has these two real roots, we can rewrite the
polynomial as $(x-1)(x+1)$. However, the polynomial $x^2+1$ has no real roots. It's
ugly that in the real number system, some second-order polynomials have two
roots, and can be factored, while others can't. In the complex number system,
they all can. For instance, $x^2+1$ has roots $i$ and $-i$, and can be factored
as $(x-i)(x+i)$. In general, the fundamental theorem of algebra\index{fundamental theorem of algebra}
states that in the complex number system,
any nth-order polynomial can be factored completely
into $n$ linear factors, and we can also say that it has $n$ complex roots,
with the understanding that some of the roots may be the same. For instance,
the fourth-order polynomial $x^4+x^2$ can be factored as $(x-i)(x+i)(x-0)(x-0)$,
and we say that it has four roots, $i$, $-i$, 0, and 0, two of which happen to be
the same. This is a sensible way to think about it, because in real life, numbers are
always approximations anyway, and if we make tiny, random changes
to the coefficients of this polynomial,
it will have four distinct roots, of which two just happen to be very close
to zero.

\startdqs

\begin{dq}
Find $\arg i$, $\arg(-i)$, and $\arg 37$, where $\arg z$ denotes the argument of the complex number $z$.
\end{dq}

\begin{dq}
Visualize the following multiplications in the complex plane using the interpretation of multiplication
in terms of multiplying magnitudes and adding arguments: $(i)(i)=-1$, $(i)(-i)=1$, $(-i)(-i)=-1$.
\end{dq}

\begin{dq}
If we visualize $z$ as a point in the complex plane, how should we visualize $-z$? What does this mean
in terms of arguments? Give similar interpretations for $z^2$ and $\sqrt{z}$.
\end{dq}

\begin{dq}
Find four different complex numbers $z$ such that $z^4=1$.
\end{dq}

\begin{dq}
Compute the following. For the final two, use the magnitude and argument, not the real and imaginary parts.
\begin{equation*}
|1+i| \quad , \quad
 \arg(1+i) \quad , \quad
 \left|\frac{1}{1+i}\right| \quad , \quad
 \arg\left(\frac{1}{1+i}\right) \quad , \quad
\end{equation*}
From these, find the real and imaginary parts of $1/(1+i)$.
\end{dq}

<% end_sec() %>

<% begin_sec("Euler's formula",nil,'euler-formula') %>
\index{Euler's formula}\index{Euler, Leonhard}
Having expanded our horizons to include the complex numbers, it's natural to want to extend
functions we knew and loved from the world of real numbers so that they can also operate on
complex numbers. The only really natural way to do this in general is to use Taylor series.
A particularly beautiful thing happens with the functions $e^x$, $\sin x$, and $\cos x$:
\begin{align*}
  e^x    &= 1 + \frac{1}{2!}x^2 + \frac{1}{3!}x^3 + \ldots \\
  \cos x &= 1 - \frac{1}{2!}x^2 + \frac{1}{4!}x^4 - \ldots \\
  \sin x &= x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \ldots 
\end{align*}
If $x=i\phi$ is an imaginary number, we have
\begin{equation*}
  e^{i\phi} = \cos \phi + i \sin \phi\eqquad,
\end{equation*}
a result known as Euler's formula.\index{Euler's formula}
The geometrical interpretation in the complex
plane is shown in figure \figref{euler}.

<% marg(100) %>
<%
  fig(
    'euler',
    'The complex number $e^{i\phi}$ lies on the unit circle.'
  )
%>

\spacebetweenfigs

<%
  fig(
    'euler-portrait',
    'Leonhard Euler (1707-1783)'
  )
%>
<% end_marg %>

Although the result may seem like something out of a freak show at first,
applying the definition of the exponential function
makes it clear how natural it is:
\begin{align*}
  e^x = \lim_{n\rightarrow \infty} \left(1+\frac{x}{n}\right)^n\eqquad.
\end{align*}
When $x=i\phi$ is imaginary, the quantity $(1+i\phi/n)$ represents a number
lying just above 1 in the complex plane. For large $n$, $(1+i\phi/n)$
becomes very close to the unit circle, and its argument is the small
angle $\phi/n$. Raising this number to the nth power multiplies its
argument by $n$, giving a number with an argument of $\phi$.

Euler's formula is used frequently in physics and engineering.

\begin{eg}{Trig functions in terms of complex exponentials}
\egquestion Write the sine and cosine functions in terms of exponentials.

\eganswer Euler's formula for $x=-i\phi$ gives $\cos \phi - i \sin \phi$,
since $\cos(-\theta)=\cos\theta$, and $\sin(-\theta)=-\sin\theta$.
\begin{align*}
  \cos x &= \frac{e^{ix}+e^{-ix}}{2} \\
  \sin x &= \frac{e^{ix}-e^{-ix}}{2i} 
\end{align*}
\end{eg}

\begin{eg}{A hard integral made easy}
\egquestion Evaluate
\begin{equation*}
  \int e^x \cos x \der x
\end{equation*}

\eganswer This seemingly impossible integral becomes easy if we rewrite
the cosine in terms of exponentials:
\begin{align*}
  \int e^x & \cos x \der x \\
      &= \int e^x \left(\frac{e^{ix}+e^{-ix}}{2}\right) \der x \\
      &= \frac{1}{2} \int (e^{(1+i)x}+e^{(1-i)x})\der x \\
      &= \frac{1}{2} \left( \frac{e^{(1+i)x}}{1+i}+\frac{e^{(1-i)x}}{1-i} \right)+ c
\end{align*}

Since this result is the integral of a real-valued function, we'd like it to be
real, and in fact it is, since the first and second terms are complex conjugates of
one another. If we wanted to, we could use Euler's theorem to convert it back to
a manifestly real result.\footnote{In general, the use of complex number techniques to
do an integral could result in a complex number, but that complex number would
be a constant, which could be subsumed within the usual constant of integration.}

\end{eg}

<% end_sec('euler-formula') %>

<% begin_sec("Impedance",nil,'impedance') %>
\index{impedance}
 % 
So far we have been thinking in terms of the free oscillations
of a circuit. This is like a mechanical oscillator that has
been kicked but then left to oscillate on its own without
any external force to keep the vibrations from dying out.
Suppose an LRC circuit is driven with a sinusoidally varying
voltage, such as will occur when a radio tuner is hooked up
to a receiving antenna. We know that a current will flow in
the circuit, and we know that there will be resonant
behavior, but it is not necessarily simple to relate current
to voltage in the most general case. Let's start instead
with the special cases of LRC circuits consisting of only a
resistance, only a capacitance, or only an inductance. We
are interested only in the steady-state response.

The purely resistive case is easy. Ohm's law gives
\begin{equation*}
                I         =    \frac{V}{R}\eqquad.  
\end{equation*}

In the purely capacitive case, the relation $V=q/C$ lets us calculate
\begin{align*}
                I         &=    \frac{\der q}{\der t}  \\
                         &=    C \frac{\der V}{\der t}\eqquad.  
\end{align*}

This is partly analogous to Ohm's law. For example, if we double the amplitude of a sinusoidally
varying AC voltage, the derivative $\der V/\der t$ will also double, and the amplitude of the
sinusoidally varying current will also double. However, it is not true that $I=V/R$, because
taking the derivative of a sinusoidal function shifts its phase by 90 degrees.
If the voltage varies as, for example, $V(t)=V_\zu{o}\sin (\omega t)$,
then the current will be $I(t)=\omega C V_\zu{o}\cos (\omega t)$.
The amplitude of the current is $\omega C V_\zu{o}$, which is proportional to $V_\zu{o}$, but
it's not true that $I(t)=V(t)/R$ for some constant $R$.

A second problem that crops up is that our entire analysis of DC resistive circuits was built
on the foundation of the loop rule and the junction rule, both of which are statements about
sums. To apply the junction rule to an AC circuit, for exampe, we would say that the sum of the
sine waves describing the currents coming into the junction is equal (at every moment in time)
to the sum of the sine waves going out.         Now sinusoidal functions have a remarkable property, which is that if you
add two different sinusoidal functions having the same frequency, the result
is also a sinusoid with that frequency.
For example, $\cos\omega t+\sin\omega t=\sqrt{2}\sin(\omega t+\pi/4)$, which can be proved
using trig identities. The trig identities can get very cumbersome, however, and there is
a much easier technique involving complex numbers.

Figure \figref{polar} shows a useful way to visualize what's going on.
When a circuit is oscillating at a frequency $\omega$, we use points in
the plane to represent sinusoidal functions with various phases and
amplitudes.

<% self_check('represent-as-complex',<<-'SELF_CHECK'
Which of the following functions can be represented in this way? $\cos(6t-4)$, $\cos^2t$, $\tan t$
  SELF_CHECK
  ) %>

<% marg(0) %>
<%
  fig(
    'capvi',
    %q{In a capacitor, the current is $90\degunit$ ahead of the voltage in phase.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'polar',
    %q{Representing functions with points in polar coordinates.}
  )
%>

\spacebetweenfigs
%

<%
  fig(
    'sinpluscos',
    %q{Adding two sinusoidal functions.}
  )
%>
<% end_marg %>

The simplest examples of how to
visualize this in polar coordinates are ones like $\cos \omega t+\cos \omega t=2\cos \omega t$,
where everything has the same phase, so all the points
lie along a single line in the polar plot, and addition is just like adding numbers
on the number line.
The less trivial example $\cos\omega t+\sin\omega t=\sqrt{2}\sin(\omega t+\pi/4)$,
can be visualized as in figure \figref{sinpluscos}.

Figure  \figref{sinpluscos} suggests that
all of this can be tied together nicely if we identify our plane with the plane
of complex numbers. For example, the complex numbers 1 and $i$ represent the
functions $\sin\omega t$ and $\cos\omega t$.
In figure \figref{capvi}, for example, the voltage across the capacitor
is a sine wave multiplied by a number that gives its amplitude, so we associate that
function with a number $\tilde{V}$ lying on the real axis. Its magnitude, $|\tilde{V}|$,
gives the amplitude in units of volts, while its argument $\arg \tilde{V}$, gives its
phase angle, which is zero. The current is a multiple of the cosine, so we identify
it with a number $\tilde{I}$ lying on the imaginary axis.
We have $\arg\tilde{I}=90\degunit$, and $|\tilde{I}|$ is the amplitude of the current,
in units of amperes. But comparing with our result above, we have $|\tilde{I}|=\omega C|\tilde{V}|$.
Bringing together the phase and magnitude information, we have $\tilde{I}=i\omega C\tilde{V}$.
This looks very much like Ohm's law, so we write
\begin{equation*}
                \tilde{I}         =    \frac{\tilde{V}}{Z_C}\eqquad,  
\end{equation*}
where the quantity
\begin{equation*}
                Z_C    =     -\frac{i}{\omega C}\eqquad, \qquad \text{[impedance of a capacitor]}  
\end{equation*}
having units of ohms, is called the \emph{impedance} of
the capacitor at this frequency.

It makes sense that
the impedance becomes infinite at zero frequency. Zero
frequency means that it would take an infinite time before
the voltage would change by any amount. In other words, this
is like a situation where the capacitor has been connected
across the terminals of a battery and been allowed to settle
down to a state where there is constant charge on both
terminals. Since the electric fields between the plates are
constant, there is no energy being added to or taken out of
the field. A capacitor that can't exchange energy with any
other circuit component is nothing more than a broken (open) circuit.

Note that we have two types of complex numbers: those that represent sinusoidal functions
of time, and those that represent impedances.
The ones that represent sinusoidal functions have tildes on top, which look like little sine
waves.

<% self_check('caplabel',<<-'SELF_CHECK'
Why can't a capacitor have its impedance printed on it along
with its capacitance?
  SELF_CHECK
  ) %>

<% marg(0) %>
<%
  fig(
    'indvi',
    %q{%
      The current through an inductor lags behind the voltage by
      a phase angle of $90\degunit$.
    }
  )
%>
<% end_marg %>

Similar math (but this time with an integral instead
of a derivative) gives
\begin{equation*}
                Z_L    =    i\omega L                    \qquad  \text{[impedance of an inductor]}  
\end{equation*}
for an inductor.\index{impedance!of an inductor} It makes sense that the inductor has lower
impedance at lower frequencies, since at zero frequency
there is no change in the magnetic field over time. No
energy is added to or released from the magnetic field, so
there are no induction effects, and the inductor acts just
like a piece of wire with negligible resistance. The term
``choke'' for an inductor refers to its ability to ``choke
out'' high frequencies.

The phase relationships shown in figures \figref{capvi} and \figref{indvi}
can be remembered using my own mnemonic, ``eVIL,'' which shows that the voltage
(V) leads the current (I) in an inductive circuit, while the opposite is
true in a capacitive one. A more traditional mnemonic is ``ELI the ICE man,''
which uses the notation E for emf, a concept closely related to voltage
(see p. \pageref{emf-term-introduced}).

Summarizing, the impedances of resistors, capacitors, and inductors are
        \begin{align*}
                Z_R        &= R\\
                Z_C &= -\frac{i}{\omega C}\\
                Z_L &= i\omega L\eqquad.
        \end{align*}

\begin{eg}{Low-pass and high-pass filters}
        An LRC circuit only responds to a certain range (band) of
frequencies centered around its resonant frequency. As a
filter, this is known as a bandpass filter. If you turn down
both the bass and the treble on your stereo, you have
created a bandpass filter.

        To create a high-pass or low-pass filter, we only need to
insert a capacitor or inductor, respectively, in series. For
instance, a very basic surge protector for a computer could
be constructed by inserting an inductor in series with the
computer. The desired 60 Hz power from the wall is
relatively low in frequency, while the surges that can
damage your computer show much more rapid time variation.
Even if the surges are not sinusoidal signals, we can think
of a rapid ``spike'' qualitatively as if it was very high in
frequency --- like a high-frequency sine wave, it changes very rapidly.

        Inductors tend to be big, heavy, expensive circuit
elements, so a simple surge protector would be more likely
to consist of a capacitor in \emph{parallel} with the
computer. (In fact one would normally just connect one side
of the power circuit to ground via a capacitor.) The
capacitor has a very high impedance at the low frequency of
the desired 60 Hz signal, so it siphons off very little of
the current. But for a high-frequency signal, the capacitor's
impedance is very small, and it acts like a zero-impedance,
easy path into which the current is diverted. 
\end{eg}

The main things to be careful about with impedance are that
(1) the concept only applies to a circuit that is being
driven sinusoidally, (2) the impedance of an inductor or
capacitor is frequency-dependent.

\startdq

\begin{dq}
Figure \figref{capvi} on page \pageref{fig:capvi}
shows the voltage and current for a capacitor.
Sketch the $q$-$t$ graph, and use it to give a physical
explanation of the phase relationship between the voltage and current. For example,
why is the current zero when the voltage is at a maximum or minimum?
\end{dq}

\begin{dq}
Figure \figref{indvi} on page \pageref{fig:indvi}
shows the voltage and current for an inductor. The power is
considered to be positive when energy is being put into the inductor's magnetic field.
Sketch the graph of the power, and then the graph of $U$, the energy stored in the magnetic field, and use it to give a physical
explanation of the $P$-$t$ graph. In particular, discuss why the frequency is doubled on
the $P$-$t$ graph.
\end{dq}

\begin{dq}
Relate the features of the graph in figure \figref{indvi} on page \pageref{fig:indvi}
to the story told in cartoons in figure \figref{inductor-voltage}/2-3 on page \pageref{fig:inductor-voltage}.
\end{dq}

<% end_sec() %>

<% begin_sec("Power") %>

How much power is delivered when an oscillating voltage
is applied to an impedance? The equation $P=IV$ is generally true,
since voltage is defined as energy per unit charge, and current is
defined as charge per unit time: multiplying them gives energy
per unit time. In a DC circuit, all three quantities were constant,
but in an oscillating (AC) circuit, all three display time variation.

<% begin_sec("A resistor") %>

 First let's examine the case of
a resistor. For instance, you're probably reading this book from
a piece of paper illuminated by a glowing lightbulb, which is
driven by an oscillating voltage with amplitude $V_\zu{o}$. 
In the special case of a resistor, we know that $I$ and $V$ are in phase.
For example, if $V$ varies as $V_\zu{o}\cos \omega t$, then $I$ will be a cosine as well,
$I_\zu{o}\cos \omega t$. The power is then $I_\zu{o}V_\zu{o}\cos^2\omega t$, which
is always positive,\footnote{A resistor always turns electrical energy into heat. It never
turns heat into electrical energy!} and varies between 0 and $I_\zu{o}V_\zu{o}$.
Even if the time variation was $\cos\omega t$ or $\sin(\omega t+\pi/4)$,
we would still have a maximum power of $I_\zu{o}V_\zu{o}$, because both the voltage and the
current would reach their maxima at the same time. In a lightbulb, the moment of maximum
power is when the circuit is most rapidly heating the filament. At the
instant when $P=0$, a quarter of a cycle later, no current is flowing, and no electrical
energy is being turned into heat. Throughout the whole cycle, the filament is getting rid
of energy by radiating light.\footnote{To many people, the word ``radiation'' implies nuclear
contamination. Actually, the word simply means something that ``radiates'' outward.
Natural sunlight is ``radiation.'' So is the light from a lightbulb, or the infrared light
being emitted by your skin right now.}
Since the circuit oscillates at a frequency\footnote{Note that this time ``frequency''
means $f$, not $\omega$! Physicists and engineers generally use $\omega$ because it
simplifies the equations, but electricians and technicians always use $f$. The 60 Hz
frequency is for the U.S.} of $60\ \zu{Hz}$, the temperature doesn't really have time to
cycle up or down very much over the 1/60 s period of the oscillation, and we don't notice
any significant variation in the brightness of the light, even with a short-exposure
photograph.

<% marg(70) %>
<%
  fig(
    'resvip',
    %q{%
      Power in a resistor: the rate at which electrical energy is being
      converted into heat.
    }
  )
%>
<% end_marg %>

Thus, what we really want to know is the average power, ``average'' meaning
the average over one full cycle. Since we're covering a whole cycle with our average, it
doesn't matter what phase we assume. Let's use a cosine. The total amount of energy
transferred over one cycle is
\begin{align*}
        E        &=        \int \der E \\
                &= \int_0^T \frac{\der E}{\der t} \der t\eqquad, \\
\intertext{where $T=2\pi/\omega$ is the period.}
        E        &= \int_0^T P \der t \\
                &= \int_0^T P \der t \\
                &= \int_0^T I_\zu{o}V_\zu{o} \cos^2\omega t \der t \\
                &= I_\zu{o}V_\zu{o} \int_0^T  \cos^2\omega t \der t \\
                &= I_\zu{o}V_\zu{o} \int_0^T \frac{1}{2} \left(1+\cos 2\omega t\right)  \der t \\
\intertext{The reason for using the trig identity $\cos^2 x=(1+\cos 2 x)/2$ in the last step is that
it lets us get the answer without doing a hard integral. Over the course of one full
cycle, the quantity $\cos 2\omega t$ goes positive, negative, positive, and negative again,
so the integral of it is zero. We then have}
        E        &= I_\zu{o}V_\zu{o} \int_0^T \frac{1}{2}   \der t \\
                &= \frac{I_\zu{o}V_\zu{o}T}{2}
\end{align*}
The average power is 
\begin{align*}
        P_{av}        &= \frac{\text{energy transferred in one full cycle}}{\text{time for one full cycle}} \\
                        &= \frac{I_\zu{o}V_\zu{o}T/2}{T} \\
                        &= \frac{I_\zu{o}V_\zu{o}}{2}\eqquad,\\
\end{align*}
i.e., the average is half the maximum. The power
varies from $0$ to $I_\zu{o}V_\zu{o}$, and it spends equal amounts of time above and below the
maximum, so it isn't surprising that the average power is half-way in between zero and
the maximum. Summarizing, we have
\begin{align*}
        P_{av} &= \frac{I_\zu{o}V_\zu{o}}{2} \qquad \text{[average power in a resistor]}\\
\end{align*}
for a resistor.

<% end_sec() %>

<% begin_sec("RMS quantities") %>

Suppose one day the electric company decided to start supplying your electricity as
DC rather than AC. How would the DC voltage have to be related to the amplitude
$V_\zu{o}$ of the AC voltage previously used if they wanted your lightbulbs to
have the same brightness as before? The resistance of the bulb, $R$, is a fixed
value, so we need to relate the power to the voltage and the resistance, eliminating
the current. In the DC case, this gives $P=IV=(V/R)V=V^2/R$. (For DC, $P$ and $P_{av}$
are the same.) In the AC case, $P_{av} = I_\zu{o}V_\zu{o}/2=V_\zu{o}^2/2R$.
Since there is no factor of 1/2 in the DC case, the same power could be provided
with a DC voltage that was smaller by a factor of $1/\sqrt{2}$. 
Although you will hear people say that household voltage in the U.S. is 110 V,
its amplitude is actually $(110\ \zu{V})\times\sqrt{2}\approx160\ \zu{V}$. The reason
for referring to $V_\zu{o}/\sqrt{2}$ as ``the'' voltage is that people who are naive
about AC circuits can plug $V_\zu{o}/\sqrt{2}$ into a familiar DC equation like
$P=V^2/R$ and get the right \emph{average} answer. The quantity $V_\zu{o}/\sqrt{2}$
is called the ``RMS'' voltage\index{root mean square}\index{RMS (root mean square)},
which stands for ``root mean square.'' The idea is that if you square the function
$V(t)$, take its average (mean) over one cycle, and then take the square root of that
average, you get $V_\zu{o}/\sqrt{2}$. Many digital meters provide RMS readouts for
measuring AC voltages and currents.

<% end_sec() %>

<% begin_sec("A capacitor") %>

For a capacitor, the calculation starts out the same, but ends up with a
twist. If the voltage varies as a cosine, $V_\zu{o}\cos \omega t$, then
the relation $I=C\der V/\der t$ tells us that the current will be some
constant multiplied by minus the sine, $-V_\zu{o}\sin \omega t$.
The integral we did in the case of a resistor now becomes
\begin{equation*}
                E = \int_0^T -I_\zu{o}V_\zu{o} \sin \omega t \cos \omega t \der t\eqquad,\\
\end{equation*}
and based on figure \figref{capvip}, you can easily
convince yourself that over the course of one full cycle, the power spends two quarter-cycles
being negative and two being positive. In other words, the average power is zero!

<% marg(50) %>
<%
  fig(
    'capvip',
    %q{%
      Power in a capacitor: the rate at which energy is being
      stored in (+) or removed from (-) the electric field.
    }
  )
%>
<% end_marg %>

Why is this? It makes sense if you think in terms of energy. A resistor converts
electrical energy to heat, never the other way around. A capacitor, however, merely
stores electrical energy in an electric field and then gives it back.
For a capacitor,
\begin{align*}
        P_{av} &= 0 \qquad \text{[average power in a capacitor]}\\
\end{align*}
Notice that although the average power is zero, the power at any given instant is
\emph{not} typically zero, as shown in figure \figref{capvip}.
The capacitor \emph{does}
transfer energy: it's just that after borrowing some energy, it always pays it
back in the next quarter-cycle. 

<% end_sec() %>

<% begin_sec("An inductor") %>

The analysis for an inductor is similar to that for a capacitor: the power averaged
over one cycle is zero. Again, we're merely storing energy temporarily in a field
(this time a magnetic field) and getting it back later.

 % 
 % 
 % 

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Impedance matching") %>
\index{impedance matching}

<% marg(0) %>
<%
  fig(
    'zmatch',
    %q{%
      We wish to maximize the power delivered to
      the load, $Z_\zu{o}$, by adjusting its impedance.
    }
  )
%>
<% end_marg %>

Figure \figref{zmatch} shows a commonly encountered situation: we wish
to maximize the average
power, $P_{av}$, delivered to the load for a fixed value of $V_\zu{o}$,
the amplitude of the oscillating driving voltage. We assume that the
impedance of the transmission line, $Z_T$ is a fixed value, over
which we have no control, but we are able to design the load, $Z_\zu{o}$, with any impedance
 we like. For now, we'll also assume that both impedances are resistive. For example,
$Z_T$ could be the resistance of a long extension cord, and 
$Z_\zu{o}$ could be a lamp at the end of it. The result generalizes immediately,
however, to any kind of impedance.
For example, the load could be a stereo
speaker's magnet coil, which is displays both inductance and resistance. (For a purely inductive
or capacitive load, $P_{av}$ equals zero, so the problem isn't very interesting!)

Since we're assuming both the load and the transmission line are resistive, their
impedances add in series, and the amplitude of the current is given by
\begin{align*}
        I_\zu{o}        &= \frac{V_\zu{o}}{Z_\zu{o}+Z_T}\eqquad,\\
\intertext{so}
        P_{av} &= I_\zu{o}V_\zu{o}/2 \\
                        &= I_\zu{o}^2Z_\zu{o}/2 \\
                        &= \frac{V_\zu{o}^2Z_\zu{o}}{\left(Z_\zu{o}+Z_T\right)^2}/2\eqquad.
\intertext{The maximum of this expression occurs where the derivative is zero,}
        0        &= \frac{1}{2}\frac{\der}{\der Z_\zu{o}}\left[\frac{V_\zu{o}^2Z_\zu{o}}{\left(Z_\zu{o}+Z_T\right)^2}\right] \\
        0        &= \frac{1}{2}\frac{\der}{\der Z_\zu{o}}\left[\frac{Z_\zu{o}}{\left(Z_\zu{o}+Z_T\right)^2}\right] \\
        0        &= \left(Z_\zu{o}+Z_T\right)^{-2}-2Z_\zu{o}\left(Z_\zu{o}+Z_T\right)^{-3} \\
        0        &= \left(Z_\zu{o}+Z_T\right)-2Z_\zu{o} \\
        Z_\zu{o} &= Z_T 
\end{align*}
In other words, to maximize the power delivered to the load, we should make the load's
impedance the same as the transmission line's. This result may seem surprising at first,
but it makes sense if you think about it. If the load's impedance is too high, it's like
opening a switch and breaking the circuit; no power is delivered. On the other hand, it doesn't
pay to make the load's impedance too small. Making it smaller does give more current, but
no matter how small we make it, the current will still be limited by the transmission line's
impedance. As the load's impedance approaches zero, the current approaches this fixed value,
and the the power delivered, $I_\zu{o}^2Z_\zu{o}$, decreases in proportion to $Z_\zu{o}$.

Maximizing the power transmission by matching $Z_T$ to $Z_\zu{o}$ is called \emph{impedance
matching}. For example, an 8-ohm home stereo speaker will be correctly matched to
a home stereo amplifier with an internal impedance of 8 ohms, and 4-ohm car  speakers
will be correctly matched to a car stereo with a 4-ohm internal impedance. You might think
impedance matching would be unimportant because even if, for example, we used a car stereo
to drive 8-ohm speakers, we could compensate for the mismatch simply by turning the volume
knob higher. This is indeed one way to compensate for any impedance mismatch, but there
is always a price to pay. When the impedances are matched, half the power is dissipated
in the transmission line and half in the load. By connecting a 4-ohm amplifier to an
8-ohm speaker, however, you would be setting up a situation in two watts were being dissipated
as heat inside the amp for every watt being delivered to the speaker. In other words, you
would be wasting energy, and perhaps burning out your amp when you turned up the volume to
compensate for the mismatch.

<% end_sec() %>

<% begin_sec("Impedances in series and parallel") %>

How do impedances combine in series and parallel? The beauty of treating them
as complex numbers is that they simply combine according to the same rules you've
already learned as resistances.

\begin{eg}{Series impedance}\label{eg:series-impedance}
\egquestion
A capacitor and an inductor in series with each other are driven by a sinusoidally
oscillating voltage. At what frequency is the current maximized?

\eganswer
Impedances in series, like resistances in series, add. The capacitor and inductor
act as if they were a single circuit element with an impedance
\begin{align*}
         Z        &=  Z_{L}+ Z_{C}\\
                        &=  i\omega L-\frac{ i}{\omega C}\eqquad.\\
\intertext{The current is then}
        \tilde{ I} = \frac{\tilde{ V}}{ i\omega L- i/\omega C}\eqquad.
\end{align*}
We don't care about the phase of the current, only its amplitude, which is
represented by the absolute value of the complex number $\tilde{ I}$, and
this can be maximized by making  
$| i\omega L- i/\omega C|$ as small as possible.
But there is some frequency at which this quantity is \emph{zero}\/ ---
\begin{gather*}
        0 =  i\omega L-\frac{ i}{\omega C}\\
        \frac{1}{\omega C} = \omega L\\
        \omega = \frac{1}{\sqrt{ LC}}
\end{gather*}
At this frequency, the current is infinite! What is going on physically?
This is an LRC circuit with $R=0$. It has a resonance at this frequency,
and because there is no damping, the response at resonance is infinite.
Of course, any real LRC circuit will have some damping, however small (cf. figure \figref{resonance} on page \pageref{fig:resonance}).
\end{eg}
\begin{eg}{Resonance with damping}\index{LRC circuit}
\egquestion
What is the amplitude of the current in a series LRC circuit?

\eganswer
Generalizing from  example \ref{eg:series-impedance}, we add a third, real impedance:
\begin{align*}
        |\tilde{ I}| &= \frac{|\tilde{ V}|}{| Z|} \\
                &= \frac{|\tilde{ V}|}{| R+ i\omega L- i/\omega C|} \\
                &= \frac{|\tilde{ V}|}{\sqrt{ R^2+(\omega L-1/\omega C)^2}} 
\end{align*}
This result would have taken pages of algebra without the complex number technique!
\end{eg}

\begin{eg}{A second-order stereo crossover filter}\label{eg:crossover}
A stereo crossover filter ensures that the high frequencies
go to the tweeter and the lows to the woofer. This can be
accomplished simply by putting a single capacitor in series
with the tweeter and a single inductor in series with the
woofer. However, such a filter does not cut off very
sharply. Suppose we model the speakers as resistors. (They
really have inductance as well, since they have coils in
them that serve as electromagnets to move the diaphragm that
makes the sound.) Then the power they draw is $I^2 R$.
Putting an inductor in series with the woofer, \figref{crossover}/1, gives a
total impedance that at high frequencies is dominated by the
inductor's, so the current is proportional to $\omega^{-1}$, and the
power drawn by the woofer is proportional to $\omega^{-2}$.  

<% marg(0) %>
<%
  fig(
    'crossover',
    %q{Example \ref{eg:crossover}.}
  )
%>
<% end_marg %>

A second-order filter, like \figref{crossover}/2, is one that cuts off more
sharply: at high frequencies, the power goes like $\omega^{-4}$. To
analyze this circuit, we first calculate the total impedance:
\begin{equation*}
                 Z  =   Z_{L}+( Z_{C}^{-1}+ Z_R^{-1})^{-1}  
\end{equation*}
All the current passes through the inductor, so if the driving voltage being supplied
on the left is $\tilde{ V}_d$, we have
\begin{equation*}
                \tilde{ V}_d  =  \tilde{ I}_{L} Z\eqquad,  
\end{equation*}
and we also have
\begin{equation*}
                \tilde{ V}_{L}  =  \tilde{ I}_{L} Z_L\eqquad.  
\end{equation*}
The loop rule, applied to the outer perimeter of the circuit, gives
\begin{equation*}
                \tilde{ V}_{d}  = \tilde{ V}_{L}+\tilde{ V}_R\eqquad.  
\end{equation*}
Straightforward algebra now results in
\begin{equation*}
                \tilde{ V}_{R}  = \frac{\tilde{ V}_{d}}%
                                                                {1+ Z_L/ Z_{C}+ Z_{L}/ Z_R}\eqquad.  
\end{equation*}
At high frequencies, the $Z_{L}/ Z_C$ term, which varies as
$\omega^2$, dominates, so $\tilde{ V}_R$ and $\tilde{ I}_R$
 are proportional to $\omega^{-2}$,
and the power is proportional to $\omega^{-4}$.

\end{eg}



 % 

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Fields by Gauss' law",4,'gauss') %>

<% begin_sec("Gauss' law") %>

The flea of subsection \ref{subsec:surfacefield} had a long and illustrious
scientific career, and we're now going to pick up her story where we left off.
This flea, whose name is Gauss\footnote{no relation to the human
mathematician of the same name}, has derived the equation $E_\perp=2\pi k\sigma$ for the
electric field very close to a charged surface with charge density $\sigma$. 
Next we will describe two improvements she is going to make to that equation.

First, she realizes
that the equation is not as useful as it could be, because it only gives the part of
the field \emph{due to the surface}. If other charges are nearby, then their fields
will add to this field as vectors, and the equation will not be true unless we carefully
subtract out the field from the other charges. This is especially problematic for her
because the planet on which she lives, known for obscure reasons as planet Flatcat, is
itself electrically charged, and so are all the fleas
 --- the only thing that keeps them from floating off into
outer space is that they are negatively charged, while Flatcat carries a positive charge, so they
are electrically attracted to it. When Gauss found the original version of her equation,
she wanted to demonstrate it to her skeptical colleagues in the laboratory, using electric 
field meters and charged pieces of metal foil. Even if she set up the measurements by
remote control, so that her the charge on her own body would be too far away to have any
effect, they would be disrupted by the ambient field of planet Flatcat. Finally, however,
she realized that she could improve her equation by rewriting it as follows:
\begin{equation*}
        E_{outward,\ on\ side\ 1}+E_{outward,\ on\ side\ 2} = 4\pi k\sigma\eqquad.
\end{equation*}
The tricky thing here is that ``outward'' means a different thing, depending on which
side of the foil we're on. On the left side, ``outward'' means to the left, while on
the right side, ``outward'' is right. A positively charged piece of metal foil has
a field that points leftward on the left side, and rightward on its right side, so
the two contributions of $2\pi k\sigma$ are both positive, and we get $4\pi k\sigma$.
On the other hand, suppose there is a field created by other charges, not by the charged foil,
that happens to point to the right. On the right side, this externally created field is in the same
direction as the foil's field, but on the left side, the it \emph{reduces} the strength
of the leftward field created by the foil. The increase in one term of the equation
balances the decrease in the other term. This new version of the equation is thus
exactly correct regardless of what externally generated fields are present!

Her next innovation starts by multiplying the equation on both sides by the area,
$A$, of one side of the foil:
\begin{align*}
        \left(E_{outward,\ on\ side\ 1}+E_{outward,\ on\ side\ 2}\right)A &= 4\pi k\sigma A \\
\intertext{or}
        E_{outward,\ on\ side\ 1}A+E_{outward,\ on\ side\ 2}A &= 4\pi kq\eqquad, \\
\end{align*}
where $q$ is the charge of the foil. The reason for this modification is that
she can now make the whole thing more attractive by 
defining a new vector, the area vector \vc{A}. As shown in figure
\figref{avectorflat}, she defines an area vector
for side 1 which has magnitude $A$ and points outward from side 1, and
an area vector for side 2 which has the same magnitude and points outward from
that side, which is in the opposite direction. The dot product of two vectors,
$\vc{u}\cdot\vc{v}$, can be interpreted as $u_{parallel\ to\ v}|\vc{v}|$, and she
can therefore rewrite her equation as 
\begin{equation*}
        \vc{E}_1\cdot\vc{A}_1+\vc{E}_2\cdot\vc{A}_2 = 4\pi k q\eqquad.
\end{equation*}
The quantity on the left side of this equation is called the \emph{flux} through
the surface, written $\Phi$.\index{flux!defined}

<% marg(81) %>
<%
  fig(
    'avectorflat',
    %q{%
      The area vector is defined to be perpendicular to the surface,
      in the outward direction. Its magnitude tells how much the area is.
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'flatcatmap',
    %q{Gauss contemplates a map of the known world.}
  )
%>
<% end_marg %>

Gauss now writes a grant proposal to her favorite funding agency, the BSGS
(Blood-Suckers' Geological Survey), and it is quickly approved. Her audacious
plan is to send out exploring teams to chart the electric fields of the whole
planet of Flatcat, and thereby determine the total electric charge of the planet.
The fleas' world is commonly assumed to be a flat disk, and its size is known
to be finite, since the sun passes behind it at sunset and comes back around on the other
side at dawn. The most daring part of the plan is that it requires surveying not just
the known side of the planet but the uncharted Far Side as well. No flea has ever
actually gone around the edge and returned to tell the tale, but Gauss assures them
that they won't fall off --- their negatively charged bodies will be attracted
to the disk no matter which side they are on.

Of course it is possible that the electric charge of planet Flatcat is not perfectly
uniform, but that isn't a problem. As discussed in subsection \ref{subsec:surfacefield},
as long as one is very close to the surface, the field only depends on the \emph{local}
charge density. In fact, a side-benefit of Gauss's program of exploration is that any
such local irregularities will be mapped out. But what the newspapers find exciting is
the idea that once all the teams get back from their voyages and tabulate their data,
the \emph{total} charge of the planet will have been determined for the first time.
Each surveying team is assigned to visit a certain list of republics, duchies, city-states, and
so on. They are to record each territory's electric field vector, as well as its area.
Because the electric field may be nonuniform, the final equation for determining the
planet's electric charge will have many terms, not just one for each side of the planet:
\begin{equation*}
        \Phi = \sum \vc{E}_j\cdot\vc{A}_j = 4\pi k q_{total}
\end{equation*}

Gauss herself leads one of the expeditions, which heads due east, toward the distant
Tail Kingdom, known only from fables and the occasional account from a
caravan of traders. A strange thing happens, however. Gauss embarks from her
college town in the wetlands of the Tongue Republic, travels straight east,
passes right through the Tail Kingdom, and one day finds herself right back at
home, all without ever seeing the edge of the world! What can have happened?
All at once she realizes that the world isn't flat.

<% marg(30) %>
<%
  fig(
    'avectormany',
    %q{%
      Each part of the surface has its own area vector. Note the
      differences in lengths of the vectors, corresponding to the unequal areas.
    }
  )
%>
<% end_marg %>

Now what? The surveying teams all return, the data are tabulated, and
the result for the total charge of Flatcat is $(1/4\pi k)\sum \vc{E}_j\cdot\vc{A}_j=37\ \zu{nC}$
(units of nanocoulombs). But the equation was derived under the assumption that
Flatcat was a disk. If Flatcat is really round, then the result may be completely
wrong. Gauss and two of her grad students go to their favorite bar, and decide to keep
on ordering Bloody Marys until they either solve their problems or forget them. One
student suggests that perhaps Flatcat really is a disk, but the edges are rounded. Maybe
the surveying teams really did flip over the edge at some point, but just didn't realize
it. Under this assumption, the original equation will be approximately valid, and
37 nC really is the total charge of Flatcat.

<% marg(0) %>
<%
  fig(
    'avector',
    %q{%
      An area vector can be defined for a sufficiently small part of
      a curved surface.
    }
  )
%>
<% end_marg %>

A second student, named Newton, suggests that they take seriously the possibility that
Flatcat is a sphere. In this scenario, their planet's surface is really curved, but the surveying
teams just didn't notice the curvature, since they were close to the surface, and the
surface was so big compared to them. They divided up the surface into a patchwork, and
each patch was fairly small compared to the whole planet, so each patch was nearly flat.
Since the patch is nearly flat, it makes sense to define an area vector that is perpendicular
to it. In general, this is how we define the direction of an area vector, as shown
in figure \figref{avector}. This only works if the areas are small. For instance, there
would be no way to define an area vector for an entire sphere, since ``outward'' is
in more than one direction.

If Flatcat is a sphere, then 
the inside of the sphere must be vast, and there is no way of knowing
exactly how the charge is arranged below the surface. However, the survey teams all found
that the electric field was approximately perpendicular to the surface everywhere, and
that its strength didn't change very much from one location to another. The simplest
explanation is that the charge is all concentrated in one small lump at the center of the sphere.
They have no way of knowing if this is really the case, but it's a hypothesis that allows them
to see how much their 37 nC result would change if they assumed a different geometry.
Making this assumption, Newton performs the following simple computation on a napkin.
The field at the surface is related to the charge at the center\label{napkin}
by

<% if is_print then
print %q(
\begin{align*}
        |\vc{E}| &= \frac{kq_{total}}{r^2}\eqquad,
\intertext{where $r$ is the radius of Flatcat. The flux is then}
        \Phi        &= \sum \vc{E}_j\cdot\vc{A}_j\eqquad, \\\\
\intertext{and since the $\vc{E}_j$ and $\vc{A}_j$ vectors are parallel, the dot
product equals $|\vc{E}_j||\vc{A}_j|$, so}
        \Phi        &= \sum \frac{kq_{total}}{r^2}|\vc{A}_j|\eqquad.\\\\
\intertext{But the field strength is always the same, so we can take it outside the sum,
giving}
        \Phi        &= \frac{kq_{total}}{r^2} \sum |\vc{A}_j| \\\\
                        &= \frac{kq_{total}}{r^2} A_{total} \\\\
                        &= \frac{kq_{total}}{r^2} 4\pi r^2 \\\\
                        &= 4\pi kq_{total}\eqquad.
\end{align*}
)
else
print %q(
\begin{equation*}
        |\vc{E}| &= \frac{kq_{total}}{r^2}\eqquad,
\end{equation*}
where $r$ is the radius of Flatcat. The flux is then
\begin{equation*}
        \Phi        &= \sum \vc{E}_j\cdot\vc{A}_j\eqquad, \\
\end{equation*}
and since the $\vc{E}_j$ and $\vc{A}_j$ vectors are parallel, the dot
product equals $|\vc{E}_j||\vc{A}_j|$, so
\begin{equation*}
        \Phi        &= \sum \frac{kq_{total}}{r^2}|\vc{A}_j|\eqquad.\\
\end{equation*}
But the field strength is always the same, so we can take it outside the sum,
giving
\begin{align*}
        \Phi        &= \frac{kq_{total}}{r^2} \sum |\vc{A}_j| \\
                        &= \frac{kq_{total}}{r^2} A_{total} \\
                        &= \frac{kq_{total}}{r^2} 4\pi r^2 \\
                        &= 4\pi kq_{total}\eqquad.
\end{align*}
)
end
%>

Not only have all the factors of $r$ canceled out, but the result is the same
as for a disk!

Everyone is pleasantly surprised by this apparent mathematical coincidence, but
is it anything more than that? For instance, what if the charge wasn't concentrated
at the center, but instead was evenly distributed throughout Flatcat's interior
volume? Newton, however, is familiar with a result called the shell theorem
(page \pageref{shelltheoremsubsection}), which states that the field of a uniformly
charged sphere is the same as if all the charge had been concentrated at its
center.\footnote{Newton's human namesake actually proved this for gravity, not
electricity, but they're both $1/r^2$ forces, so the proof works equally well in
both cases.} We now have three different assumptions about the shape of Flatcat and
the arrangement of the charges inside it, and all three lead to exactly the
\emph{same} mathematical result, $\Phi = 4\pi kq_{total}$. This is starting to
look like more than a coincidence. In fact, there is a general mathematical theorem,
called Gauss' theorem, which states the following:

For any region of space, the flux through the surface equals $4\pi kq_{in}$, where
$q_{in}$ is the total charge in that region.\index{flux!in Gauss' theorem}\index{Gauss' theorem}

Don't memorize the factor of $4\pi$ in front --- you can rederive it any time you
need to, by considering a spherical surface centered on a point charge.

Note that although region and its surface had a definite physical existence in our
story --- they are the planet Flatcat and the surface of planet Flatcat --- Gauss'
law is true for any region and surface we choose, and in general, the Gaussian
surface has no direct physical significance. It's simply a computational tool.

Rather than proving Gauss' theorem and then presenting some examples and applications,
it turns out to be easier to show some examples that demonstrate its salient properties.
Having understood these properties, the proof becomes quite simple.

<% self_check('influx',<<-'SELF_CHECK'
Suppose we have a negative point charge, whose field points
inward, and we pick a Gaussian surface which is a sphere centered on that charge.
How does Gauss' theorem apply here?
  SELF_CHECK
  ) %>

 % 

<% end_sec() %>

<% begin_sec("Additivity of flux") %>

Figure \figref{addflux} shows two two different ways in which flux is additive. Figure
 \figref{addflux}/1, additivity by charge, shows that we can break down a charge distribution
 into two or more parts, and the flux equals the sum of the fluxes due to the individual
 charges. This follows directly from the fact that the flux is defined in terms of a dot
 product, $\vc{E}\cdot\vc{A}$, and the dot product has the additive property
 $(\vc{a}+\vc{b})\cdot\vc{c}=\vc{a}\cdot\vc{c}+\vc{b}\cdot\vc{c}$.\index{flux!additivity by charge}

<% marg(30) %>
<%
  fig(
    'addflux',
    %q{%
      1. The flux due to two charges equals the sum of the fluxes from each one.
      2. When two regions are joined together, the flux through the new region equals the sum
      of the fluxes through the two parts.
    }
  )
%>
<% end_marg %>

To understand additivity of flux by region, \figref{addflux}/2, we have to consider 
the parts of the two surfaces that were eliminated when they were joined together,
like knocking out a wall to make two small apartments into one big one. Although the
two regions shared this wall before it was removed, the area vectors were opposite:
the direction that is outward from one region is inward with respect to the other.
Thus if the field on the wall contributes positive flux to one region, it contributes
an equal amount of negative flux to the other region, and we can therefore eliminate
the wall to join the two regions, without changing the total flux.
\index{flux!additivity by region}

 % 

<% end_sec() %>

<% begin_sec("Zero flux from outside charges") %>
\label{tinycubeproof}
A third important property of Gauss' theorem is that it only refers to the charge
\emph{inside} the region we choose to discuss. In other words, it asserts that any
charge outside the region contributes zero to the flux. This makes at least some sense, because
a charge outside the region will have field vectors pointing into the surface on one
side, and out of the surface on the other. Certainly there 
should be at least partial cancellation between the
negative (inward) flux on one side and the positive (outward) flux on the other. But why
should this cancellation be exact?

To see the reason for this perfect cancellation,
we can imagine space as being built out of tiny cubes, and we can think of any charge
distribution as being composed of point charges. The additivity-by-charge property tells us that
any charge distribution can be handled by considering its point charges individually,
and the additivity-by-region property tells us that if we have a single point charge
outside a big region, we can break the region down into tiny cubes. If we can prove
that the flux through such a tiny cube really does cancel exactly, then the same must
be true for any region, which we could build out of such cubes, and any charge distribution,
which we can build out of point charges.  

For simplicity, we will carry out this calculation only in the special case shown
in figure \figref{pointflux}, where the charge lies along one axis of the cube.
Let the sides of the cube have length
$2b$, so that the area of each side is $(2b)^2=4b^2$. The cube extends a
distance $b$ above, below, in front of, and behind
the horizontal $x$ axis. There is a distance $d-b$ from the charge to the left
side, and $d+b$ to the right side.  

<% marg(0) %>
<%
  fig(
    'pointflux',
    %q{The flux through a tiny cube due to a point charge.}
  )
%>
<% end_marg %>

There
will be one negative flux, through the left side, and five
positive ones. Of these positive ones, the one through the right side is very nearly
the same in magnitude as the negative flux through the left side, but just a little less
because the field is weaker on the right, due to the greater distance from the charge.
The fluxes through the other four sides are very small, since the field is nearly
perpendicular to their area vectors, and the dot product $\vc{E}_j\cdot\vc{A}_j$ is
zero if the two vectors are perpendicular. In the limit where $b$ is very small, we can approximate
the flux by evaluating the field at the center of each of the cube's six sides,
giving\label{tinycube}
\begin{align*}
        \Phi        &= \Phi_{left}+4\Phi_{side}+\Phi_{right} \\
                        &= |\vc{E}_{left}||\vc{A}_{left}|\cos 180\degunit
                                +4|\vc{E}_{side}||\vc{A}_{side}|\cos \theta_{side} \\
                                & \quad +|\vc{E}_{right}||\vc{A}_{right}|\cos 0\degunit\eqquad,\\
\intertext{and a little trig gives $\cos\theta_{side}\approx b/d$, so}
        \Phi        &= -|\vc{E}_{left}||\vc{A}_{left}|
                                +4|\vc{E}_{side}||\vc{A}_{side}|\frac{b}{d}
                                +|\vc{E}_{right}||\vc{A}_{right}|\\
                        &= \left(4b^2\right)\left(-|\vc{E}_{left}|
                                +4|\vc{E}_{side}|\frac{b}{d}
                                +|\vc{E}_{right}|\right)\\
                        &= \left(4b^2\right)\left(-\frac{kq}{(d-b)^2}
                                +4\frac{kq}{d^2}\frac{b}{d}
                                +\frac{kq}{(d+b)^2}\right)\\
                        &= \left(\frac{4kqb^2}{d^2}\right)\left(-\frac{1}{(1-b/d)^2}
                                +\frac{4b}{d}
                                +\frac{1}{(1+b/d)^2}\right)\eqquad.\\
\intertext{Using the approximation $(1+\epsilon)^{-2}\approx 1-2\epsilon$ for small $\epsilon$, this
becomes}
        \Phi        &= \left(\frac{4kqb^2}{d^2}\right)\left(-1-\frac{2b}{d}
                                +\frac{4b}{d}
                                +1-\frac{2b}{d}\right) \\
                        &= 0\eqquad.
\end{align*}
Thus in the limit of a very small cube,  $b\ll d$, we have proved that the flux due to
this exterior charge is zero. The proof can be extended to the case where the
charge is not along any axis of the
cube,\footnote{The math gets messy for the off-axis case. This part of the proof can be
completed more easily and transparently using the techniques of section
\ref{sec:gaussdiff}, and that is exactly we'll do in example \ref{eg:divpointcharge}
on page \pageref{eg:divpointcharge}.}
and based on additivity we then have a proof that the flux due to an outside charge is
always zero.

\begin{eg}{No charge on the interior of a conductor}\label{eg:no-charge-on-interior}
I asserted on p.~\pageref{assert-no-charge-on-interior} that for a perfect conductor in equilibrium,
excess charge is found only at the surface, never in the interior. This can be proved using Gauss's theorem.
Suppose that a charge $q$ existed at some point in the interior, and it was in stable equilibrium. For concreteness,
let's say $q$ is positive. If its
equilibrium is to be stable, then we need an electric field everywhere around it that points inward like
a pincushion, so that if the charge were to be perturbed slightly, the field would bring it back to its
equilibrium position. Since Newton's third law forbids objects from making forces on themselves, this field
would have to be the field contributed by all the other charges, not by $q$ itself. 
But this is impossible, because this kind of inward-pointing pincushion pattern would have
a nonzero (negative) flux through the pincushion, but Gauss's theorem says we can't have flux
from outside charges.
\end{eg}

\pagebreak

\startdqs

<%
  fig(
    'dq-gauss-examples',
    %q{Discussion question \ref{dq:gauss-example1}-\ref{dq:gauss-example4}.},
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

\begin{dq}\label{dq:gauss-example1}
One question that might naturally occur to you about Gauss's law is what happens for charge that is exactly on the
surface --- should it be counted toward the enclosed charge, or not? If charges can be perfect, infinitesimal points,
then this could be a physically meaningful question. Suppose we approach this question by way of a limit: start with charge $q$ spread out over a sphere of finite size, and then make the size of the sphere approach zero.
The figure shows a uniformly charged sphere that's exactly half-way in and half-way out of the cubical Gaussian surface.
What is the flux through the cube, compared to what it would be if the charge was entirely enclosed? (There are at least
three ways to find this flux: by direct integration, by Gauss's law, or by the additivity of flux by region.)
\end{dq}

\begin{dq}
The dipole is completely enclosed in the cube. What does Gauss's law say about the flux through the cube? If you imagine
the dipole's field pattern, can you verify that this makes sense?
\end{dq}

\begin{dq}\label{dq:gauss-example3}
The wire passes in through one side of the cube and out through the other. If the current through the wire is
increasing, then the wire will act like an inductor, and there will be a voltage difference between its ends.
(The inductance will be relatively small, since the wire isn't coiled up, and the $\Delta V$ will therefore
also be fairly small, but still not zero.) The $\Delta V$ implies the
existence of electric fields, and yet Gauss's law says the flux must be zero, since there is no charge inside
the cube. Why isn't Gauss's law violated?
\end{dq}

\begin{dq}\label{dq:gauss-example4}
The charge has been loitering near the edge of the cube, but is then suddenly hit with a mallet, causing it to
fly off toward the left side of the cube. We haven't yet discussed in detail how disturbances in the electric and magnetic fields
ripple outward through space, but it turns out that they do so at the speed of light. (In fact, that's what light
is: ripples in the electric and magnetic fields.) Because the charge is closer to the left side of the cube,
the change in the electric field occurs there before the information reaches the right side. 
This would seem certain to lead to a violation of Gauss's law. How can the ideas explored in
discussion question \ref{dq:gauss-example3} show the resolution to this paradox?
\end{dq}

 % 

<% end_sec() %>

<% begin_sec("Proof of Gauss' theorem") %>

With the computational machinery we've developed, it is now simple to prove Gauss'
theorem. Based on additivity by charge, it suffices to prove the law for a point charge.
We have already proved Gauss' law for a point charge in the case where the point
charge is outside the region. If we can prove it for the inside case, then we're all done.

<% marg(0) %>
<%
  fig(
    'gaussproof',
    %q{Completing the proof of Gauss' theorem.}
  )
%>
<% end_marg %>

If the charge is inside, we reason as follows. First, we forget about the actual Gaussian
surface of interest, and instead construct a spherical one, centered on the charge.
For the case of a sphere, we've already seen the proof written on a napkin by
the flea named Newton (page \pageref{napkin}). Now wherever the actual surface sticks
out beyond the sphere, we glue appropriately shaped pieces onto the sphere. In the
example shown in figure \figref{gaussproof}, we have to add two Mickey Mouse ears.
Since these added pieces do not contain the point charge, the flux through them is zero,
and additivity of flux by region therefore tells us that the total flux is not changed
when we make this alteration. Likewise, we need to chisel out any regions where the
sphere sticks out beyond the actual surface. Again, there is no change in flux, since
the region being altered doesn't contain the point charge. This proves that the flux
through the Gaussian surface of interest is the same as the flux through the sphere,
and since we've already proved that that flux equals $4\pi kq_{in}$, our proof of
Gauss' theorem is complete.\index{Gauss' theorem!proof of}

\startdqs

\begin{dq}
A critical part of the proof of Gauss' theorem was the proof that a tiny
cube has zero flux through it due to an external charge. Discuss qualitatively
why this proof would fail if Coulomb's law was a $1/r$ or $1/r^3$ law.
\end{dq}

 % 

<% end_sec() %>

<% begin_sec("Gauss' law as a fundamental law of physics") %>

Note that the proof of Gauss' theorem depended on the computation on the napkin
discussed on page \ref{napkin}. The crucial point in this computation was that
the electric field of a point charge falls off like $1/r^2$, and since the
area of a sphere is proportional to $r^2$, the result is independent of $r$.
The $1/r^2$ variation of the field also came into play on page \pageref{tinycube}
in the proof that the flux due to an outside charge is zero.
In other words, if we discover some other force of nature which is proportional to
$1/r^3$ or $r$, then Gauss' theorem will not apply to that force. Gauss' theorem
is not true for nuclear forces, which fall off exponentially with distance. However, this
is the \emph{only} assumption we had to make about the nature of the field. Since
gravity, for instance, also has fields that fall off as $1/r^2$, Gauss' theorem
is equally valid for gravity --- we just have to replace mass with
charge, change the Coulomb constant $k$ to the gravitational constant
$G$, and insert a minus sign because the gravitational fields around a (positive)
mass point inward.\index{Gauss' theorem!for gravity}

Gauss' theorem can only be proved if we assume a $1/r^2$ field, and the converse
is also true: any field that satisfies Gauss' theorem must be a $1/r^2$ field.
Thus although we previously thought of Coulomb's law as the fundamental law
of nature describing electric forces, it is equally valid to think of Gauss'
theorem as the basic law of nature for electricity. From this point of view,
Gauss' theorem is not a mathematical fact but an experimentally testable statement
about nature, so we'll refer to it as Gauss' \emph{law}, just as we speak of
Coulomb's \emph{law} or Newton's \emph{law} of gravity.\index{Gauss' law}

If Gauss' law is equivalent to Coulomb's law, why not just use Coulomb's law?
First, there are some cases where calculating a field is easy with Gauss'
law, and hard with Coulomb's law. More importantly, Gauss' law and Coulomb's
law are only mathematically equivalent under the assumption that all our charges
are standing still, and all our fields are constant over time, i.e., in
the study of electrostatics, as opposed to electrodynamics. As we broaden our
scope to study generators, inductors, transformers, and radio antennas, we will
encounter cases where Gauss' law is valid, but Coulomb's law is not.

<% end_sec() %>

<% begin_sec("Applications") %>

Often we encounter situations where we have a static charge distribution, and
we wish to determine the field. Although superposition is a generic
strategy for solving this type of problem, if the charge distribution is
symmetric in some way, then Gauss' law is often a far easier way to carry
out the computation.  

<% begin_sec("Field of a long line of charge") %>

Consider the field of an infinitely long line of charge, holding a uniform
charge per unit length $\lambda$. Computing this
field by brute-force superposition was fairly laborious (examples
\ref{eg:chargedrodside} on page \pageref{eg:chargedrodside} 
and \ref{eg:pointlinesurface} on page \pageref{eg:pointlinesurface}).
With Gauss' law it becomes a very simple calculation.

<% marg(0) %>
<%
  fig(
    'gaussline',
    %q{Applying Gauss' law to an infinite line of charge.}
  )
%>
<% end_marg %>

The problem has two types of symmetry. The line of charge, and therefore the resulting
field pattern, look the same if we rotate them about the line. The second symmetry
occurs because the line is infinite: if we slide the line along its own length,
nothing changes. This sliding symmetry, known as a translation symmetry,
tells us that the field must point directly away from the line at any given point.

Based on these symmetries, we choose the Gaussian surface shown in
figure \figref{gaussline}. If we want to know the field at a distance $R$ from
the line, then we choose this surface to have a radius $R$, as shown in the
figure. The length, $L$, of the surface is irrelevant.

The field is parallel to the surface on the end caps,
and therefore perpendicular to the end caps' area vectors, so there is no
contribution to the flux. On the long, thin strips that make up the rest of
the surface, the field is perpendicular to the surface, and therefore parallel
to the area vector of each strip, so that the dot product occurring in the
definition of the flux is $\vc{E}_j\cdot\vc{A}_j=|\vc{E}_j||\vc{A}_j||\cos\ 0\degunit=|\vc{E}_j||\vc{A}_j|$.
Gauss' law gives
\begin{align*}
        4\pi k q_{in}        &= \sum \vc{E}_j\cdot\vc{A}_j \\
        4\pi k \lambda L        &= \sum |\vc{E}_j||\vc{A}_j|\eqquad.\\
\intertext{The magnitude of the field is the same on every strip, so we can take it outside the sum.}
        4\pi k \lambda L        &= |\vc{E}| \sum |\vc{A}_j| \\
\intertext{In the limit where the strips are infinitely narrow, the surface becomes a
cylinder, with (area)=(circumference)(length)=$2\pi RL$.}
        4\pi k \lambda L        &= |\vc{E}| \times 2\pi RL \\
        |\vc{E}|                        &= \frac{2k\lambda}{R} 
\end{align*}

<% end_sec() %>

<% begin_sec("Field near a surface charge") %>

As claimed earlier, the result $E=2\pi k\sigma$ for the field near a charged
surface is a special case of Gauss' law. We choose a Gaussian surface of the shape
shown in figure \figref{pillbox}, known as a Gaussian pillbox.\index{Gaussian pillbox}\index{pillbox!Gaussian}
The exact shape of the flat end caps is unimportant.

<% marg(0) %>
<%
  fig(
    'pillbox',
    %q{Applying Gauss' law to an infinite charged surface.}
  )
%>
<% end_marg %>

The symmetry of the charge distribution tells us that the field points directly away
from the surface, and is equally strong on both sides of the surface.
This means that the end caps contribute equally to the flux, and the curved sides have zero
flux through them. If the area of each end cap is $A$, then
\begin{align*}
        4\pi k q_{in}        &=  \vc{E}_1\cdot\vc{A}_1+\vc{E}_2\cdot\vc{A}_2\eqquad, \\
\intertext{where the subscripts 1 and 2 refer to the two end caps. We have $\vc{A}_2=-\vc{A}_1$, so}
        4\pi k q_{in}        &=  \vc{E}_1\cdot\vc{A}_1-\vc{E}_2\cdot\vc{A}_1 \\
        4\pi k q_{in}        &=  \left(\vc{E}_1-\vc{E}_2\right)\cdot\vc{A}_1\eqquad, \\
\intertext{and by symmetry the magnitudes of the two fields are equal, so}
        2|\vc{E}|A &= 4 \pi k \sigma A\\
        |\vc{E}| &= 2\pi k\sigma
\end{align*}

The symmetry between the two sides could be broken by the existence of other charges
nearby, whose fields would add onto the field of the surface itself. 
Even then, Gauss's law still guarantees
\begin{align*}
4\pi k q_{in}        &=  \left(\vc{E}_1-\vc{E}_2\right)\cdot\vc{A}_1\eqquad,
\intertext{or}
  |\vc{E}_{\perp,1}-\vc{E}_{\perp,2}| &= 4\pi k \sigma\eqquad,
\end{align*}
where the subscript $\perp$ indicates the component of the field parallel
to the surface (i.e., parallel to the area vectors). In other words,
the electric field changes discontinuously when we pass through a charged
surface; the discontinuity occurs in the component of the field perpendicular
to the surface, and the amount of discontinuous change is
$4\pi k \sigma$. This is a completely general statement that is true near
any charged surface, regardless of the existence of other charges nearby.\label{e-discontinuity}

\vfill

<% end_sec() %>

<% end_sec() %>

<% end_sec %>

<% begin_sec("Gauss' law in differential form",4,'gaussdiff') %>

<% begin_sec("Gauss's law as a local law",nil,'gauss-local') %>

Gauss' law is a bit spooky. It relates the field on the Gaussian surface
to the charges inside the surface. What if the charges have been moving
around, and the field at the surface right now is the one that was created
by the charges in their previous locations? Gauss' law --- unlike
Coulomb's law --- still works in cases like these, but it's far from
obvious how the flux and the charges can still stay in agreement if
the charges have been moving around.

For this reason, it would be more physically attractive to restate Gauss'
law in a different form, so that it related the behavior of the field
at one point to the charges that were actually present at that point.
This is essentially what we were doing in the fable of the flea named
Gauss: the fleas' plan for surveying their planet was essentially one of 
dividing up the surface of their planet (which they believed was flat)
into a patchwork, and then constructing \emph{small} a Gaussian pillbox
around each \emph{small} patch. The equation $E_{\perp}=2\pi k\sigma$
then related a particular property of the \emph{local} electric field to
the \emph{local} charge density.

In general, charge distributions need not be confined to a flat surface ---
life is three-dimensional --- but the general approach of defining
very small Gaussian surfaces is still a good one. Our strategy is
to divide up space into tiny cubes, like the one on page
\pageref{tinycubeproof}. Each such cube constitutes a Gaussian surface,
which may contain some charge. Again we approximate the field using
its six values at the center of each of the six sides. Let the
cube extend from $x$ to $x+\der x$, from $y$ to $y+\der y$, and from $y$ to $y+\der y$.

<% marg(0) %>
<%
  fig(
    'divcube',
    %q{A tiny cubical Gaussian surface.}
  )
%>
<% end_marg %>

The sides at $x$ and $x+\der x$ have area vectors $-\der y\der z\hat{\vc{x}}$
and $\der y\der z\hat{\vc{x}}$, respectively.
The flux through the side at $x$ is $-E_x(x)\der y\der z$, and the flux
through the opposite side, at $x+\der x$ is $E_x(x+\der x)\der y\der z$.
The sum of these is $(E_x(x+\der x)-E_x(x))\der y\der z$, and if the field was
uniform, the flux through these two opposite sides would be zero. It will only
be zero if the
field's $x$ component changes as a function of $x$.
The difference $E_x(x+\der x)-E_x(x)$ can be rewritten as
$\der E_x=(\der E_x)/(\der x)\der x$, so the contribution to the flux
from these two sides of the cube ends up being
\begin{equation*}
        \frac{\der E_x}{\der x}\der x\der y\der z\eqquad.
\end{equation*}
Doing the same for the other sides, we end up with a total flux
\begin{align*}
        \der \Phi        &= \left(\frac{\der E_x}{\der x}+\frac{\der E_y}{\der y}
                                                +\frac{\der E_z}{\der z}\right)\der x\der y\der z \\
                                &= \left(\frac{\der E_x}{\der x}+\frac{\der E_y}{\der y}
                                                +\frac{\der E_z}{\der z}\right)\der v\eqquad,\\
\intertext{where $\der v$ is the volume of the cube. In evaluating each of these
three derivatives, we are going to treat the other two variables as constants,
to emphasize this we use the partial derivative notation $\partial$ introduced
in chapter \ref{ch:3},}
        \der \Phi        &= \left(\frac{\partial E_x}{\partial x}+\frac{\partial E_y}{\partial y}
                                                +\frac{\partial E_z}{\partial z}\right)\der v\eqquad.\\
\intertext{Using Gauss' law,}
        4\pi k q_{in}        &= \left(\frac{\partial E_x}{\partial x}+\frac{\partial E_y}{\partial y}
                                                +\frac{\partial E_z}{\partial z}\right)\der v\eqquad,\\
\intertext{and we introduce the notation $\rho$ (Greek letter rho) for the charge
per unit volume, giving}
        4\pi k \rho        &= \frac{\partial E_x}{\partial x}+\frac{\partial E_y}{\partial y}
                                                +\frac{\partial E_z}{\partial z}\eqquad.\\
\intertext{The quantity on the right is called the \emph{divergence} of the electric
field, written $\divg \vc{E}$. Using this notation, we have}
        \divg \vc{E} = 4\pi k \rho\eqquad.
\end{align*}
This equation has all the same physical implications as Gauss' law. After all, we
proved Gauss' law by breaking down space into little cubes like this. We therefore
refer to it as the differential form of Gauss' law, as opposed to 
$\Phi=4\pi kq_{in}$, which is called the integral form.\index{Gauss' law!differential form}
\index{partial derivative}\index{divergence}

<% marg(20) %>
<%
  fig(
    'divmeter',
    %q{A meter for measuring $\divg\vc{E}$.}
  )
%>
<% end_marg %>

Figure \figref{divmeter} shows an intuitive way of visualizing the meaning
of the divergence. The meter consists of some electrically charged balls
connected by springs. If the divergence is positive, then the whole cluster will
expand, and it will contract its volume if it is placed at a point where the
field has $\divg\vc{E}<0$. What if the field is constant? We know based on the
definition of the divergence that we should have $\divg\vc{E}=0$ in this case,
and the meter does give the right result: all the balls
will feel a force in the same direction, but they will neither expand nor contract.

<% marg(0) %>
<%
  fig(
    'sinewavevector',
    %q{Example \ref{eg:divsine}.}
  )
%>
<% end_marg %>

\begin{eg}{Divergence of a sine wave}\label{eg:divsine}
\egquestion
Figure \figref{sinewavevector} shows an electric field that varies as a sine
wave. This is in fact what you'd see in a light wave: light is a wave pattern
made of electric and magnetic fields. (The magnetic field would look similar,
but would be in a plane perpendicular to the page.) What is the divergence
of such a field, and what is the physical significance of the result?

\eganswer
Intuitively, we can see that no matter where we put the div-meter in this
field, it will neither expand nor contract. For instance, if we put it at
the center of the figure, it will start spinning, but that's it.

Mathematically, let the $x$ axis be to the right and let $y$ be up.
The field is of the form
\begin{equation*}
        \vc{E}        = (\zu{sin}  Kx)\: \hat{\vc{y}}\eqquad,
\end{equation*}
where the constant $K$ is not to be confused with Coulomb's constant.
Since the field has only a $y$ component, the only term in the divergence
we need to evaluate is
\begin{equation*}
        \vc{E}        = \frac{\partial  E_{y}}{\partial  y}\eqquad,
\end{equation*}
but this vanishes, because $E_y$ depends only on $x$, not $y$\/:
we treat $y$ as a constant when evaluating the partial derivative
$\partial  E_{y}/\partial  y$, and the derivative of an expression
containing only constants must be zero.

Physically this is a very important result: it tells us that a light wave
can exist without any charges along the way to ``keep it going.'' In other
words, light can travel through a vacuum, a region with no particles in it.
If this wasn't true, we'd be dead, because the sun's light wouldn't be able
to get to us through millions of kilometers of empty space!
\end{eg}

\begin{eg}{Electric field of a point charge}\label{eg:divpointcharge}
        The case of a point charge is tricky, because the field
behaves badly right on top of the charge, blowing up and
becoming discontinuous. At this point, we cannot use the
component form of the divergence, since none of the
derivatives are well defined. However, a little visualization
using the original definition of the divergence will quickly
convince us that div $E$ is infinite here, and that makes
sense, because the density of charge has to be infinite at a
point where there is a zero-size point of charge (finite
charge in zero volume).

        At all other points, we have
\begin{equation*}
                        \vc{E}          =    \frac{ kq}{ r^2}\hat{\vc{r}}\eqquad,  
\end{equation*}
where $\hat{\vc{r}}=\vc{r}/ r=( x\hat{\vc{x}}+ y\hat{\vc{y}}+ z\hat{\vc{z}})/ r$ 
is the unit vector pointing radially away from
the charge. The field can therefore be written as
\begin{align*}
                        \vc{E}         &=     \frac{ kq}{ r^3}\hat{\vc{r}} \\
                                 &=      \frac{ kq( x\hat{\vc{x}}+ y\hat{\vc{y}}+ z\hat{\vc{z}})}%
                                                         {\left( x^2+ y^2+ z^2\right)^\zu{3/2}}\eqquad.  \\
\intertext{The three terms in the divergence are all similar, e.g.,}
        \frac{\partial  E_{x}}{\partial  x}         
                                &=    kq\frac{\partial}{\partial  x}%
                                                \left[\frac{ x}{\left( x^2+ y^2+ z^2\right)^\zu{3/2}}\right]  \\
                                 &=   kq\left[\frac{1}{\left( x^2+ y^2+ z^2\right)^\zu{3/2}}%
                                                -\frac{3}{2}\:\frac{2 x^2}{\left( x^2+ y^2+ z^2\right)^\zu{5/2}}\right]    \\
                                 &=   kq\left( r^{-3}-3 x^2 r^{-5}\right)\eqquad.  
\end{align*}
Straightforward algebra shows that adding in the other two
terms results in zero, which makes sense, because there is no
charge except at the origin.
\end{eg}

        Gauss' law in differential form lends itself most easily to
finding the charge density when we are give the field. What if we
want to find the field given the charge density? As demonstrated
in the following example, one technique that
often works is to guess the general form of the field
based on experience or physical intuition, and then
try to use Gauss' law to find what specific version
of that general form will be a solution.

\begin{eg}{The field inside a uniform sphere of charge}\label{eg:divsphere}
\egquestion
Find the field inside a uniform sphere of charge
whose charge density is $\rho$. (This is very much like
finding the gravitational field at some depth below the
surface of the earth.)

\eganswer
By symmetry we know that the field must be purely
radial (in and out). We guess that the solution might be of the form
\begin{equation*}
                        \vc{E}         =     br^ p\hat{\vc{r}}\eqquad,  
\end{equation*}
where $r$ is the distance from the center, and $b$ and $p$
are constants. A negative value of $p$ would indicate a
field that was strongest at the center, while a positive $p$
would give zero field at the center and stronger fields farther out.
Physically, we know by symmetry that the field is zero at the center,
so we expect $p$ to be positive.

                As in the example \ref{eg:divpointcharge},
we rewrite $\hat{\vc{r}}$ as $\vc{r}/ r$,
and to simplify the writing we
define $n= p-1$, so 
\begin{equation*}
                        \vc{E}         =     br^ n\vc{r}\eqquad.
\end{equation*}
Gauss' law in differential form is
\begin{equation*}
                        \divg\vc{E}         =     4\pi  k\rho\eqquad,  
\end{equation*}
so we want a field whose divergence is constant. For a field
of the form we guessed, the divergence has terms in it like
\begin{align*}
        \frac{\partial  E_{x}}{\partial  x}         
                &= \frac{\partial}{\partial  x}\left( br^{n} x\right) \\
                &=  b\left( nr^{ n-1}\frac{\partial  r}{\partial  x} x+%
                         r^ n\right) \\
\end{align*}
The partial derivative $\partial  r/\partial  x$ is easily calculated to be
 $x/ r$, so
\begin{equation*}
        \frac{\partial  E_{x}}{\partial  x}         
                =  b\left( nr^{ n-2} x^2+%
                         r^ n\right)
\end{equation*}
Adding in similar expressions for the other two terms in the
divergence, and making use of $x^2+ y^2+ z^2= r^2$,
we have
\begin{equation*}
                        \divg\vc{E}         =     b( n+3) r^ n\eqquad.  
\end{equation*}
This can indeed be constant, but only if $n$ is 0 or $-3$,
i.e., $p$ is 1 or $-2$.
The second solution gives a divergence which is constant and \emph{zero}\/:
this is the solution for the \emph{outside} of the sphere! The first
solution, which has the field directly proportional to $r$,
must be the one that applies to the inside of the sphere,
which is what we care about right now.
Equating the
coefficient in front to the one in Gauss' law, the field is
\begin{equation*}
                        \vc{E}         =    \frac{4\pi k\rho}{3} r\:\hat{\vc{r}}\eqquad.  
\end{equation*}
The field is zero at the center, and gets stronger and stronger as we approach
the surface.
\end{eg}

\startdqs

<% marg(0) %>
<%
  fig(
    'divmeterinsinewave',
    %q{Discussion question \ref{eg:divmeterinsinewave}.}
  )
%>
<% end_marg %>

\begin{dq}\label{eg:divmeterinsinewave}
As suggested by the figure, discuss the results you would get by inserting the div-meter
at various locations in the sine-wave field.
\end{dq}

<% end_sec('gauss-local') %>

<% begin_sec("Poisson's equation and Laplace's equation",nil,'poisson-eqn') %>

Gauss's law, $\divg \vc{E}=4\pi k \rho$, can also be stated in terms of the potential.
Since $\vc{E} = \nabla V$, we have $\divg\nabla V=4\pi k \rho$. If we work out
the combination of operators $\divg\nabla$ in a Cartesian coordinate system, we get 
$\partial^2/\partial x^2+\partial^2/\partial y^2+\partial^2/\partial z^2$, which is called
the Laplacian and notated $\nabla^2$.\label{laplacian-em}\index{Laplacian} The Laplacian
is discussed in more detail on p.~\pageref{laplacian-qm}. The version of Gauss's law
written in terms of the potential,
\begin{equation*}
  \nabla^2 V = 4\pi k \rho,
\end{equation*}
is called Poisson's equation, while in the special case of a vacuum, with $\rho=0$, we have
\begin{equation*}
  \nabla^2 V = 0,
\end{equation*}
known as Laplace's equation. Many problems in electrostatics can be stated in terms of
finding  potential that satisfies Laplace's equation, usually with some set of
\emph{boundary conditions}. For example, if an infinite parallel-plate capacitor has plates parallel
to the $x$-$y$ plane at  certain given potentials, then these plates form a boundary for the region
between the plates, and Laplace's equation has a solution in this region of the form
$V=az+b$. It's easy to verify that this is a solution of Laplace's equation, since
all three of the  partial derivatives vanish.

<% end_sec('poisson-eqn') %>

<% begin_sec("The method of images",nil,'electrostatics-images') %>
\index{images!method of (electrostatics)}
A car's radio antenna is usually in the form of a whip sticking up above its metal roof.
This is an example involving radio waves, which are time-varying electric and magnetic fields,
but a similar, simpler electrostatic example is the following. Suppose that we position a charge $q>0$
at a distance $\ell$ from a conducting plane. What is the resulting electric field? The conductor has
charges that are free to move, and due to the field of the charge $q$, we will end up with a
net concentration of negative charge in the part of the plane near $q$. The field in the vacuum surrounding
$q$ will be a sum of fields due to $q$ and fields due to these charges in the conducting plane.
The problem can be stated as that of finding a solution to Poisson's equation with the boundary
condition that $V=0$ at the conducting plane. Figure \subfigref{method-of-images}{1} shows the
kind of field lines we expect.

<% marg(0) %>
<%
  fig(
    'method-of-images',
    %q{The method of images.}
  )
%>
<% end_marg %>

This looks like a very complicated problem, but there is trick that allows us to find
a simple solution. We can convert the problem into an equivalent one in which the conductor
isn't present, but a fictitious \emph{image} charge $-q$ is placed at an equal distance behind
the plane, like a reflection in a mirror, as in figure
\subfigref{method-of-images}{2}. The field is then simply the sum of the fields of the
charges $q$ and $-q$, so we can either add the field vectors or add the potentials. By symmetry, the
field lines are perpendicular to the plane, so the plane is an surface of constant potential, as required.

<% end_sec('electrostatics-images') %>

\backofchapterboilerplate{efield}

<% end_sec() %>

<% begin_hw_sec %>

<% begin_hw('sparkplug',0) %>__incl(hw/sparkplug)<% end_hw() %>

<% begin_hw('distantrsquared') %>__incl(hw/distantrsquared)<% end_hw() %>

<% begin_hw('galaxy-mass') %>__incl(hw/galaxy-mass)<% end_hw() %>

<% begin_hw('chargemotioninfield',0) %>__incl(hw/chargemotioninfield)<% end_hw() %>

 % 

<% begin_hw('altedefunits',0) %>__incl(hw/altedefunits)<% end_hw() %>

 % 

<% begin_hw('dipole-change-origin') %>__incl(hw/dipole-change-origin)<% end_hw() %>

<% begin_hw('dipolechoiceoforigin') %>__incl(hw/dipolechoiceoforigin)<% end_hw() %>

<% begin_hw('lineandsquaredipoles') %>__incl(hw/lineandsquaredipoles)<% end_hw() %>

<% begin_hw('quadrupole') %>__incl(hw/quadrupole)<% end_hw() %>

<% begin_hw('ringve',0) %>__incl(hw/ringve)<% end_hw() %>

<% begin_hw('esquare') %>__incl(hw/esquare)<% end_hw() %>

<% marg(50) %>
<%
  fig(
    'lineandsquaredipoles',
    %q{Problem \ref{hw:lineandsquaredipoles}.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'hw-esquare',
    %q{Problem \ref{hw:esquare}.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'hw-neuron',
    %q{Problem \ref{hw:neuronfield}.}
  )
%>
<% end_marg %>

 % 

<% begin_hw('dipolefarfield',0) %>__incl(hw/dipolefarfield)<% end_hw() %>

 % 

<% begin_hw('neuronfield') %>__incl(hw/neuronfield)<% end_hw() %>

<% begin_hw('proton-in-nonuniform-field') %>__incl(hw/proton-in-nonuniform-field)<% end_hw() %>

<% begin_hw('dipolev') %>__incl(hw/dipolev)<% end_hw() %>

<% begin_hw('screened') %>__incl(hw/screened)<% end_hw() %>

<% begin_hw('carbondioxide',0) %>__incl(hw/carbondioxide)<% end_hw() %>

<% begin_hw('electron-cloud') %>__incl(hw/electron-cloud)<% end_hw() %>

<% marg(0) %>
<%
  fig(
    'hw-dipolemidplane',
    %q{Problem \ref{hw:dipolemidplane}.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'hw-hyperbolic',
    %q{Problem \ref{hw:hyperbolic}.}
  )
%>
<% end_marg %>

<% begin_hw('dipolemidplane',2) %>__incl(hw/dipolemidplane)<% end_hw() %>


<% begin_hw('hyperbolic',0) %>__incl(hw/hyperbolic)<% end_hw() %>

<% begin_hw('quadraticvoltage') %>__incl(hw/quadraticvoltage)<% end_hw() %>

<% begin_hw('estrips') %>__incl(hw/estrips)<% end_hw() %>

<% marg(0) %>
<%
  fig(
    'halfinfinitecylinder',
    %q{Problem \ref{hw:halfinfinitecylinder}.}
  )
%>
<% end_marg %>

<% begin_hw('halfinfinitecylinder') %>__incl(hw/halfinfinitecylinder)<% end_hw() %>

<% begin_hw('lightning') %>__incl(hw/lightning)<% end_hw() %>

<% begin_hw('epointinfty') %>__incl(hw/epointinfty)<% end_hw() %>

<% marg(0) %>
<%
  fig(
    'hw-neuron',
    %q{Problem \ref{hw:neuronenergy}.},
    {'suffix'=>'2'}
  )
%>

\spacebetweenfigs

<%
  fig(
    'hwcubecaps',
    %q{Problem \ref{hw:cubecaps}.}
  )
%>
<% end_marg %>

<% begin_hw('neuronenergy') %>__incl(hw/neuronenergy)<% end_hw() %>

 % 

<% begin_hw('cubecaps') %>__incl(hw/cubecaps)<% end_hw() %>

<% begin_hw('earthcap',0) %>__incl(hw/earthcap)<% end_hw() %>

<% marg(0) %>
<%
  fig(
    'hw-infinite-strip',
    %q{Problem \ref{hw:infinite-strip}.}
  )
%>
<% end_marg %>

<% begin_hw('infinite-strip') %>__incl(hw/infinite-strip)<% end_hw() %>

<% begin_hw('charged-solid-cyl',1) %>__incl(hw/charged-solid-cyl)<% end_hw() %>

<% begin_hw('vedgedisk',2) %>__incl(hw/vedgedisk)<% end_hw() %>

<% begin_hw('capenergy',0) %>__incl(hw/capenergy)<% end_hw() %>

<% begin_hw('seriescapacitors') %>__incl(hw/seriescapacitors)<% end_hw() %>

<% begin_hw('complex-trig') %>__incl(hw/complex-trig)<% end_hw() %>

<% begin_hw('lrcunits',0) %>__incl(hw/lrcunits)<% end_hw() %>

<% begin_hw('parallelinductors') %>__incl(hw/parallelinductors)<% end_hw() %>

<% begin_hw('i-to-the-i') %>__incl(hw/i-to-the-i)<% end_hw() %>

<% begin_hw('strayimpedance',0) %>__incl(hw/strayimpedance)<% end_hw() %>

<% begin_hw('lomega',0) %>__incl(hw/lomega)<% end_hw() %>

<% begin_hw('charged-long-box',1) %>__incl(hw/charged-long-box)<% end_hw() %>

<% begin_hw('charge-gun',1) %>__incl(hw/charge-gun)<% end_hw() %>

<% begin_hw('fmradiolrc',0) %>__incl(hw/fmradiolrc)<% end_hw() %>

<% begin_hw('rc-par-impedance') %>__incl(hw/rc-par-impedance)<% end_hw() %>

<% begin_hw('lrc-z-plot') %>__incl(hw/lrc-z-plot)<% end_hw() %>

<% begin_hw('series-to-parallel') %>__incl(hw/series-to-parallel)<% end_hw() %>

<% begin_hw('coaxcap') %>__incl(hw/coaxcap)<% end_hw() %>

<% begin_hw('gauss-const-e',1) %>__incl(hw/gauss-const-e)<% end_hw() %>

<% begin_hw('lc-e-sharing',1) %>__incl(hw/lc-e-sharing)<% end_hw() %>

<% begin_hw('solidchargedcylinder') %>__incl(hw/solidchargedcylinder)<% end_hw() %>

<% begin_hw('gauss-simple-e',1) %>__incl(hw/gauss-simple-e)<% end_hw() %>

<% begin_hw('gauss-em-wave',1) %>__incl(hw/gauss-em-wave)<% end_hw() %>

<% begin_hw('charged-cylinder-div') %>__incl(hw/charged-cylinder-div)<% end_hw() %>

<% begin_hw('divtrans') %>__incl(hw/divtrans)<% end_hw() %>

<% begin_hw('div-rot-invariant',2) %>__incl(hw/div-rot-invariant)<% end_hw() %>

<% begin_hw('cylindrical-charge-given-field') %>__incl(hw/cylindrical-charge-given-field)<% end_hw() %>

  % answer: c/2ku (units check)

<% begin_hw('addition-theorem-for-sine') %>__incl(hw/addition-theorem-for-sine)<% end_hw() %>

<% begin_hw('cube-roots-of-unity') %>__incl(hw/cube-roots-of-unity)<% end_hw() %>

<% begin_hw('factor-cubic') %>__incl(hw/factor-cubic)<% end_hw() %>

<% begin_hw('dipole-in-nonuniform-field') %>__incl(hw/dipole-in-nonuniform-field)<% end_hw() %>

<% begin_hw('cosmic-ray-lightning') %>__incl(hw/cosmic-ray-lightning)<% end_hw() %>

<% begin_hw('image-charge-unstable') %>__incl(hw/image-charge-unstable)<% end_hw() %>

<% marg(0) %>
<%
  fig(
    'hw-image-charge-corner',
    %q{Problem \ref{hw:image-charge-corner}.}
  )
%>
<% end_marg %>

<% begin_hw('image-charge-corner',2) %>__incl(hw/image-charge-corner)<% end_hw() %>

\pagebreak

<% begin_hw('complex-cube-root') %>__incl(hw/complex-cube-root)<% end_hw() %>

<% marg(300) %>
<%
  fig(
    'hw-complex-cube-root',
    %q{Problem \ref{hw:complex-cube-root}.}
  )
%>
<% end_marg %>

<% begin_hw('complex-hundredth-derivative') %>__incl(hw/complex-hundredth-derivative)<% end_hw() %>

<% end_hw_sec %>

 % ==================================================================== 
 % ==================================================================== 
 % ==================================================================== 
\begin{exsection}
\extitle{A}{Field Vectors}

\noindent Apparatus:
\begin{indentedblock}

3 solenoids

DC power supply

compass

ruler

cut-off plastic cup
\end{indentedblock}

At this point you've studied the gravitational field, $\vc{g}$,
and the electric field, $\vc{E}$, but not the magnetic field,
$\vc{B}$. However, they all have some of the same mathematical
behavior: they act like vectors. Furthermore, magnetic
fields are the easiest to manipulate in the lab. Manipulating
gravitational fields directly would require futuristic
technology capable of moving planet-sized masses around!
Playing with electric fields is not as ridiculously
difficult, but static electric charges tend to leak off
through your body to ground, and static electricity effects
are hard to measure numerically. Magnetic fields, on the
other hand, are easy to make and control. Any moving charge,
i.e., any current, makes a magnetic field.

A practical device for making a strong magnetic field is
simply a coil of wire, formally known as a solenoid. The
field pattern surrounding the solenoid gets stronger or
weaker in proportion to the amount of current passing through the wire.

1. With a single solenoid connected to the power supply and
laid with its axis horizontal, use a magnetic compass to
explore the field pattern inside and outside it. The compass
shows you the field vector's direction, but not its
magnitude, at any point you choose. Note that the field the
compass experiences is a combination (vector sum) of the
solenoid's field and the earth's field. 

2. What happens when you bring the compass extremely far
away from the solenoid? 

\vspace{20mm}

What does this tell you about the way the solenoid's field
varies with distance? 

\vspace{20mm}

Thus although the compass doesn't tell you the field
vector's magnitude numerically, you can get at least some
general feel for how it depends on distance.

\pagebreak[4]

3. The figure below is a cross-section of the solenoid
in the plane containing its axis. Make a sea-of-arrows sketch of the magnetic field
in this plane. The length
of each arrow should at least approximately reflect the
strength of the magnetic field at that point.

\vspace{30mm}

\hspace{50mm}\anonymousinlinefig{map-solenoid-field}

\vspace{30mm}

Does the field seem to have sources or sinks?

4. What do you think would happen to your sketch if
you reversed the wires?

\vspace{20mm}

Try it.

\pagebreak[4]

5. Now hook up the two solenoids in parallel. You are going
to measure what happens when their two fields combine
at a certain point in space. As you've seen already, the solenoids'
nearby fields are much stronger than the earth's field; so
although we now theoretically have three fields involved
(the earth's plus the two solenoids'), it will be safe to
ignore the earth's field. The basic idea here is to place
the solenoids with their axes at some angle to each other,
and put the compass at the intersection of their axes, so
that it is the same distance from each solenoid. Since the
geometry doesn't favor either solenoid, the only factor that
would make one solenoid influence the compass more than the
other is current. You can use the cut-off plastic cup as a
little platform to bring the compass up to the same level as
the solenoids' axes.

a)What do you think will happen with the solenoids' axes at
90 degrees to each other, and equal currents? Try it. Now
represent the vector addition of the two magnetic fields
with a diagram. Check your diagram with your instructor to
make sure you're on the right track.

\hspace{30mm}\anonymousinlinefig{crossed-fields}

b) Now try to make a similar diagram of  what would happen if
you switched the wires on one of the solenoids. 

\vspace{25mm}

After predicting what the compass will do, try it and
see if you were right.

c)Now suppose you were to go back to the arrangement you had
in part a, but you changed one of the currents to half its
former value. Make a vector addition diagram, and use trig
to predict the angle. 

\anonymousinlinefig{crossed-fields-half}

Try it. To cut the current to one of the solenoids in half,
an easy and accurate method is simply to put the third solenoid in
series with it, and put that third solenoid so far away that its
magnetic field doesn't have any significant effect on the compass.

\end{exsection}

<% end_chapter() %>
