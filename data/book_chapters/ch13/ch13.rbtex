
<%
  require "../eruby_util.rb"
%>

<%
  chapter(
    '13',
    %q{Quantum Physics},
    'ch:quantum',
    '',
    {'opener'=>''}
  )
%>

<% begin_sec("Rules of randomness",0,'randomness') %>

\epigraphlong{Given for one instant an intelligence which could comprehend
all the forces by which nature is animated and the
respective positions of the things which compose it...nothing
would be uncertain, and the future as the past would be laid
out before its eyes.}{Pierre Simon de Laplace, 1776}\index{Laplace, Pierre Simon de}

\epigraphlong{The energy produced by the atom is a very poor kind of
thing. Anyone who expects a source of power from the
transformation of these atoms is talking moonshine.}{Ernest Rutherford, 1933}
\index{Rutherford!Ernest}

\epigraphlong{The Quantum Mechanics is very imposing. But an inner voice
tells me that it is still not the final truth. The theory
yields much, but it hardly brings us nearer to the secret of
the Old One. In any case, I am convinced that He does not play 
dice.}{Albert Einstein}\index{Einstein, Albert}

<% marg(50) %>
<%
  fig(
    'volcano',
    %q{%
      In 1980, the continental U.S. got its first
      taste of active volcanism in recent memory with the eruption of
      Mount St. Helens.
    }
  )
%>
<% end_marg %>

However radical \index{Newton!Isaac}Newton's clockwork
universe seemed to his contemporaries, by the early
twentieth century it had become a sort of smugly accepted
dogma. Luckily for us, this deterministic picture of the
universe breaks down at the atomic level. The clearest
demonstration that the laws of physics contain elements of
randomness is in the behavior of radioactive atoms. Pick two
identical atoms of a radioactive isotope, say the naturally
occurring uranium 238, and watch them carefully. They will
decay at different times, even though there was no
difference in their initial behavior.

We would be in big trouble if these atoms' behavior was as
predictable as expected in the Newtonian world-view, because
radioactivity is an important source of heat for our planet.
In reality, each atom chooses a random moment at which to
release its energy, resulting in a nice steady heating
effect. The earth would be a much colder planet if only
sunlight heated it and not radioactivity. Probably there
would be no volcanoes, and the oceans would never have been
liquid. The deep-sea \index{geothermal vents}geothermal
vents in which life first evolved would never have existed.
But there would be an even worse consequence if radioactivity
was deterministic: after a few billion years of peace, all
the uranium 238 atoms in our planet would presumably pick
the same moment to decay. The huge amount of stored nuclear
energy, instead of being spread out over eons, would all be
released at one instant, blowing our whole planet to 
Kingdom Come.\footnote{This is under the assumption that all the uranium
atoms were created at the same time. In reality, we have only
a general idea of the processes that might have created the
heavy elements in the nebula from which our solar system
condensed. Some portion of them may have come from nuclear reactions
in supernova explosions in that particular nebula, but some may
have come from previous supernova explosions throughout our
galaxy, or from exotic events like collisions of white dwarf
stars.}

The new version of physics, incorporating certain kinds of
randomness, is called quantum physics (for reasons that will
become clear later). It represented such a dramatic break
with the previous, deterministic tradition that everything
that came before is considered ``\index{classical physics}classical,''
even the theory of relativity. This chapter is
a basic introduction to \index{quantum physics}quantum physics.

\startdq

\begin{dq}
I said ``Pick two identical atoms of a radioactive
isotope.'' Are two atoms really identical? If their
electrons are orbiting the nucleus, can we distinguish each
atom by the particular arrangement of its electrons at
some instant in time?
\end{dq}

<% begin_sec("Randomness isn't random.") %>

\index{Einstein!and randomness}Einstein's distaste for
\index{randomness}randomness, and his association of
determinism with divinity, goes back to the \index{Enlightenment}Enlightenment
conception of the universe as a gigantic piece of clockwork
that only had to be set in motion initially by the Builder.
Many of the founders of quantum mechanics were interested in
possible links between physics and Eastern and Western
religious and philosophical thought, but every educated
person has a different concept of religion and philosophy.
Bertrand \index{Russell!Bertrand}Russell remarked, ``Sir
Arthur \index{Eddington!Arthur}Eddington deduces religion
from the fact that atoms do not obey the laws of mathematics.
Sir James \index{Jeans!James}Jeans deduces it from the
fact that they do.''

Russell's witticism, which implies incorrectly that
mathematics cannot describe randomness, remind us how
important it is not to oversimplify this question of
randomness. You should not simply surmise, ``Well, it's all
random, anything can happen.'' For one thing, certain things
simply cannot happen, either in classical physics or quantum
physics. The conservation laws of mass, energy, momentum,
and angular momentum are still valid, so for instance
processes that create energy out of nothing are not just
unlikely according to quantum physics, they are impossible.

A useful analogy can be made with the role of randomness in
\index{evolution!randomness in}evolution. \index{Darwin, Charles}Darwin
was not the first biologist to suggest that species changed
over long periods of time. His two new fundamental ideas
were that (1) the changes arose through random genetic
variation, and (2) changes that enhanced the organism's
ability to survive and reproduce would be preserved, while
maladaptive changes would be eliminated by natural
selection. Doubters of evolution often consider only the
first point, about the randomness of natural variation, but
not the second point, about the systematic action of natural
selection. They make statements such as, ``the development
of a complex organism like Homo sapiens via random chance
would be like a whirlwind blowing through a junkyard and
spontaneously assembling a jumbo jet out of the scrap
metal.'' The flaw in this type of reasoning is that it
ignores the deterministic constraints on the results of
random processes. For an atom to violate conservation of
energy is no more likely than the conquest of the world by
chimpanzees next year.

\startdq

\begin{dq}
Economists often behave like wannabe physicists, probably
because it seems prestigious to make numerical calculations
instead of talking about human relationships and organizations
like other social scientists. Their striving to make
economics work like Newtonian physics extends to a parallel
use of mechanical metaphors, as in the concept of a market's
supply and demand acting like a self-adjusting machine, and
the idealization of people as economic automatons who
consistently strive to maximize their own wealth. What
evidence is there for randomness rather than mechanical
determinism in economics?
\end{dq}

<% end_sec() %>

<% begin_sec("Calculating randomness",nil,'calculating-randomness') %>

You should also realize that even if something is random, we
can still understand it, and we can still calculate
probabilities numerically. In other words, physicists are
good bookmakers. A good bookmaker can calculate the odds
that a horse will win a race much more accurately than an
inexperienced one, but nevertheless cannot predict what will
happen in any particular race.

<% begin_sec("Statistical independence") %>
\index{independence!statistical}
As an illustration of a general technique for calculating
odds, suppose you are playing a 25-cent slot machine. Each
of the three wheels has one chance in ten of coming up with
a cherry. If all three wheels come up cherries, you win
\$100. Even though the results of any particular trial are
random, you can make certain quantitative predictions.
First, you can calculate that your odds of winning on any
given trial are $1/10\times1/10\times1/10=1/1000=0.001$.\label{slot-machine-game}
Here, I am
representing the probabilities as numbers from 0 to 1, which
is clearer than statements like ``The odds are 999 to 1,''
and makes the calculations easier. A probability of 0
represents something impossible, and a probability of 1
represents something that will definitely happen.  

Also, you can say that any given trial is equally likely to
result in a win, and it doesn't matter whether you have won
or lost in prior games. Mathematically, we say that each
trial is statistically independent, or that separate games
are uncorrelated.  Most gamblers are mistakenly convinced
that, to the contrary, games of chance are correlated. If
they have been playing a slot machine all day, they are
convinced that it is ``getting ready to pay,'' and they do
not want anyone else playing the machine and ``using up''
the jackpot that they ``have coming.'' In other words, they
are claiming that a series of trials at the slot machine is
negatively correlated, that losing now makes you more likely
to win later. Craps players claim that you should go to a
table where the person rolling the dice is ``hot,'' because
she is likely to keep on rolling good numbers. Craps
players, then, believe that rolls of the dice are positively
correlated, that winning now makes you more likely to win later.

My method of calculating the probability of winning on the
slot machine was an example of the following important rule
for calculations based on independent probabilities:

\index{independent probabilities!law of}\label{statistical-independence}
\begin{lessimportant}[the law of independent probabilities]
If the probability of one event happening is $P_A$, and the
probability of a second statistically independent event
happening is $P_B$, then the probability that they will both
occur is the product of the probabilities, $P_AP_B$. If
there are more than two events involved, you simply
keep on multiplying.
\end{lessimportant}

This can be taken as the definition of statistical independence.\index{independence!statistical}

Note that this only applies to independent probabilities.
For instance, if you have a nickel and a dime in your
pocket, and you randomly pull one out, there is a probability
of 0.5 that it will be the nickel. If you then replace the
coin and again pull one out randomly, there is again a
probability of 0.5 of coming up with the nickel, because the
probabilities are independent. Thus, there is a probability
of 0.25 that you will get the nickel both times.

Suppose instead that you do not replace the first coin
before pulling out the second one. Then you are bound to
pull out the other coin the second time, and there is no way
you could pull the nickel out twice. In this situation, the
two trials are not independent, because the result of the
first trial has an effect on the second trial. The law of
independent probabilities does not apply, and the probability
of getting the nickel twice is zero, not 0.25.

Experiments have shown that in the case of radioactive
decay, the probability that any nucleus will decay during a
given time interval is unaffected by what is happening to
the other nuclei, and is also unrelated to how long it has
gone without decaying. The first observation makes sense,
because nuclei are isolated from each other at the centers
of their respective atoms, and therefore have no physical
way of influencing each other. The second fact is also
reasonable, since all atoms are identical. Suppose we wanted
to believe that certain atoms were ``extra tough,'' as
demonstrated by their history of going an unusually long
time without decaying. Those atoms would have to be
different in some physical way, but nobody has ever
succeeded in detecting differences among atoms. There is no
way for an atom to be changed by the experiences it
has in its lifetime.

<% end_sec() %>

<% begin_sec("Addition of probabilities") %>
\index{probabilities!addition of}

The law of independent probabilities tells us to use
multiplication to calculate the probability that both A and
B will happen, assuming the probabilities are independent.
What about the probability of an ``or'' rather than an
``and''? If two events A and $B$ are mutually exclusive,
then the probability of one or the other occurring is the
sum $P_A+P_B$. For instance, a bowler might have a 30\%
chance of getting a strike (knocking down all ten pins) and
a 20\% chance of knocking down nine of them. The bowler's
chance of knocking down either nine pins or ten pins is therefore 50\%.

It does not make sense to add probabilities of things that
are not mutually exclusive, i.e., that could both happen. Say
I have a 90\% chance of eating lunch on any given day, and a
90\% chance of eating dinner. The probability that I will
eat either lunch or dinner is not 180\%.  

<% end_sec() %>

<% begin_sec("Normalization") %>
\index{probabilities!normalization of}\index{normalization}
If I spin a globe and randomly pick a point on it, I have
about a 70\% chance of picking a point that's in an ocean
and a 30\% chance of picking a point on land. The probability
of picking either water or land is $70\%+30\%=100\%$. Water
and land are mutually exclusive, and there are no other
possibilities, so the probabilities had to add up to 100\%.
It works the same if there are more than two possibilities
---  if you can classify all possible outcomes into a list
of mutually exclusive results, then all the probabilities
have to add up to 1, or 100\%. This property of probabilities
is known as normalization.

<% marg(20) %>
<%
  fig(
    'globe',
    %q{%
      Normalization: the probability of picking land plus
      the probability of picking water adds up to 1.
    }
  )
%>
<% end_marg %>

\index{averages}
<% end_sec() %>

<% begin_sec("Averages") %>

Another way of dealing with randomness is to take averages.
The casino knows that in the long run, the number of times
you win will approximately equal the number of times you
play multiplied by the probability of winning. In the slot-machine game
described on page \pageref{slot-machine-game}, where the probability of winning is 0.001,
if you spend a week playing, and pay \$2500 to play 10,000
times, you are likely to win about 10 times $(10,000\times0.001=10)$,
and collect \$1000. On the average, the casino will make a
profit of \$1500 from you. This is an example of the following rule.

\index{averages!rule for calculating}
\begin{lessimportant}[Rule for Calculating Averages]
If you conduct $N$ identical, statistically independent
trials, and the probability of success in each trial is $P$,
then on the average, the total number of successful trials
will be $NP$. If $N$ is large enough, the relative
error in this estimate will become small.
\end{lessimportant}

The statement that the rule for calculating averages gets
more and more accurate for larger and larger $N$ (known
popularly as the ``law of averages'') often provides a
correspondence principle that connects classical and quantum
physics. For instance, the amount of power produced by a
nuclear power plant is not random at any detectable level,
because the number of atoms in the reactor is so large. In
general, random behavior at the atomic level tends to
average out when we consider large numbers of atoms, which
is why physics seemed deterministic before physicists
learned techniques for studying atoms individually.

<% marg(300) %>
<% fig('kolmogorov','',{__incl(text/kolmogorov)})%>
<% end_marg %>

We can achieve great precision with averages in quantum
physics because we can use identical atoms to reproduce
exactly the same situation many times. If we were betting on
horses or dice, we would be much more limited in our
precision. After a thousand races, the horse would be ready
to retire. After a million rolls, the dice would be worn out.

When the number of trials is large, the accuracy of averages follows
from the fact that the frequency of an event gets close to its
probability. This is known as the law of large numbers. 

The sidebar summarizes five basic facts 
that form the basis of probability theory.

<% self_check('independence',<<-'SELF_CHECK'
Which of the following things \emph{must} be independent,
which \emph{could} be independent, and which definitely are
\emph{not} independent?
(1) the probability of successfully making two free-throws
in a row in basketball; (2) the probability that it will rain in London tomorrow and
the probability that it will rain on the same day in a
certain city in a distant galaxy; 
(3) your probability of dying today and of dying tomorrow.
  SELF_CHECK
  ) %>

\startdqs

\begin{dq}
Newtonian physics is an essentially perfect approximation
for describing the motion of a pair of dice. If Newtonian
physics is deterministic, why do we consider the result of
rolling dice to be random?
\end{dq}

<% marg(-300) %>

<%
  fig(
    'dice',
    %q{Why are dice random?}
  )
%>
<% end_marg %>

\begin{dq}
Why isn't it valid to define randomness by saying that
randomness is when all the outcomes are equally likely?
\end{dq}

\begin{dq}\label{dq:meaning-of-randomness}
The sequence of digits 121212121212121212 seems clearly
nonrandom, and 41592653589793 seems random. The latter
sequence, however, is the decimal form of pi, starting with
the third digit. There is a story about the Indian
mathematician Ramanujan, a self-taught prodigy, that a
friend came to visit him in a cab, and remarked that the
number of the cab, 1729, seemed relatively uninteresting.
Ramanujan replied that on the contrary, it was very
interesting because it was the smallest number that could be
represented in two different ways as the sum of two cubes.
The Argentine author Jorge Luis Borges wrote a short story
called ``The Library of Babel,'' in which he imagined a
library containing every book that could possibly be written
using the letters of the alphabet. It would include a book
containing only the repeated letter ``a;'' all the ancient
Greek tragedies known today, all the lost Greek tragedies,
and millions of Greek tragedies that were never actually
written; your own life story, and various incorrect versions
of your own life story; and countless anthologies containing
a short story called ``The Library of Babel.'' Of course, if
you picked a book from the shelves of the library, it would
almost certainly look like a nonsensical sequence of letters
and punctuation, but it's always possible that the seemingly
meaningless book would be a science-fiction screenplay
written in the language of a Neanderthal tribe, or the lyrics to a set of
incomparably beautiful love songs written in a language that
never existed. In view of these examples, what does it
really mean to say that something is random?
\end{dq}

<% end_sec() %>

<% end_sec('calculating-randomness') %>

<% begin_sec("Probability distributions") %>
\index{probability distributions}
So far we've discussed random processes having only two
possible outcomes: yes or no, win or lose, on or off. More
generally, a random process could have a result that is a
number. Some processes yield integers, as when you roll a
die and get a result from one to six, but some are not
restricted to whole numbers, for example the number of
seconds that a uranium-238 atom will exist before undergoing radioactive decay.

<% marg(0) %>
<%
  fig(
    'single-die',
    %q{Probability distribution for the result of rolling a single die.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'two-dice',
    %q{Rolling two dice and adding them up.}
  )
%>
<% end_marg %>

Consider a throw of a die. If the die is honest, then we
expect all six values to be equally likely. Since all six
probabilities must add up to 1, then the probability of any
particular value must be 1/6. We can summarize
this in a graph, \figref{single-die}. Areas under the curve can be
interpreted as total probabilities. For instance, the area
under the curve from 1 to 3 is $1/6+1/6+1/6=1/2$, so the
probability of getting a result from 1 to 3 is 1/2. The
function shown on the graph is called the probability distribution.

Figure \figref{two-dice} shows the probabilities of various results
obtained by rolling two dice and adding them together, as in
the game of craps. The probabilities are not all the same.
There is a small probability of getting a two, for example,
because there is only one way to do it, by rolling a one and
then another one. The probability of rolling a seven is high
because there are six different ways to do it: 1+6, 2+5, etc.

If the number of possible outcomes is large but finite, for
example the number of hairs on a dog, the graph would start
to look like a smooth curve rather than a ziggurat.

What about probability distributions for random numbers that
are not integers? We can no longer make a graph with
probability on the $y$ axis, because the probability of
getting a given exact number is typically zero. For
instance, there is zero probability that a radioactive atom
will last for \emph{exactly} 3 seconds, since there is are
infinitely many possible results that are close to 3 but not
exactly three: 2.999999999999999996876876587658465436, for
example. It doesn't usually make sense, therefore, to talk
about the probability of a single numerical result, but it
does make sense to talk about the probability of a certain
range of results. For instance, the probability that an atom
will last more than 3 and less than 4 seconds is a perfectly
reasonable thing to discuss. We can still summarize the
probability information on a graph, and we can still
interpret areas under the curve as probabilities.

But the $y$ axis can no longer be a unitless probability
scale. In radioactive decay, for example, we want the $x$
axis to have units of time, and we want areas under the
curve to be unitless probabilities. The area of a single
square on the graph paper is then
\begin{gather*}
\text{(unitless area of a square)}   \\
= \text{(width of square with time units)}\\
         \times \text{(height of square)}\eqquad.
\end{gather*}
If the units are to cancel out, then the height of the
square must evidently be a quantity with units of inverse
time. In other words, the $y$ axis of the graph is to be
interpreted as probability per unit time, not probability.

<% marg(30) %>
<%
  fig(
    'human-height',
    %q{%
      A probability distribution for height of human adults 
      (not real data).
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'human-height-tail',
    %q{Example \ref{eg:basketball}.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'average',
    %q{The average of a probability distribution.}
  )
%>
<% end_marg %>

Figure \figref{human-height} shows another example, a probability distribution
for people's height. This kind of bell-shaped curve is quite common.

<% self_check('rangeofheights',<<-'SELF_CHECK'

Compare the number of people with heights in the range of
130-135 cm to the number in the range 135-140.
  SELF_CHECK
  ) %>

%%%%%%%%%%% basketball example %%%%%%%%%%%%%
__incl(eg/basketball)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{probability  distributions!widths of}
\index{probability  distributions!averages of}

<% begin_sec("Average and width of a probability distribution") %>

If the next Martian you meet asks you, ``How tall is an
adult human?,'' you will probably reply with a statement
about the average human height, such as ``Oh, about 5 feet 6
inches.'' If you wanted to explain a little more, you could
say, ``But that's only an average. Most people are somewhere
between 5 feet and 6 feet tall.'' Without bothering to draw
the relevant bell curve for your new extraterrestrial
acquaintance, you've summarized the relevant information by
giving an average and a typical range of variation.  

The average of a probability distribution can be defined
geometrically as the horizontal position at which it could
be balanced if it was constructed out of cardboard. A
convenient numerical measure of the amount of variation
about the average, or amount of uncertainty, is the full
width at half maximum, or FWHM, shown in figure \figref{fwhm}.\index{FWHM}\index{full width at half maximum}

A great deal more could be said about this topic, and indeed
an introductory statistics course could spend months on ways
of defining the center and width of a distribution. Rather
than force-feeding you on mathematical detail or techniques
for calculating these things, it is perhaps more relevant to
point out simply that there are various ways of defining
them, and to inoculate you against the misuse of certain definitions.

<% marg(0) %>
<%
  fig(
    'fwhm',
    %q{The full width at half maximum (FWHM) of a probability distribution.}
  )
%>
<% end_marg %>

The average is not the only possible way to say what is a
typical value for a quantity that can vary randomly; another
possible definition is the median,\index{median} defined as the value that
is exceeded with 50\% probability. When discussing incomes
of people living in a certain town, the average could be
very misleading, since it can be affected massively if a
single resident of the town is Bill Gates. Nor is the FWHM
the only possible way of stating the amount of random
variation; another possible way of measuring it is the
standard deviation (defined as the square root of the
average squared deviation from the average value).

 % 

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Exponential decay and half-life",nil,'half-life') %>

\index{decay!exponential}\index{exponential decay}

<% begin_sec("Half-life") %>
\index{half-life}
Most people know that radioactivity ``lasts a certain amount
of time,'' but that simple statement leaves out a lot. As an
example, consider the following medical procedure used to
diagnose thyroid function. A very small quantity of the
isotope $^{131}\zu{I}$, produced in a nuclear reactor, is fed to
or injected into the patient. The body's biochemical systems
treat this artificial, radioactive isotope exactly the same
as $^{127}\zu{I}$, which is the only naturally occurring type.
(Nutritionally, iodine is a necessary trace element. Iodine
taken into the body is partly excreted, but the rest becomes
concentrated in the thyroid gland. Iodized salt has had
iodine added to it to prevent the nutritional deficiency
known as \index{goiters}goiters, in which the \index{iodine}iodine-starved
thyroid becomes swollen.) As the $^{131}\zu{I}$ undergoes beta
decay, it emits electrons, neutrinos, and gamma rays. The
gamma rays can be measured by a detector passed over the
patient's body. As the radioactive iodine becomes concentrated
in the thyroid, the amount of gamma radiation coming from
the thyroid becomes greater, and that emitted by the rest of
the body is reduced. The rate at which the iodine concentrates
in the thyroid tells the doctor about the health of the thyroid.

If you ever undergo this procedure, someone will presumably
explain a little about radioactivity to you, to allay your
fears that you will turn into the Incredible Hulk, or that
your next child will have an unusual number of limbs. Since
iodine stays in your thyroid for a long time once it gets
there, one thing you'll want to know is whether your thyroid
is going to become radioactive forever. They may just tell
you that the radioactivity ``only lasts a certain amount of
time,'' but we can now carry out a quantitative derivation
of how the radioactivity really will die out.

Let $P_{surv}(t)$ be the probability that an iodine atom
will survive without decaying for a period of at least $t$.
It has been experimentally measured that half all $^{131}\zu{I}$
atoms decay in 8 hours, so we have
\begin{equation*}
                P_{surv}(8\ \zu{hr})         =  0.5\eqquad.  
\end{equation*}

Now using the law of independent probabilities, the
probability of surviving for 16 hours equals the probability
of surviving for the first 8 hours multiplied by the
probability of surviving for the second 8 hours,
\begin{align*}
                P_{surv}(16\ \zu{hr})         &=  0.50\times0.50  \\
                                 &=  0.25\eqquad.  
\end{align*}
Similarly we have
\begin{align*}
                P_{surv}(24\ \zu{hr})         &=  0.50\times0.5\times0.5  \\
                                 &=  0.125\eqquad.  
\end{align*}
Generalizing from this pattern, the probability of surviving
for any time $t$ that is a multiple of 8 hours is
\begin{equation*}
                P_{surv}(t)                 =            0.5^{t/8\ \zu{hr}}\eqquad.
\end{equation*}
We now know how to find the probability of survival at
intervals of 8 hours, but what about the points in time in
between? What would be the probability of surviving for 4
hours? Well, using the law of independent probabilities again, we have
\begin{equation*}
  P_{surv}(8\ \zu{hr})  =  P_{surv}(4\ \zu{hr}) \times P_{surv}(4\ \zu{hr})\eqquad,  
\end{equation*}
which can be rearranged to give
\begin{align*}
  P_{surv}(4\ \zu{hr})          &= \sqrt{P_{surv}(8\ \zu{hr})}   \\
                                 &=  \sqrt{0.5}  \\
                                 &=  0.707\eqquad.  
\end{align*}
This is exactly what we would have found simply by plugging
in $P_{surv}(t)=0.5^{t/8\ \zu{hr}}$ and ignoring the restriction to
multiples of 8 hours. Since 8 hours is the amount of time
required for half of the atoms to decay, it is known as the
half-life, written $t_{1/2}$. The general rule is as follows:

\begin{lessimportant}[Exponential Decay Equation]
\begin{equation*}
                P_{surv}(t)  =    0.5^{t/t_{1/2}}
\end{equation*}
\end{lessimportant}

Using the rule for calculating averages, we can also find
the number of atoms, $N(t)$, remaining in a sample at time $t\/$:
\begin{equation*}
                N(t)  =  N(0)    \times 0.5^{t/t_{1/2}} 
\end{equation*}
Both of these equations have graphs that look like dying-out
exponentials, as in the example below.

\begin{eg}{Radioactive contamination at Chernobyl}
\egquestion One of the most dangerous radioactive isotopes
released by the Chernobyl disaster in 1986 was $^{90}\zu{Sr}$,
whose half-life is 28 years. (a) How long will it be before
the contamination is reduced to one tenth of its original
level? (b) If a total of $10^{27}$  atoms was released,
about how long would it be before not a single atom was left?

\eganswer (a) We want to know the amount of time that a
$^{90}\zu{Sr}$ nucleus has a probability of 0.1 of surviving.
Starting with the exponential decay formula,
\begin{equation*}
                P_{surv}  =    0.5^{t/t_{1/2}}\eqquad,  
\end{equation*}
we want to solve for $t$. Taking natural logarithms of both sides,
\begin{equation*}
                \ln  P  = \frac{t}{t_{1/2}}\ln 0.5\eqquad,  
\end{equation*}
so
\begin{equation*}
                t  =    \frac{t_{1/2}}{\ln 0.5}\ln P
\end{equation*}
Plugging in $P=0.1$ and $t_{1/2}=28$ years, we get $t=93$ years.

(b) This is just like the first part, but $P=10^{-27}$ . The
result is about 2500 years.
\end{eg}

<%
  fig(
    'carbon-fourteen',
    %q{%
      Calibration of the $^{14}\zu{C}$ dating method using
      tree rings and artifacts whose ages were known from
      other methods. Redrawn from Emilio Segr\`{e}, \textbf{Nuclei and Particles}, 1965.
    },
    {
      'width'=>'wide'
    }
  )
%>

\begin{eg}{$^{14}\zu{C}$ \index{carbon-14 dating}Dating}
Almost all the carbon on Earth is $^{12}\zu{C}$, but not quite.
The isotope $^{14}\zu{C}$, with a half-life of 5600 years, is
produced by cosmic rays in the atmosphere. It decays
naturally, but is replenished at such a rate that the
fraction of $^{14}\zu{C}$ in the atmosphere remains constant, at
$1.3\times10^{-12}$ . Living plants and animals take in both
$^{12}\zu{C}$ and $^{14}\zu{C}$ from the atmosphere and incorporate
both into their bodies. Once the living organism dies, it no
longer takes in C atoms from the atmosphere, and the
proportion of $^{14}\zu{C}$ gradually falls off as it undergoes
radioactive decay. This effect can be used to find the age
of dead organisms, or human artifacts made from plants or
animals. Figure \figref{carbon-fourteen} on page
\pageref{fig:carbon-fourteen} shows the exponential decay
curve of $^{14}\zu{C}$ in various objects. Similar methods, using
longer-lived isotopes, provided the first firm proof that
the earth was billions of years old, not a few thousand as
some had claimed on religious grounds.
\end{eg}

<% end_sec() %>

<% begin_sec("Rate of decay") %>
\label{rate-of-decay}\index{exponential decay!rate of}
If you want to find how many radioactive decays occur within
a time interval lasting from time $t$ to time $t+\Delta t$,
the most straightforward approach is to calculate it like this:
\begin{align*}
        (\text{number of}&\text{ decays between } t \text{ and } t+\Delta t) \\
                         &=  N(t) - N(t+\Delta t)
\end{align*}
Usually we're interested in the case where $\Delta t$ is small compared to $t_{1/2}$,
and in this limiting case the calculation starts to look exactly like the limit that goes into
the definition of the derivative $\der N/\der t$. It is therefore more convenient to
talk about the \emph{rate} of decay $-\der N/\der t$ rather than the \emph{number} of decays
in some finite time interval.
Doing calculus on the function $e^x$ is also easier than with $0.5^x$, so we rewrite the
function $N(t)$ as
\begin{equation*}
  N = N(0) e^{-t/\tau}\eqquad,
\end{equation*}
where $\tau=t_{1/2}/\ln 2$ is shown in example \ref{eg:average-lifetime} on
p.~\pageref{eg:average-lifetime} to be the average time of survival. The rate of decay
is then
\begin{equation*}
  -\frac{\der N}{\der t} = \frac{N(0)}{\tau} e^{-t/\tau}\eqquad.
\end{equation*}
Mathematically, differentating an exponential just gives back another exponential.
Physically, this is telling us that as $N$ falls off exponentially, the rate of
decay falls off at the same exponential rate, because a lower $N$ means fewer
atoms that remain available to decay.

<% self_check('rate-of-decay-units',<<-'SELF_CHECK'
  Check that both sides of the equation for the rate of decay have units of $\sunit^{-1}$,
  i.e., decays per unit time.
  SELF_CHECK
  ) %>

\begin{eg}{The hot potato}
\egquestion A nuclear physicist with a demented sense of humor
tosses you a cigar box, yelling ``hot potato.'' The label on
the box says ``contains $10^{20}$  atoms of $^{17}\zu{F}$,
half-life of 66 s, produced today in our reactor at 1
p.m.'' It takes you two seconds to read the label, after
which you toss it behind some lead bricks and run away. The
time is 1:40 p.m. Will you die?

\eganswer The time elapsed since the radioactive fluorine
was produced in the reactor was 40 minutes, or 2400 s. The
number of elapsed half-lives is therefore $t/t_{1/2}=  36$. The
initial number of atoms was $N(0)=10^{20}$ . The number of
decays per second is now about $10^7\ \zu{s}^{-1}$, so it produced
about $2\times10^7$  high-energy electrons while you held it
in your hands. Although twenty million electrons sounds like
a lot, it is not really enough to be dangerous.
\end{eg}

By the way, none of the equations we've derived so far was
the actual probability distribution for the time at which a
particular radioactive atom will decay. That probability
distribution would be found by substituting $N(0)=1$ into
the equation for the rate of decay.

\startdqs

\begin{dq}
In the medical procedure involving $^{131}\zu{I}$, why is it
the gamma rays that are detected, not the electrons or
neutrinos that are also emitted?
\end{dq}

\begin{dq}
For 1 s, Fred holds in his hands 1 kg of radioactive
stuff with a half-life of 1000 years. Ginger holds 1 kg of a
different substance, with a half-life of 1 min, for the same
amount of time. Did they place themselves in equal danger, or not?
\end{dq}

\begin{dq}
How would you interpret it if you calculated $N(t)$, and
found it was less than one?
\end{dq}

\begin{dq}
Does the half-life depend on how much of the substance
you have? Does the expected time until the sample decays
completely depend on how much of the substance you have?
\end{dq}

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Applications of calculus") %>

The area under the probability distribution is of course an
integral. If we call the random number $x$ and the
probability distribution $D(x)$, then the probability that
$x$ lies in a certain range is given by 
\begin{equation*}
        \text{(probability of $a\le x \le b$)}=\int_a^b D(x) \der x\eqquad.
\end{equation*}
What about averages? If $x$ had a finite number of equally
probable values, we would simply add them up and divide by
how many we had. If they weren't equally likely, we'd make
the weighted average $x_1P_1+x_2P_2+$... But we need to
generalize this to a variable $x$ that can take on any of a
continuum of values. The continuous version of a sum is an
integral, so the average is
\begin{equation*}
                \text{(average value of $x$)}  = \int x D(x) \der x\eqquad,  
\end{equation*}
where the integral is over all possible values of $x$.

\begin{eg}{Probability distribution for radioactive decay}
Here is a rigorous justification for the statement in subsection \ref{subsec:half-life}
that the probability distribution for
radioactive decay is found by substituting $N(0)=1$ into the
equation for the rate of decay. We know that the probability
distribution must be of the form
\begin{equation*}
                D(t)          =   k 0.5^{t/t_{1/2}}\eqquad,  
\end{equation*}
where $k$ is a constant that we need to determine. The atom
is guaranteed to decay eventually, so normalization gives us
\begin{align*}
        \text{(probability of $0\le t < \infty$)}
                         &=  1  \\
                         &=  \int_0^\infty D(t) \der t\eqquad.
\end{align*}
The integral is most easily evaluated by converting the
function into an exponential with $e$ as the base
\begin{align*}
                D(t)          &=  k \exp\left[\ln\left(0.5^{t/t_{1/2}}\right)\right]  \\
                         &=  k \exp\left[\frac{t}{t_{1/2}}\ln 0.5\right]  \\
                         &=  k \exp\left(-\frac{\ln 2}{t_{1/2}}t\right)\eqquad,
\end{align*}
which gives an integral of the familiar form $\int e^{cx}\der x=(1/c)e^{cx}$. We thus have
\begin{equation*}
                1         =    \left.-\frac{kt_{1/2}}{\ln 2}\exp\left(-\frac{\ln 2}{t_{1/2}}t\right)\right]_0^\infty\eqquad,  
\end{equation*}
which gives the desired result:
\begin{equation*}
                k         =   \frac{\ln 2}{t_{1/2}}\eqquad.  
\end{equation*}
\end{eg}

\begin{eg}{Average lifetime}\label{eg:average-lifetime}
You might think that the half-life would also be the average
lifetime of an atom, since half the atoms' lives are shorter
and half longer. But the half whose lives are longer include
some that survive for many half-lives, and these rare
long-lived atoms skew the average. We can calculate the
average lifetime as follows:
\begin{equation*}
        (\text{average lifetime})    = \int_0^\infty t\: D(t)\der t
\end{equation*}
Using the convenient base-$e$ form again, we have
\begin{equation*}
        (\text{average lifetime})
                = \frac{\ln 2}{t_{1/2}}
                         \int_0^\infty t \exp\left(-\frac{\ln 2}{t_{1/2}}t\right) \der t\eqquad.  
\end{equation*}
This integral is of a form that can either be attacked with
integration by parts or by looking it up in a table. The
result is $\int x e^{cx}\der x=\frac{x}{c}e^{cx}-\frac{1}{c^2}e^{cx}$, 
and the first term can be ignored for our
purposes because it equals zero at both limits of integration. We end up with
\begin{align*}
\text{(average lifetime)} &= \frac{\ln 2}{t_{1/2}}\left(\frac{t_{1/2}}{\ln 2}\right)^2    \\
                         &=  \frac{t_{1/2}}{\ln 2}  \\
                         &=  1.443 \: t_{1/2}\eqquad,  
\end{align*}
which is, as expected, longer than one half-life.
\end{eg}

 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
\vfill\pagebreak[4]

<%
  fig(
    'ozone',
    %q{%
      In recent decades, a huge hole in the ozone
      layer has spread out from Antarctica. Left: November 1978. Right: November 1992
    },
    {
      'width'=>'wide'
    }
  )
%>

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Light as a particle",0,'light-as-a-particle') %>

\epigraph{The only thing that interferes with my learning is my 
education.}{Albert \index{Einstein, Albert}Einstein}

Radioactivity is random, but do the laws of physics exhibit
randomness in other contexts besides radioactivity? Yes.
Radioactive decay was just a good playpen to get us started
with concepts of randomness, because all atoms of a given
isotope are identical. By stocking the playpen with an
unlimited supply of identical atom-toys, nature helped us to
realize that their future behavior could be different
regardless of their original identicality. We are now ready
to leave the playpen, and see how randomness fits into the
structure of physics at the most fundamental level.

The laws of physics describe light and matter, and the
quantum revolution rewrote both descriptions. Radioactivity
was a good example of matter's behaving in a way that was
inconsistent with classical physics, but if we want to get
under the hood and understand how nonclassical things
happen, it will be easier to focus on light rather than
matter. A radioactive atom such as uranium-235 is after all
an extremely complex system, consisting of 92 protons, 143
neutrons, and 92 electrons. Light, however, can be a simple sine wave.

However successful the classical wave theory of light had
been --- allowing the creation of \index{radio}radio and
\index{radar}radar, for example --- it still failed to
describe many important phenomena. An example that is
currently of great interest is the way the \index{ozone
layer}ozone layer protects us from the dangerous short-wavelength
\index{ultraviolet light}ultraviolet part of the sun's
spectrum. In the classical description, light is a wave.
When a wave passes into and back out of a medium, its
frequency is unchanged, and although its wavelength is
altered while it is in the medium, it returns to its
original value when the wave reemerges. Luckily for us, this
is not at all what ultraviolet light does when it passes
through the ozone layer, or the layer would offer no protection at all!

<% begin_sec("Evidence for light as a particle") %>

For a long time, physicists tried to explain away the
problems with the classical theory of light as arising from
an imperfect understanding of atoms and the interaction of
light with individual atoms and molecules. The ozone
paradox, for example, could have been attributed to the
incorrect assumption that one could think of the ozone layer
as a smooth, continuous substance, when in reality it was
made of individual ozone molecules. It wasn't until 1905
that Albert Einstein threw down the gauntlet, proposing that
the problem had nothing to do with the details of light's
interaction with atoms and everything to do with the
fundamental nature of light itself.

<%
  fig(
    'ccd-spot',
    %q{%
      Digital camera images of dimmer and dimmer sources
      of light. The dots are records of individual photons.
    },
    {
      'width'=>'wide'
    }
  )
%>

In those days the data were sketchy, the ideas vague, and
the experiments difficult to interpret; it took a genius
like Einstein to cut through the thicket of confusion and
find a simple solution. Today, however, we can get right to
the heart of the matter with a piece of ordinary consumer
electronics, the \index{digital camera}digital camera.
Instead of film, a digital camera has a computer chip with
its surface divided up into a grid of light-sensitive
squares, called ``pixels.'' Compared to a grain of the
silver compound used to make regular photographic film, a
digital camera pixel is activated by an amount of light
energy orders of magnitude smaller. We can learn something
new about light by using a digital camera to detect smaller
and smaller amounts of light, as shown in figure \figref{ccd-spot}.
Figure \figref{ccd-spot}/1 is fake, but \figref{ccd-spot}/2 and \figref{ccd-spot}/3 are
real digital-camera images made by Prof. Lyman Page of
Princeton University as a classroom demonstration.\label{lymanpage} Figure
\figref{ccd-spot}/1 is what we would see if we used the digital camera to
take a picture of a fairly dim source of light. In figures
\figref{ccd-spot}/2 and \figref{ccd-spot}/3, the intensity of the light was  drastically
reduced by inserting semitransparent absorbers like the
tinted plastic used in sunglasses. Going from \figref{ccd-spot}/1 to \figref{ccd-spot}/2 to
\figref{ccd-spot}/3, more and more light energy is being thrown away by the absorbers.

<% marg(0) %>
<%
  fig(
    'attenuation-wave',
    %q{A wave is partially absorbed.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'attenuation-bullets',
    %q{A stream of particles is partially absorbed.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'seurat',
    %q{%
      Einstein and Seurat: twins separated at birth?
      \emph{Seine Grande Jatte} by Georges Seurat (19th century).
    }
  )
%>
<% end_marg %>

The results are drastically different from what we would
expect based on the wave theory of light. If light was a
wave and nothing but a wave, \figref{attenuation-wave},
then the absorbers would
simply cut down the wave's amplitude across the whole
wavefront. The digital camera's entire chip would be
illuminated uniformly, and weakening the wave with an
absorber would just mean that every pixel would take a long
time to soak up enough energy to register a signal.

But figures \figref{ccd-spot}/2 and \figref{ccd-spot}/3 
show that some pixels take strong
hits while others pick up no energy at all. Instead of the
wave picture, the image that is naturally evoked by the data
is something more like a hail of bullets from a machine gun,
\figref{attenuation-bullets}.
 Each ``bullet'' of light apparently carries only a tiny
amount of energy, which is why detecting them individually
requires a sensitive digital camera rather than an
eye or a piece of film.

Although Einstein was interpreting different observations,
this is the conclusion he reached in his 1905 paper: that
the pure wave theory of light is an oversimplification, and
that the energy of a beam of light comes in finite chunks
rather than being spread smoothly throughout a region of space.

We now think of these chunks as particles of light, and call
them ``\index{photon!Einstein's early theory}photons,''
although Einstein avoided the word ``particle,'' and the
word ``photon'' was invented later. Regardless of words, the
trouble was that waves and particles seemed like inconsistent
categories. The reaction to Einstein's paper could be kindly
described as vigorously skeptical. Even twenty years later,
Einstein wrote, ``There are therefore now two theories of
light, both indispensable, and --- as one must admit today
despite twenty years of tremendous effort on the part of
theoretical physicists --- without any logical connection.''
In the remainder of this section we will learn how the
seeming paradox was eventually resolved.

\startdqs

\begin{dq}
Suppose someone rebuts the digital camera data in figure \figref{ccd-spot}, claiming
that the random pattern of dots occurs not because of
anything fundamental about the nature of light but simply
because the camera's pixels are not all exactly the same  --- some are just more sensitive
than others.
How could we test this interpretation?
\end{dq}

\begin{dq}
Discuss how the correspondence principle applies to the
observations and concepts discussed in this section.
\end{dq}

<% end_sec() %>

<% begin_sec("How much light is one photon?",4,'how-much-light-is-one-photon') %>

<% begin_sec("The photoelectric effect") %>
\index{photoelectric effect}
We have seen evidence that light energy comes in little
chunks, so the next question to be asked is naturally how
much energy is in one chunk. The most straightforward
experimental avenue for addressing this question is a
phenomenon known as the photoelectric effect. The photoelectric
effect occurs when a photon strikes the surface of a solid
object and knocks out an electron. It occurs continually all
around you. It is happening right now at the surface of your
skin and on the paper or computer screen from which you are
reading these words. It does not ordinarily lead to any
observable electrical effect, however, because on the
average free electrons are wandering back in just as
frequently as they are being ejected. (If an object did
somehow lose a significant number of electrons, its growing
net positive charge would begin attracting the electrons
back more and more strongly.)

<% marg(0) %>
<%
  fig(
    'photoelectric-apparatus-a',
    %q{%
      Apparatus for observing the photoelectric
       effect. A beam of light strikes a capacitor plate inside a vacuum
       tube, and electrons are ejected (black arrows).
    }
  )
%>
<% end_marg %>

Figure \figref{photoelectric-apparatus-a} shows a practical method for detecting the
photoelectric effect. Two very clean parallel metal plates
(the electrodes of a capacitor) are sealed inside a vacuum
tube, and only one plate is exposed to light. Because there
is a good vacuum between the plates, any ejected electron
that happens to be headed in the right direction will almost
certainly reach the other capacitor plate without colliding
with any air molecules.

The illuminated (bottom) plate is left with a net positive
charge, and the unilluminated (top) plate acquires a
negative charge from the electrons deposited on it. There is
thus an electric field between the plates, and it is because
of this field that the electrons' paths are curved, as shown
in the diagram. However, since vacuum is a good insulator,
any electrons that reach the top plate are prevented from
responding to the electrical attraction by jumping back
across the gap. Instead they are forced to make their way
around the circuit, passing through an ammeter. The ammeter
allows a measurement of the strength of the photoelectric effect.

<% end_sec() %>

<% begin_sec("An unexpected dependence on frequency") %>

The photoelectric effect was discovered serendipitously by
Heinrich \index{Hertz, Heinrich}Hertz in 1887, as he was
experimenting with radio waves. He was not particularly
interested in the phenomenon, but he did notice that the
effect was produced strongly by ultraviolet light and more
weakly by lower frequencies. Light whose frequency was lower
than a certain critical value did not eject any electrons at
all. (In fact this was all prior to Thomson's discovery of
the electron, so Hertz would not have described the effect
in terms of electrons --- we are discussing everything with
the benefit of hindsight.) This dependence on frequency
didn't make any sense in terms of the classical wave theory
of light. A light wave consists of electric and magnetic
fields. The stronger the fields, i.e., the greater the wave's
amplitude, the greater the forces that would be exerted on
electrons that found themselves bathed in the light. It
should have been amplitude (brightness) that was relevant,
not frequency. The dependence on frequency not only proves
that the wave model of light needs modifying, but with the
proper interpretation it allows us to determine how much
energy is in one photon, and it also leads to a connection
between the wave and particle models that we need in
order to reconcile them.

<% marg(0) %>
<%
  fig(
    'photoelectric-hamster',
    %q{%
      The hamster in her hamster ball
      is like an electron emerging from the metal (tiled kitchen floor)
      into the surrounding vacuum (wood floor). The wood floor is higher
      than the tiled floor, so as she rolls up the step, the hamster will
      lose a certain amount of kinetic energy, analogous to $E_s$. If her
      kinetic energy is too small, she won't even make it up the step.
    }
  )
%>
<% end_marg %>

 % 
To make any progress, we need to consider the physical
process by which a photon would eject an electron from the
metal electrode. A metal contains electrons that are free to
move around. Ordinarily, in the interior of the metal, such
an electron feels attractive forces from atoms in every
direction around it. The forces cancel out. But if the
electron happens to find itself at the surface of the metal,
the attraction from the interior side is not balanced out by
any attraction from outside.
In popping out through the surface the electron therefore loses
some amount of energy $E_s$, which depends on the type of metal used.

Suppose a photon strikes an electron, annihilating itself
and giving up all its energy to the electron. (We now know
that this is what always happens in the photoelectric
effect, although it had not yet been established in 1905
whether or not the photon was completely annihilated.) The
electron will (1) lose kinetic energy through collisions
with other electrons as it plows through the metal on its
way to the surface; (2) lose an amount of kinetic energy
equal to $E_s$ as it emerges through the surface; and (3) lose
more energy on its way across the gap between the plates,
due to the electric field between the plates. Even if the
electron happens to be right at the surface of the metal
when it absorbs the photon, and even if the electric field
between the plates has not yet built up very much, $E_s$ is
the bare minimum amount of energy that it must receive from
the photon if it is to contribute to a measurable current.
The reason for using very clean electrodes is to minimize
$E_s$ and make it have a definite value characteristic of the
metal surface, not a mixture of values due to the various
types of dirt and crud that are present in tiny amounts on
all surfaces in everyday life.

We can now interpret the frequency dependence of the
photoelectric effect in a simple way: apparently the amount
of energy possessed by a photon is related to its frequency.
A low-frequency red or infrared photon has an energy less
than $E_s$, so a beam of them will not produce any current.  A
high-frequency blue or violet photon, on the other hand,
packs enough of a punch to allow an electron to make it to
the other plate. At frequencies higher than the minimum, the
photoelectric current continues to increase with the
frequency of the light because of effects (1) and (3).

<% end_sec() %>

<% begin_sec("Numerical relationship between energy and frequency") %>

Figure \figref{photoelectric-apparatus-b} shows an experiment that is used sometimes in
college laboratory courses to probe the relationship between the energy and frequency
of a photon. The idea is simply to illuminate
one plate of the vacuum tube with light of a single
wavelength and monitor the voltage difference between the
two plates as they charge up. Since the resistance of a
voltmeter is very high (much higher than the resistance of
an ammeter), we can assume to a good approximation that
electrons reaching the top plate are stuck there permanently,
so the voltage will keep on increasing for as long as
electrons are making it across the vacuum tube.

<% marg(0) %>
<%
  fig(
    'photoelectric-apparatus-b',
    %q{A different way of studying the photoelectric effect.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'photoelectric-graph',
    %q{%
      The quantity $E_s+e\Delta V$ indicates the energy of one photon. It
       is found to be proportional to the frequency of the light.
    }
  )
%>
<% end_marg %>

At a moment when the voltage difference has reached a
value $\Delta V$, the minimum energy required by an electron
to make it out of the bottom plate and across the gap to the
other plate is $E_s+e\Delta $V. As $\Delta V$ increases, we
eventually reach a point at which $E_s+e\Delta V$ \emph{equals} the
energy of one photon. No more electrons can cross the gap,
and the reading on the voltmeter stops rising. The quantity
$E_s+e\Delta V$ now tells us the energy of one photon. If we
determine this energy for a variety of wavelengths, \figref{photoelectric-graph}, we
find the following simple relationship between the energy of
a \index{photon!energy of}photon and the frequency of the light:
\begin{equation*}
                E  =  hf\eqquad,  
\end{equation*}
where $h$ is a constant with the value
$6.63\times10^{-34}\ \zu{J}\cdot\zu{s}$. Note how the equation brings the
wave and particle models of light under the same roof: the
left side is the energy of one \emph{particle} of light,
while the right side is the frequency of the same light,
interpreted as a \emph{wave}. The constant $h$ is known as
\index{Planck, Max}\index{Planck's constant}Planck's constant,\label{planck-constant}
for historical reasons explained in the footnote beginning on the preceding
page.

<% self_check('extracth',<<-'SELF_CHECK'

How would you extract $h$ from the graph in figure \figref{photoelectric-graph}?
What if you didn't even know $E_s$ in advance, and could only graph
$e\Delta V$ versus $f$?
  SELF_CHECK
  ) %>

Since the energy of a photon is $hf$, a beam of light
can only have energies of $hf$, $2hf$, $3hf$,
etc. Its energy is quantized --- there is no such thing as a
fraction of a photon. Quantum physics gets its name from the
fact that it quantizes quantities like energy, momentum, and
angular momentum that had previously been thought to be
smooth, continuous and infinitely divisible.

\pagebreak

\begin{eg}{Photons from a lightbulb}\label{eg:photons-from-lightbulb}
\egquestion Roughly how many photons are emitted by a 100 watt
lightbulb in 1 second?

\eganswer People tend to remember wavelengths rather than
frequencies for visible light. The bulb emits photons with a
range of frequencies and wavelengths, but let's take 600 nm
as a typical wavelength for purposes of estimation. The
energy of a single photon is
\begin{align*}
                E_{photon}         &=    hf  \\
                         &=    hc/\lambda   
\end{align*}
A power of 100 W means 100 joules per second, so the
number of photons is
\begin{align*}
        (100\ \zu{J})/E_{photon}
                &=    (100\ \zu{J}) / (hc/\lambda )  \\
                &\approx 3\times10^{20}
\end{align*}
This hugeness of this number is consistent with the correspondence principle.
The experiments that established the classical theory of optics
weren't wrong. They were right, within their domain of applicability, in which
the number of photons was so large as to be indistinguishable from a continuous beam.
\end{eg}

\begin{eg}{Measuring the wave}\label{eg:am-radio-photon-density}
When surfers are out on the water waiting for their chance to catch a wave, they're interested
in both the height of the waves and when the waves are going to arrive. In other words, they
observe both the amplitude and phase of the waves, and it doesn't matter to them that
the water is granular at the molecular level. The correspondence principle requires that we
be able to do the same thing for electromagnetic waves, since the classical theory of electricity
and magnetism was all stated and verified experimentally in terms of the fields $\vc{E}$ and $\vc{B}$,
which are the amplitude of an electromagnetic wave. The phase is also necessary, since the induction effects
predicted by Maxwell's equation would flip their signs depending on whether an oscillating field is on its way up or
on its way back down.

This is a more demanding application of the correspondence principle than the one in 
example \ref{eg:photons-from-lightbulb}, since amplitudes and phases constitute more detailed
information than the over-all intensity of a beam of light. Eyeball measurements can't detect
this type of information, since the eye is much bigger than a wavelength,
but for example an AM radio receiver can do it with radio waves, since
the wavelength for a station at 1000 kHz is about 300 meters, which is much larger than the antenna.
The correspondence principle demands that we be able to explain this in terms of the photon
theory, and this requires not just that we have a large number of photons emitted by the transmitter
per second, as in example \ref{eg:photons-from-lightbulb}, but that even by the time they spread out
and reach the receiving antenna, there should be many photons overlapping each other within a space
of one cubic wavelength. Problem \ref{hw:am-radio-photon-density} on p.~\pageref{hw:am-radio-photon-density}
verifies that the number is in fact extremely large.
\end{eg}

\begin{eg}{Momentum of a photon}
\egquestion According to the theory of relativity, the
momentum of a beam of light is given by $p=E/c$.
Apply this to find the momentum of a
single photon in terms of its frequency, and in terms of its wavelength.

\eganswer Combining the equations $p=E/c$ and $E=hf$, we find
\begin{align*}
                p         &=    E/c  \\
                         &= \frac{h}{c}f\eqquad.  
\end{align*}
To reexpress this in terms of wavelength, we use $c=f\lambda $:
\begin{align*}
                p         &=  \frac{h}{c}\cdot\frac{c}{\lambda}    \\
                         &=  \frac{h}{\lambda}    
\end{align*}
The second form turns out to be simpler.
\end{eg}

\startdqs

\begin{dq}
The photoelectric effect only ever ejects a
very tiny percentage of the electrons available
near the surface of an object.
How well does this agree with the wave
model of light, and how well with the particle model?
Consider the two different distance scales involved:
the wavelength of the light, and the size of an atom,
which is on the order of $10^{-10}$ or $10^{-9}$ m.
\end{dq}

\begin{dq}
What is the significance of the fact that Planck's
constant is numerically very small? How would our everyday
experience of light be different if it was not so small?
\end{dq}

\begin{dq}
How would the experiments described above be affected if
a single electron was likely to get hit by more than one photon?
\end{dq}

\begin{dq}
Draw some representative trajectories of electrons for
$\Delta V=0$, $\Delta V$ less than the maximum value, and
$\Delta V$ greater than the maximum value.
\end{dq}

\begin{dq}
Explain based on the photon theory of light why
ultraviolet light would be more likely than visible or
infrared light to cause cancer by damaging DNA molecules.
How does this relate to discussion question C?
\end{dq}

\begin{dq}
Does $E=hf$ imply that a photon changes its energy
when it passes from one transparent material into another
substance with a different index of refraction?
\end{dq}

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Wave-particle duality",4,'wave-particle-duality') %>

__incl(text/wave-particle-duality)

<% end_sec('wave-particle-duality') %>

__incl(text/nonlocality)

<% begin_sec("Photons in three dimensions") %>

\index{photon!in three dimensions}

Up until now I've been sneaky and avoided a full discussion
of the three-dimensional aspects of the probability
interpretation. The example of the carrot in the microwave
oven, for example, reduced to a one-dimensional situation
because we were considering three points along the same line
and because we were only comparing ratios of probabilities.

A typical example of a probability distribution in section \ref{sec:randomness}
was the distribution of heights of human beings. The thing
that varied randomly, height, $h$, had units of meters, and
the probability distribution was a graph of a function
$D(h)$. The units of the probability distribution had to be
$\zu{m}^{-1}$ (inverse meters) so that areas under the curve,
interpreted as probabilities, would be unitless:
$(\text{area})=(\text{height})(\text{width})=\zu{m}^{-1}\cdot\zu{m}$.

<% marg(80) %>
<%
  fig(
    'volume-under-surface',
    %q{%
      Probability is the volume under
      a surface defined by $D(x,y)$.
    }
  )
%>
<% end_marg %>

Now suppose we have a two-dimensional problem, e.g., the
probability distribution for the place on the surface of a
digital camera chip where a photon will be detected. The
point where it is detected would be described with two
variables, $x$ and $y$, each having units of meters. The
probability distribution will be a function of both
variables, $D(x,y)$. A probability is now visualized as the
volume under the surface described by the function $D(x,y)$,
as shown in figure \figref{volume-under-surface}.
The units of $D$ must be $\zu{m}^{-2}$ so
that probabilities will be unitless:
$(\text{probability})=(\text{depth})(\text{length})(\text{width})
=\zu{m}^{-2}\cdot\zu{m}\cdot\zu{m}$. In terms of calculus,
we have $P\:=\:\int D\der x \der y$.

Generalizing finally to three dimensions, we find by analogy
that the probability distribution will be a function of all
three coordinates, $D(x,y,z)$, and will have units of $\zu{m}^{-3}$.
It is unfortunately impossible to visualize the graph
unless you are a mutant with a natural feel for life in four
dimensions. If the probability distribution is nearly
constant within a certain volume of space $v$, the
probability that the photon is in that volume is simply
$vD$. If not, then we can use an integral,
$P\:=\:\int D\der x \der y\der z$.

 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
\pagebreak[4]
\inlinefignocaption{../../../share/quantum/figs/melting-witch}

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Matter as a wave",0,'matter-as-a-wave') %>
\index{matter!as a wave}

m4_include(../share/quantum/text/wicked-witch.tex)

<% begin_sec("Electrons as waves") %>

\index{electron!as a wave}
We started our journey into quantum physics by studying the
random behavior of \emph{matter} in radioactive decay, and
then asked how randomness could be linked to the basic laws
of nature governing \emph{light}. The probability interpretation
of wave-particle duality was strange and hard to accept, but
it provided such a link. It is now natural to ask whether
the same explanation could be applied to matter. If the
fundamental building block of light, the photon, is a
particle as well as a wave, is it possible that the basic
units of matter, such as electrons, are waves as well as particles?

A young French aristocrat studying physics, Louis \index{de
Broglie!Louis}de Broglie (pronounced ``broylee''), made
exactly this suggestion in his 1923 Ph.D. thesis. His idea
had seemed so farfetched that there was serious doubt about
whether to grant him the degree. Einstein was asked for his
opinion, and with his strong support, de Broglie got his degree.

Only two years later, American physicists C.J. \index{Davisson!C.J.}Davisson
and L. \index{Germer, L.}Germer confirmed de Broglie's idea
by accident. They had been studying the scattering of
electrons from the surface of a sample of nickel, made of
many small crystals. (One can often see such a crystalline
pattern on a brass doorknob that has been polished by
repeated handling.) An accidental explosion occurred, and
when they put their apparatus back together they observed
something entirely different: the scattered electrons were
now creating an interference pattern! This dramatic proof of
the wave nature of matter came about because the nickel
sample had been melted by the explosion and then resolidified
as a single crystal. The nickel atoms, now nicely arranged
in the regular rows and columns of a crystalline lattice,
were acting as the lines of a diffraction grating. The new
crystal was analogous to the type of ordinary diffraction
grating in which the lines are etched on the surface of a
mirror (a reflection grating) rather than the kind in which
the light passes through the transparent gaps between the
lines (a transmission grating).

<%
  fig(
    'neutron-interference',
    %q{%
      A double-slit interference pattern
       made with neutrons. (A. Zeilinger, R. G\"{a}hler, C.G. Shull,
       W. Treimer, and W. Mampe, \emph{Reviews of Modern Physics}, Vol. 60, 1988.)
    },
    {
      'width'=>'wide'
    }
  )
%>

Although we will concentrate on the wave-particle duality of
electrons because it is important in chemistry and the
physics of atoms, all the other ``particles'' of matter
you've learned about show wave properties as well.
Figure \figref{neutron-interference}, for instance, shows a wave interference
pattern of neutrons.  

It might seem as though all our work was already done for
us, and there would be nothing new to understand about
electrons: they have the same kind of funny wave-particle
duality as photons. That's almost true, but not quite. There
are some important ways in which electrons differ significantly from photons:
\begin{enumerate}
\item Electrons have mass, and photons don't. 
\item Photons always move at the speed of light, but electrons
can move at any speed less than $c$. 
\item Photons don't have electric charge, but electrons do, so
electric forces can act on them. The most important example
is the atom, in which the electrons are held by the electric
force of the nucleus.
\item Electrons cannot be absorbed or emitted as photons are.
Destroying an electron or creating one out of nothing would
violate conservation of charge.
\end{enumerate}
(In section \ref{sec:atom} we will learn of one more fundamental way in
which electrons differ from photons, for a total of five.)

Because electrons are different from photons, it is not
immediately obvious which of the photon equations from
chapter \ref{ch:em} can be applied to electrons as well. A
particle property, the energy of one photon, is related to
its wave properties via $E=hf$ or, equivalently,
$E=hc/\lambda $. The momentum of a photon was given
by $p=hf/c$ or $p=h/\lambda $. Ultimately it was a
matter of experiment to determine which of these equations,
if any, would work for electrons, but we can make a quick
and dirty guess simply by noting that some of the equations
involve $c$, the speed of light, and some do not. Since $c$
is irrelevant in the case of an electron, we might guess
that the equations of general validity are those that do
not have $c$ in them:
\begin{align*}
                E  &=  hf  \\
                p  &=  h/\lambda   
\end{align*}

This is essentially the reasoning that de Broglie went
through, and experiments have confirmed these two equations
for all the fundamental building blocks of light and matter,
not just for photons and electrons.

The second equation, which I soft-pedaled in the previous
chapter, takes on a greater importance for electrons. This is
first of all because the momentum of matter is more likely
to be significant than the momentum of light under ordinary
conditions, and also because force is the transfer of
momentum, and electrons are affected by electrical forces.

\begin{eg}{The wavelength of an elephant}
\egquestion What is the wavelength of a trotting elephant?

\eganswer One may doubt whether the equation should be
applied to an elephant, which is not just a single particle
but a rather large collection of them. Throwing caution to
the wind, however, we estimate the elephant's mass at $10^3$
 kg and its trotting speed at 10 m/s. Its wavelength
is therefore roughly
\begin{align*}
        \lambda         &=    \frac{h}{p}  \\
          &=    \frac{h}{mv}  \\
  &= \frac{6.63\times10^{-34}\ \zu{J}\unitdot\sunit}{(10^3\ \kgunit)(10\ \munit/\sunit)} \\
  &\sim 10^{-37}\ \frac{\left(\kgunit\unitdot\munit^2/\sunit^2\right)\unitdot\sunit}{\kgunit\unitdot\munit/\sunit} \\
        &= 10^{-37}\ \munit
\end{align*}
\end{eg}

The wavelength found in this example is so fantastically
small that we can be sure we will never observe any
measurable wave phenomena with elephants or any other
human-scale objects. The result is numerically small because
Planck's constant is so small, and as in some examples
encountered previously, this smallness is in accord with the
correspondence principle.

Although a smaller mass in the equation $\lambda =h/mv$
does result in a longer wavelength, the wavelength is still
quite short even for individual electrons under typical
conditions, as shown in the following example.

\begin{eg}{The typical wavelength of an electron}
\egquestion Electrons in circuits and in atoms are typically
moving through voltage differences on the order of 1 V,
so that a typical energy is $(e)(1\ \zu{V})$, which is on the
order of $10^{-19}\ \junit$. What is the wavelength of an electron
with this amount of kinetic energy?

\eganswer This energy is nonrelativistic, since it is much
less than $mc^2$. Momentum and energy are therefore related
by the nonrelativistic equation $K=p^2/2m$. Solving
for $p$ and substituting in to the equation for the wavelength, we find
\begin{align*}
                \lambda          &=  \frac{h}{\sqrt{2mK}}    \\
                         &=    1.6\times10^{-9}\ \zu{m}\eqquad.
\end{align*}
This is on the same order of magnitude as the size of an
atom, which is no accident: as we will discuss in the next
chapter in more detail, an electron in an atom can be
interpreted as a standing wave. The smallness of the
wavelength of a typical electron also helps to explain why
the wave nature of electrons wasn't discovered until a
hundred years after the wave nature of light. To scale the
usual wave-optics devices such as diffraction gratings down
to the size needed to work with electrons at ordinary
energies, we need to make them so small that their parts are
comparable in size to individual atoms. This is essentially
what Davisson and Germer did with their nickel crystal.
\end{eg}

<% self_check('longwavelengthelectron',<<-'SELF_CHECK'

These remarks about the inconvenient smallness of electron
wavelengths apply only under the assumption that the
electrons have typical energies. What kind of energy would
an electron have to have in order to have a longer
wavelength that might be more convenient to work with?
  SELF_CHECK
  ) %>

<% begin_sec("What kind of wave is it?") %>

\index{wavefunction!of the electron}\index{electron!wavefunction}
If a sound wave is a vibration of matter, and a photon is a
vibration of electric and magnetic fields, what kind of a
wave is an electron made of? The disconcerting answer is
that there is no experimental ``observable,'' i.e., directly
measurable quantity, to correspond to the electron wave
itself. In other words, there are devices like microphones
that detect the oscillations of air pressure in a sound
wave, and devices such as radio receivers that measure the
oscillation of the electric and magnetic fields in a light
wave, but nobody has ever found any way to measure the
electron wave directly.

<% marg(30) %>
<%
  fig(
    'electron-wave-phase',
    %q{%
      These two electron
       waves are not distinguishable by any measuring device.
    }
  )
%>
<% end_marg %>

We can of course detect the energy (or momentum) possessed
by an electron just as we could detect the energy of a
photon using a digital camera. (In fact I'd imagine that an
unmodified digital camera chip placed in a vacuum chamber
would detect electrons just as handily as photons.) But this
only allows us to determine where the wave carries high
probability and where it carries low probability. Probability
is proportional to the square of the wave's amplitude, but
measuring its square is not the same as measuring the wave
itself. In particular, we get the same result by squaring
either a positive number or its negative, so there is no way
to determine the positive or negative sign of an electron wave.
This unobservability of the phase of the wavefunction is discussed
in more detail on p.~\pageref{subsubsec:linearity-of-schrodinger}.\label{phase-unobservable-basic}\index{phase in quantum mechanics!not observable}

Most physicists tend toward the school of philosophy known
as operationalism, which says that a concept is only
meaningful if we can define some set of operations for
observing, measuring, or testing it. According to a strict
operationalist, then, the electron wave itself is a
meaningless concept. Nevertheless, it turns out to be one of
those concepts like love or humor that is impossible to
measure and yet very useful to have around. We therefore
give it a symbol, $\Psi $ (the capital Greek letter psi),
and a special name, the electron \emph{wavefunction}
(because it is a function of the coordinates $x$, $y$, and $z$
that specify where you are in space). It would be impossible,
for example, to calculate the shape of the electron wave in
a hydrogen atom without having some symbol for the wave. But
when the calculation produces a result that can be compared
directly to experiment, the final algebraic result will turn
out to involve only $\Psi^2$, which is what is observable, not $\Psi $ itself.

Since $\Psi $, unlike $E$ and $B$, is not directly
measurable, we are free to make the probability equations
have a simple form: instead of having the probability
density equal to some funny constant multiplied by $\Psi^2$,
we simply define $\Psi $ so that the constant of
proportionality is one:
\begin{equation*}
  (\text{probability distribution})  =  |\Psi| ^2\eqquad.  
\end{equation*}
Since the probability distribution has units of $\zu{m}^{-3}$, the units
of $\Psi $ must be $\zu{m}^{-3/2}$. The square of a negative
number is still positive, so the absolute value signs may seem unnecessary,
but as we'll see on p.~\pageref{subsubsec:complex-wavefunction} in sec.~\ref{subsec:schrodinger},
the wavefunction may in general be a complex number. In fact, only standing waves, not traveling waves,
can really be represented by real numbers, although we will often cheat and draw
pictures of traveling waves as if they were real-valued functions.\label{cheat-with-real-psi}

\startdq

\begin{dq}
Frequency is oscillations per second, whereas wavelength is
meters per oscillation. How could the equations $E=hf$
and $p=h/\lambda$  be made to look more alike by
using quantities that were more closely analogous?
(This more symmetric treatment makes it easier to
incorporate relativity into quantum mechanics, since
relativity says that space and time are not entirely
separate.)
\end{dq}

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Dispersive waves",nil,'dispersive-waves') %>

\index{wave!dispersive}\index{dispersion}
A colleague of mine who teaches chemistry loves to tell the
story about an exceptionally bright student who, when told
of the equation $p=h/\lambda $, protested, ``But when I
derived it, it had a factor of 2!'' The issue that's
involved is a real one, albeit one that could be glossed
over (and is, in most textbooks) without raising any alarms
in the mind of the average student. The present optional
section addresses this point; it is intended for the student
who wishes to delve a little deeper.

Here's how the now-legendary student was presumably
reasoning. We start with the equation $v=f\lambda $, which
is valid for any sine wave, whether it's quantum or
classical. Let's assume we already know $E=hf$, and
are trying to derive the relationship between wavelength and momentum:
\begin{align*}
        \lambda         &=    \frac{v}{f}  \\
                 &=    \frac{vh}{E}  \\
                 &=    \frac{vh}{\frac{1}{2}mv^2}  \\
                 &=    \frac{2h}{mv}  \\
                 &=    \frac{2h}{p}\eqquad.  
\end{align*}

The reasoning seems valid, but the result does contradict
the accepted one, which is after all solidly based on experiment.

<% marg(0) %>
<%
  fig(
    'sine-wave',
    %q{Part of an infinite sine wave.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'sine-wave-pulse',
    %q{A finite-length sine wave.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'beats',
    %q{A beat pattern created by superimposing two sine waves with slightly different wavelengths.}
  )
%>
<% end_marg %>

The mistaken assumption is that we can figure everything out
in terms of pure sine waves. Mathematically, the only wave
that has a perfectly well defined wavelength and frequency
is a sine wave, and not just any sine wave but an infinitely
long sine wave, \figref{sine-wave}.
 The unphysical thing about such a wave
is that it has no leading or trailing edge, so it can never
be said to enter or leave any particular region of space.
Our derivation made use of the velocity, $v$, and if
velocity is to be a meaningful concept, it must tell us how
quickly stuff (mass, energy, momentum, \ldots) is transported
from one region of space to another. Since an infinitely
long sine wave doesn't remove any stuff from one region and
take it to another, the ``velocity of its stuff'' is not a
well defined concept.

Of course the individual wave peaks do travel through space,
and one might think that it would make sense to associate
their speed with the ``speed of stuff,'' but as we will see,
the two velocities are in general unequal when a wave's
velocity depends on wavelength. Such a wave is called a
\emph{dispersive} wave, because a wave pulse consisting of a
superposition of waves of different wavelengths will
separate (disperse) into its separate wavelengths as the
waves move through space at different speeds.  Nearly all
the waves we have encountered have been nondispersive. For
instance, sound waves and light waves (in a vacuum) have
speeds independent of wavelength. A water wave is one good
example of a dispersive wave. Long-wavelength water waves
travel faster, so a ship at sea that encounters a storm
typically sees the long-wavelength parts of the wave first.
When dealing with dispersive waves, we need symbols and
words to distinguish the two \index{velocity!group}\index{velocity!phase}\index{group
velocity}\index{phase velocity}speeds. The speed at which
wave peaks move is called the phase velocity, $v_p$, and the
speed at which ``stuff'' moves is called the group velocity, $v_g$.

An infinite sine wave can only tell us about the phase
velocity, not the group velocity, which is really what we
would be talking about when we refer to the speed of an
electron. If an infinite sine wave is the simplest possible
wave, what's the next best thing? We might think the runner
up in simplicity would be a wave train consisting of a
chopped-off segment of a sine wave, \figref{sine-wave-pulse}. However, this kind
of wave has kinks in it at the end. A simple wave should be
one that we can build by superposing a small number of
infinite sine waves, but a kink can never be produced by
superposing any number of infinitely long sine waves.

Actually the simplest wave that transports stuff from place
to place is the pattern shown in figure 
\figref{beats}. Called a beat
pattern, it is formed by superposing two sine waves whose
wavelengths are similar but not quite the same. If you have
ever heard the pulsating howling sound of musicians in the
process of tuning their instruments to each other, you have
heard a beat pattern. The beat pattern gets stronger and
weaker as the two sine waves go in and out of phase with
each other. The beat pattern has more ``stuff'' (energy, for
example) in the areas where constructive interference
occurs, and less in the regions of cancellation. As the
whole pattern moves through space, stuff is transported from
some regions and into other ones.

If the frequency of the two sine waves differs by 10\%, for
instance, then ten periods will be occur between times when
they are in phase. Another way of saying it is that the
sinusoidal ``envelope'' (the dashed lines in figure \figref{beats}) has
a frequency equal to the difference in frequency between the
two waves. For instance, if the waves had frequencies of 100
Hz and 110 Hz, the frequency of the envelope would be 10 Hz.

To apply similar reasoning to the wavelength, we must define
a quantity $z=1/\lambda $ that relates to wavelength in the
same way that frequency relates to period. In terms of this
new variable, the $z$ of the envelope equals the difference
between the $z's$ of the two sine waves.

The group velocity is the speed at which the envelope moves
through space. Let $\Delta f$ and $\Delta z$ be the
differences between the frequencies and $z's$ of the two
sine waves, which means that they equal the frequency and
$z$ of the envelope. The group velocity is $v_g=f_{envelope}\lambda_{envelope}=\Delta
f/\Delta $z. If $\Delta f$  and $\Delta z$ are sufficiently
small, we can approximate this expression as a derivative,
\begin{equation*}
                v_g         =    \frac{\der f}{\der z}\eqquad.  
\end{equation*}
This expression is usually taken as the definition of the
group velocity for wave patterns that consist of a
superposition of sine waves having a narrow range of
frequencies and wavelengths. In quantum mechanics, with
$f=E/h$ and $z=p/h$, we have $v_g=\der E/\der p$. In the case of a
nonrelativistic electron the relationship between energy and
momentum is $E=p^2/2m$, so the group velocity is $\der E/\der p=p/m=v$,
exactly what it should be. It is only the phase velocity
that differs by a factor of two from what we would have expected,
but the phase velocity is not the physically important thing.

<% end_sec() %>

<% begin_sec("Bound states",nil,'qm-bound-states') %>

\index{states!bound}\index{bound states}
Electrons are at their most interesting when they're in
atoms, that is, when they are bound within a small region of
space. We can understand a great deal about atoms and
molecules based on simple arguments about such bound states,
without going into any of the realistic details of atom. The
simplest model of a bound state is known as the particle in
a box: like a ball on a pool table, the electron feels zero
force while in the interior, but when it reaches an edge it
encounters a wall that pushes back inward on it with a large
force. In particle language, we would describe the electron
as bouncing off of the wall, but this incorrectly assumes
that the electron has a certain path through space. It is
more correct to describe the electron as a wave that
undergoes 100\% reflection at the boundaries of the box.

<% marg(0) %>
<%
  fig(
    'particle-in-a-box',
    %q{Three possible standing-wave patterns for a particle in a box.}
  )
%>
<% end_marg %>

Like generations of physics students before me, I rolled my
eyes when initially introduced to the unrealistic idea of
putting a particle in a box. It seemed completely impractical,
an artificial textbook invention. Today, however, it has
become routine to study electrons in rectangular boxes in
actual laboratory experiments. The ``box'' is actually just
an empty cavity within a solid piece of silicon, amounting
in volume to a few hundred atoms. The methods for creating
these \index{box!particle in a}\index{particle in a
box}electron-in-a-box setups (known as ``\index{quantum
dot}quantum dots'') were a by-product of the development of
technologies for fabricating computer chips.

For simplicity let's imagine a one-dimensional electron in a
box, i.e., we assume that the electron is only free to move
along a line. The resulting standing wave patterns, of which
the first three are shown in the figure, are just like some
of the patterns we encountered with sound waves in musical
instruments. The wave patterns must be zero at the ends of
the box, because we are assuming the walls are impenetrable,
and there should therefore be zero probability of finding
the electron outside the box. Each wave pattern is labeled
according to $n$, the number of peaks and valleys it has. In
quantum physics, these wave patterns are referred to as
``states'' of the particle-in-the-box system.

The following seemingly innocuous observations about the
particle in the box lead us directly to the solutions to
some of the most vexing failures of classical physics:

\noindent\emph{The particle's \index{energy!quantization of for bound
states}energy is quantized (can only have certain values).}
Each wavelength corresponds to a certain momentum, and a
given momentum implies  a definite kinetic energy,
$E=p^2/2m$. (This is the second type of energy quantization
we have encountered. The type we studied previously had to
do with restricting the number of particles to a whole
number, while assuming some specific wavelength and energy
for each particle. This type of quantization refers to the
energies that a single particle can have. Both photons and
matter particles demonstrate both types of quantization
under the appropriate circumstances.)

\noindent\emph{The particle has a minimum kinetic energy.} Long wavelengths
correspond to low momenta and low energies. There can be no
state with an energy lower than that of the $n=1$ state, 
called the ground state.

\noindent\emph{The smaller the space in which the particle is confined, the
higher its kinetic energy must be.} Again, this is because
long wavelengths give lower energies.

<% marg(0) %>
<%
  fig(
    'sirius-spectrum',
    %q{The spectrum of the light from the star Sirius.}
  )
%>
<% end_marg %>

\begin{eg}{Spectra of thin gases}\label{eg:spectra-of-thin-gases}
\index{spectrum!absorption}\index{spectrum!emission}\index{absorption}
\index{emission spectrum}\index{gas!spectrum of}
A fact that was inexplicable by classical physics was
that thin gases absorb and emit light only at certain
wavelengths. This was observed both in earthbound laboratories
and in the spectra of stars. The figure on the left shows
the example of the spectrum of the star \index{Sirius}Sirius,
in which there are ``gap teeth'' at certain wavelengths.
Taking this spectrum as an example, we can give a straightforward
explanation using quantum physics.

   Energy is released in the dense interior of the star, but
the outer layers of the star are thin, so the atoms are far
apart and electrons are confined within individual atoms.
Although their standing-wave patterns are not as simple as
those of the particle in the box, their energies are quantized.

   When a photon is on its way out through the outer layers,
it can be absorbed by an electron in an atom, but only if
the amount of energy it carries happens to be the right
amount to kick the electron from one of the allowed energy
levels to one of the higher levels. The photon energies that
are missing from the spectrum are the ones that equal the
difference in energy between two electron energy levels.
(The most prominent of the absorption lines in Sirius's
spectrum are absorption lines of the hydrogen atom.)
\end{eg}

\begin{eg}{The stability of atoms}
   In many \index{Star Trek}Star Trek episodes the
Enterprise, in orbit around a planet, suddenly lost engine
power and began spiraling down toward the planet's surface.
This was utter nonsense, of course, due to conservation of
energy: the ship had no way of getting rid of energy, so it
did not need the engines to replenish it.

   Consider, however, the electron in an atom as it orbits
the nucleus. The electron \emph{does} have a way to release
energy:  it has an acceleration due to its continuously
changing direction of motion, and according to classical
physics, any accelerating charged particle emits electromagnetic
waves. According to classical physics, atoms should collapse!

   The solution lies in the observation that a bound state
has a minimum energy. An electron in one of the higher-energy
atomic states can and does emit photons and hop down step by
step in energy. But once it is in the ground state, it
cannot emit a photon because there is no lower-energy
state for it to go to.
\end{eg}

<% marg(0) %>
<%
  fig(
    'h-molecule',
    %q{%
      Two hydrogen atoms bond to form
       an $\zu{H}_2$ molecule. In the molecule, the two electrons' wave patterns overlap
      , and are about twice as wide.
    }
  )
%>
<% end_marg %>

m4_include(../share/quantum/eg/h2-bond.tex)

\startdqs

__incl(dq/dineutron)

\begin{dq}
The following table shows the energy gap between the
ground state and the first excited state for four nuclei, in
units of picojoules. (The nuclei were chosen to be ones
that have similar structures, e.g., they are all spherical in shape.)

\begin{tabular}{ll}
    nucleus    &  energy gap (picojoules)\\
    $^4\zu{He}$       & 3.234\\
    $^{16}\zu{O}$     & 0.968\\
    $^{40}\zu{Ca}$    & 0.536\\
    $^{208}\zu{Pb}$   & 0.418\\
\end{tabular}

\noindent Explain the trend in the data.
\end{dq}

<% end_sec('qm-bound-states') %>

<% begin_sec('The uncertainty principle',nil,'uncertainty-principle') %>

\index{Heisenberg!Werner}\index{Heisenberg uncertainty
principle}\index{uncertainty principle}

<% begin_sec("Eliminating randomness through measurement?") %>

A common reaction to quantum physics, among both early-twentieth-century
physicists and modern students, is that we should be able to
get rid of randomness through accurate measurement. If I
say, for example, that it is meaningless to discuss the path
of a photon or an electron, one might suggest that we simply
measure the particle's position and velocity many times in a
row. This series of snapshots would amount to a description of its path.

A practical objection to this plan is that the process of
measurement will have an effect on the thing we are trying
to measure. This may not be of much concern, for example,
when a traffic cop measure's your car's motion with a radar
gun, because the energy and momentum of the radar pulses are
insufficient to change the car's motion significantly. But
on the subatomic scale it is a very real problem. Making a
videotape through a microscope of an electron orbiting a
nucleus is not just difficult, it is theoretically
impossible. The video camera makes pictures of things using
light that has bounced off them and come into the camera. If
even a single photon of visible light was to bounce off of
the electron we were trying to study, the electron's recoil
would be enough to change its behavior significantly.  

<% end_sec() %>

<% marg(100) %>
<%
  fig(
    'heisenberg',
    %q{Werner Heisenberg (1901-1976). Heisenberg helped to develop the foundations of quantum mechanics,
       including the Heisenberg uncertainty principle. He was the scientific leader of
       the Nazi atomic-bomb program up until its cancellation in 1942, when the
       military decided that it was too ambitious a project
       to undertake in wartime, and too unlikely to produce results.}
  )
%>
<% end_marg %>

<% begin_sec("The Heisenberg uncertainty principle") %>

This insight, that measurement changes the thing being
measured, is the kind of idea that clove-cigarette-smoking
intellectuals outside of the physical sciences like to claim
they knew all along. If only, they say, the physicists had
made more of a habit of reading literary journals, they
could have saved a lot of work. The anthropologist Margaret
Mead has recently been accused of inadvertently encouraging
her teenaged Samoan informants to exaggerate the freedom of
youthful sexual experimentation in their society. If this is
considered a damning critique of her work, it is because she
could have done better: other anthropologists claim to have
been able to eliminate the observer-as-participant problem
and collect untainted data.

The German physicist Werner Heisenberg, however, showed that
in quantum physics, \emph{any} measuring technique runs into
a brick wall when we try to improve its accuracy beyond a
certain point. Heisenberg showed that the limitation is a
question of \emph{what there is to be known}, even in
principle, about the system itself, not of the ability or
inability of a specific measuring device to ferret out
information that is knowable but not previously hidden.

Suppose, for example, that we have constructed an electron
in a box (quantum dot) setup in our laboratory, and we are
able to adjust the length $L$ of the box as desired. All the
standing wave patterns pretty much fill the box, so our
knowledge of the electron's position is of limited accuracy.
If we write $\Delta x$ for the range of uncertainty in our
knowledge of its position, then $\Delta x$ is roughly the
same as the length of the box:\label{heisenberg-argument}
\begin{equation*}
        \Delta x \approx L
\end{equation*}
If we wish to know its position more accurately, we can
certainly squeeze it into a smaller space by reducing $L$,
but this has an unintended side-effect. A standing wave is
really a superposition of two traveling waves going in
opposite directions. The equation $p=h/\lambda $ really only
gives the magnitude of the momentum vector, not its
direction, so we should really interpret the wave as a 50/50
mixture of a right-going wave with momentum $p=h/\lambda $
and a left-going one with momentum $p=-h/\lambda $. The
uncertainty in our knowledge of the electron's momentum is
$\Delta p=2h/\lambda$, covering the range between these two
values. Even if we make sure the electron is in the ground
state, whose wavelength $\lambda =2L$ is the longest
possible, we have an uncertainty in momentum of 
$\Delta p=h/L$. In general, we find
\begin{equation*}
        \Delta p \gtrsim h/L\eqquad,
\end{equation*}
with equality for the ground state and inequality for the
higher-energy states. Thus if we reduce $L$ to improve our
knowledge of the electron's position, we do so at the cost
of knowing less about its momentum. This trade-off is neatly
summarized by multiplying the two equations to give
\begin{equation*}
        \Delta p\Delta x \gtrsim h\eqquad.
\end{equation*}
Although we have derived this in the special case of a
particle in a box, it is an example of a principle of
more general validity:
\begin{important}[The Heisenberg uncertainty principle]
It is not possible, even in principle, to know the momentum
and the position of a particle simultaneously and with
perfect accuracy. The uncertainties in these two quantities
are always such that $\Delta p\Delta x \gtrsim h$.
\end{important}
\noindent (This approximation can be made into a strict inequality,
$\Delta p\Delta x>h/4\pi$, but only with more careful
definitions, which we will not bother with.)

Note that although I encouraged you to think of this
derivation in terms of a specific real-world system, the
quantum dot, no reference was ever made to any specific
laboratory equipment or procedures. The argument is simply
that we cannot \emph{know} the particle's position very
accurately unless it \emph{has} a very well defined
position, it cannot have a very well defined position unless
its wave-pattern covers only a very small amount of space,
and its wave-pattern cannot be thus compressed without
giving it a short wavelength and a correspondingly uncertain
momentum. The uncertainty principle is therefore a
restriction on how much there is to know about a particle,
not just on what we can know about it with a certain technique.

\begin{eg}{An estimate for electrons in atoms}
\egquestion A typical energy for an electron in an atom is on
the order of $(\text{1 volt})\cdot e$, which corresponds to a speed of
about 1\% of the speed of light. If a typical atom has a
size on the order of 0.1 nm, how close are the electrons to
the limit imposed by the uncertainty principle?

\eganswer If we assume the electron moves in all directions
with equal probability, the uncertainty in its momentum is
roughly twice its typical momentum. This is only an order-of-magnitude
estimate, so we take $\Delta p$ to be the same as a typical momentum:
\begin{align*}
 \Delta p \Delta x         &=    p_{typical} \Delta x   \\
                         &=    (m_{electron}) (0.01c) (0.1\times10^{-9}\ \munit)  \\
                         &=    3\times 10^{-34}\ \zu{J}\unitdot\zu{s}  
\end{align*}
This is on the same order of magnitude as Planck's constant,
so evidently the electron is ``right up against the wall.''
(The fact that it is somewhat less than $h$ is of no concern
since this was only an estimate, and we have not stated the
uncertainty principle in its most exact form.)
\end{eg}

<% self_check('smallh',<<-'SELF_CHECK'

If we were to apply the uncertainty principle to human-scale
objects, what would be the significance of the small
numerical value of Planck's constant?
  SELF_CHECK
  ) %>

<% end_sec() %>

\startdqs

\begin{dq}
Compare $\Delta p$  and $\Delta x$ for the two lowest
energy levels of the one-dimensional particle in a box, and
discuss how this relates to the uncertainty principle.
\end{dq}

\begin{dq}
On a graph of $\Delta p$ versus $\Delta $x, sketch the
regions that are allowed and forbidden by the Heisenberg
uncertainty principle. Interpret the graph: Where does an
atom lie on it? An elephant? Can either $p$ or $x$ be
measured with perfect accuracy if we don't care about the other?
\end{dq}

<% end_sec() %>

<% begin_sec("Electrons in electric fields",nil,'electrons-in-fields') %>

So far the only electron wave patterns we've considered have
been simple sine waves, but whenever an electron finds
itself in an electric field, it must have a more complicated
wave pattern. Let's consider the example of an electron
being accelerated by the electron gun at the back of a TV
tube. Newton's laws are not useful, because they implicitly
assume that the path taken by the particle is a meaningful
concept. Conservation of energy is still valid in quantum
physics, however. In terms of energy, the electron is moving
from a region of low voltage into a region of higher
voltage. Since its charge is negative, it loses electrical energy by moving
to a higher voltage, so its kinetic energy increases. As its electrical
energy goes down, its kinetic energy goes up by an equal
amount, keeping the total energy constant. Increasing
kinetic energy implies a growing momentum, and therefore a
shortening wavelength, \figref{accelerating-electron}.

The wavefunction as a whole does not have a single
well-defined wavelength, but the wave changes so gradually
that if you only look at a small part of it you can still
pick out a wavelength and relate it to the momentum and
energy. (The picture actually exaggerates by many orders of
magnitude the rate at which the wavelength changes.)

<% marg(100) %>
<%
  fig(
    'accelerating-electron',
    %q{%
      An electron in a gentle electric 
      field gradually shortens its wavelength as it gains energy.
      (As discussed on p.~\pageref{cheat-with-real-psi}, it is actually not quite correct to
      graph the wavefunction of an electron as a real number unless it is a standing wave, which
      isn't the case here.)
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'kinks',
    %q{%
      The wavefunction's tails go where classical
      physics says they shouldn't.
    }
  )
%>
<% end_marg %>

But what if the electric field was stronger? The electric
field in an old-fashioned vacuum tube TV screen is only $\sim10^5$  N/C, but the electric field
within an atom is more like $10^{12}$  N/C. In figure \figref{osculating},
the wavelength changes so rapidly that there is nothing that
looks like a sine wave at all. We could get a rough idea of
the wavelength in a given region by measuring the distance
between two peaks, but that would only be a rough approximation.
Suppose we want to know the wavelength at point $P$. The
trick is to construct a sine wave, like the one shown with
the dashed line, which matches the curvature of the actual
wavefunction as closely as possible near $P$. The sine wave
that matches as well as possible is called the ``osculating''
curve, from a Latin word meaning ``to kiss.'' The wavelength
of the osculating curve is the wavelength that will relate
correctly to conservation of energy.

<%
  fig(
    'osculating',
    %q{%
      A typical wavefunction of
       an electron in an atom (heavy curve) and the
       osculating sine wave (dashed curve) that matches its curvature at point P.
    },
    {
      'width'=>'wide'
    }
  )
%>

\index{tunneling}
<% begin_sec("Tunneling") %>

We implicitly assumed that the particle-in-a-box wavefunction
would cut off abruptly at the sides of the box, \figref{kinks}/1, but
that would be unphysical. A kink has infinite curvature, and
curvature is related to energy, so it can't be infinite. A
physically realistic wavefunction must always ``tail off''
gradually, \figref{kinks}/2. In classical physics, a particle can never
enter a region in which its interaction energy $U$ would be
greater than the amount of energy it has available. But in
quantum physics the wavefunction will always have a tail
that reaches into the classically forbidden region. If it
was not for this effect, called tunneling, the fusion
reactions that power the sun would not occur due to the high
electrical energy nuclei need in order to get close together!
Tunneling is discussed in more detail in the following subsection.

<% end_sec() %>

<% end_sec() %>

<% begin_sec("The Schr\\\"odinger equation",nil,'schrodinger') %>

\index{Schr\"odinger equation}
In subsection \ref{subsec:electrons-in-fields} we were able to apply conservation
of energy to an electron's wavefunction, but only by using
the clumsy graphical technique of osculating sine waves as a
measure of the wave's curvature. You have learned a more
convenient measure of curvature in calculus: the second
derivative. To relate the two approaches, we take the second
derivative of a sine wave:
\begin{align*}
 \frac{\der^2}{\der x^2}\sin(2\pi x/\lambda)
 &= \frac{\der}{\der x}\left(\frac{2\pi}{\lambda}\cos\frac{2\pi x}{\lambda}\right)     \\
         &= -\left(\frac{2\pi}{\lambda}\right)^2 \sin\frac{2\pi x}{\lambda}
\end{align*}

Taking the second derivative gives us back the same
function, but with a minus sign and a constant out in front
that is related to the wavelength. We can thus relate the
second derivative to the osculating wavelength:

\begin{equation}\label{eq:schreqna}
        \frac{\der^2\Psi}{\der x^2} = -\left(\frac{2\pi}{\lambda}\right)^2\Psi
\end{equation}

This could be solved for $\lambda $ in terms of $\Psi $, but
it will turn out below to be more convenient to leave it in this form.

Applying this to conservation of energy, we have
\begin{align}\label{eq:schreqnb}
\begin{split}
           E         &=    K  +  U  \\
                 &=   \frac{p^2}{2m}  + U  \\
                 &=   \frac{(h/\lambda)^2}{2m}  + U        
\end{split}
\end{align}

\noindent Note that both equation \eqref{eq:schreqna} and equation \eqref{eq:schreqnb} have $\lambda^2$
in the denominator. We can simplify our algebra by
multiplying both sides of equation \eqref{eq:schreqnb} by $\Psi $ to make it
look more like equation \eqref{eq:schreqna}:

\begin{align*}
        E \cdot \Psi          &=    \frac{(h/\lambda)^2}{2m}\Psi    +   U \cdot \Psi   \\
 &=   \frac{1}{2m}\left(\frac{h}{2\pi}\right)^2\left(\frac{2\pi}{\lambda}\right)^2\Psi
                                 +   U \cdot \Psi  \\
 &=  -\frac{1}{2m}\left(\frac{h}{2\pi}\right)^2 \frac{\der^2\Psi}{\der x^2} 
                                 +   U \cdot \Psi
\end{align*}

\noindent Further simplification is achieved by using the symbol $\hbar$ ($h$
with a slash through it, read ``h-bar'') as an abbreviation
for $h/2\pi $. We then have the important result known as
the \labelimportantintext{Schr\"odinger equation}:

\begin{important}\label{s-eqn-simplest-initial-statement}
\begin{equation*}
        E \cdot \Psi = -\frac{\hbar^2}{2m}\frac{\der^2\Psi}{\der x^2}  +   U \cdot \Psi
\end{equation*}
\end{important}
\noindent (Actually this is a simplified version of the Schr\"odinger
equation, applying only to standing waves in one dimension.)
Physically it is a statement of conservation of energy. The
total energy $E$ must be constant, so the equation tells us
that a change in interaction energy $U$ must be accompanied by a
change in the curvature of the wavefunction. This change in
curvature relates to a change in wavelength, which
corresponds to a change in momentum and kinetic energy.

<% self_check('schrodingerassumptions',<<-'SELF_CHECK'
Considering the assumptions that were made in deriving the
Schr\"odinger equation, would it be correct to apply it to a
photon? To an electron moving at relativistic speeds?
  SELF_CHECK
  ) %>

Usually we know right off the bat how $U$ depends on
$x$, so the basic mathematical problem of quantum physics is
to find a function $\Psi (x$) that satisfies the Schr\"odinger
equation for a given interaction-energy function $U(x)$.
An equation, such as the Schr\"odinger equation, that
specifies a relationship between a function and its
derivatives is known as a differential equation.

The detailed study of the solution of the Schr\"odinger equation is
beyond the scope of this book,
but we can gain some
important insights by considering the easiest version of the
Schr\"odinger equation, in which the interaction energy $U$ is
constant. We can then rearrange the Schr\"odinger equation as follows:
\begin{align*}
   \frac{\der^2\Psi}{\der x^2} &= \frac{2m(U-E)}{\hbar^2} \Psi\eqquad,
\intertext{which boils down to}
   \frac{\der^2\Psi}{\der x^2} &= a\Psi\eqquad,
\end{align*}
where, according to our assumptions, $a$ is independent of
$x$. We need to find a function whose second derivative is
the same as the original function except for a multiplicative
constant. The only functions with this property are sine
waves and exponentials:

\begin{align*}
 \frac{\der^2}{\der x^2}\left[\:q\sin(rx+s)\:\right] &= -qr^2\sin(rx+s)     \\
 \frac{\der^2}{\der x^2}\left[qe^{rx+s}\right] &= qr^2e^{rx+s}
\end{align*}

The sine wave gives negative values of $a$, $a=-r^2$, and the
exponential gives positive ones, $a=r^2$. The former applies
to the classically allowed region with $U<E$.

\label{quantitativetunneling}

<% marg(60) %>
<%
  fig(
    'barrier-with-u-notation',
    %q{Tunneling through a barrier.
      (As discussed on p.~\pageref{cheat-with-real-psi}, it is actually not quite correct to
      graph the wavefunction of an electron as a real number unless it is a standing wave, which
      isn't the case here.)
    }
  )
%>
<% end_marg %>

This leads us to a quantitative calculation of the tunneling
effect discussed briefly in the preceding subsection. The
wavefunction evidently tails off exponentially in the
classically forbidden region. Suppose, as shown in
figure \figref{barrier-with-u-notation},
a wave-particle traveling to the right encounters a
barrier that it is classically forbidden to enter. Although
the form of the Schr\"odinger equation we're using technically
does not apply to traveling waves (because it makes no
reference to time), it turns out that we can still use it to
make a reasonable calculation of the probability that the
particle will make it through the barrier. If we let the
barrier's width be $w$, then the ratio of the wavefunction
on the left side of the barrier to the wavefunction on the right is
\begin{equation*}
        \frac{qe^{rx+s}}{qe^{r(x+w)+s}}  = e^{-rw}\eqquad.  
\end{equation*}
\pagebreak
Probabilities are proportional to the squares of wavefunctions,
so the probability of making it through the barrier is\label{tunneling-probability}
\begin{align*}
   P         &=  e^{-2rw}    \\
         &= \exp\left(-\frac{2w}{\hbar}\sqrt{2m(U-E)}\right) .
\end{align*}

<% self_check('walkthroughwall',<<-'SELF_CHECK'
If we were to apply this equation to find the probability
that a person can walk through a wall, what would the small
value of Planck's constant imply?
  SELF_CHECK
  ) %>

\begin{eg}{Tunneling in alpha decay}\label{eg:alpha-tunneling}
Naively, we would expect alpha decay to be a very fast process. The typical speeds of neutrons and
protons inside a nucleus are extremely high (see problem \ref{hw:lead}). If we imagine an alpha
particle coalescing out of neutrons and protons inside the nucleus, then at the typical speeds we're
talking about, it takes a ridiculously small amount of time for them to reach the surface and try
to escape. Clattering back and forth inside the nucleus, we could imagine them making a vast number of
these ``escape attempts'' every second.

Consider 
figure \figref{alpha-potential}, however, which shows the interaction energy for an alpha particle escaping from a nucleus.
The electrical energy is $kq_1q_2/r$ when the alpha is outside the nucleus, while its variation inside
the nucleus has the shape of a parabola, as a consequence of the shell theorem.
The nuclear energy is constant when the alpha is inside the nucleus, because the forces from all the
neighboring neutrons and protons cancel out; it rises sharply near the surface, and flattens out
to zero over a distance of $\sim 1$ fm, which is the maximum distance scale at which the strong force
can operate.
There is a classically forbidden region immediately outside the nucleus, so the alpha particle can only
escape by quantum mechanical tunneling. (It's true, but somewhat counterintuitive, that a \emph{repulsive}
electrical force can make it more difficult for the alpha to get \emph{out}.) 

In reality, alpha-decay half-lives are often extremely long --- sometimes billions of years --- because
the tunneling probability is so small. Although the shape of the barrier is not a rectangle, the equation
for the tunneling probability on page \pageref{tunneling-probability}
can still be used as a rough guide to our thinking. Essentially the tunneling probability is so small
because $U-E$ is fairly big, typically about 30 MeV at the peak of the barrier.
\end{eg}

<% marg(150) %>
<%
  fig(
    'alpha-potential',
    %q{The electrical, nuclear, and total interaction energies for an alpha particle escaping from a nucleus.}
  )
%>
<% end_marg %>

\begin{eg}{The correspondence principle for $E>U$}\label{eg:step-correspondence}
The correspondence principle demands that in the classical limit $h\rightarrow0$,
we recover the correct result for a particle encountering a barrier $U$,
for both $E<U$ and $E>U$.
The $E<U$ case was analyzed in self-check \ref{sc:walkthroughwall}
on p.~\pageref{sc:walkthroughwall}.
In the remainder of this example, we analyze $E>U$, which
turns out to be a little trickier. 

The particle has enough energy to
get over the barrier, and the classical result is that it continues forward
at a different speed (a reduced speed if $U>0$, or an increased one if $U<0$), then
regains its original speed as it emerges from the other side.
What happens quantum-mechanically in this case? We would like to get a ``tunneling''
probability of 1 in the classical limit. The expression derived on p.~\pageref{tunneling-probability}, however,
doesn't apply here, because it was derived under the assumption that the wavefunction
inside the barrier was an exponential; in the classically allowed case, the barrier
isn't classically forbidden, and the wavefunction inside it is a sine wave.

<% marg(10) %>
<%
  fig(
    'step-potential',
    %q{A particle encounters a step of height $U<E$ in the interaction energy. Both sides are
       classically allowed. A reflected wave exists, but is not shown in the figure.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'marble-no-reflection',
    %q{The marble has zero probability of being reflected from the edge of the table.
       (This example has $U<0$, not $U>0$ as in figures \figref{step-potential} and \figref{step-potential-smoother}).}
  )
%>

\spacebetweenfigs

<%
  fig(
    'step-potential-smoother',
    %q{Making the step more gradual reduces the probability of reflection.}
  )
%>
<% end_marg %>

We can simplify things a little by letting the width $w$ of the
barrier go to infinity.  Classically, after all, there is no
possibility that the particle will turn around, no matter how wide the
barrier. We then have the situation shown in figure
\figref{step-potential}.\footnote{As in several previous examples, we
cheat by representing a traveling wave as a real-valued function. See
pp.~\pageref{cheat-with-real-psi} and \pageref{subsubsec:s-eqn-partial-refl}.}

The analysis is similar to that for any other wave being partially
reflected at the boundary between two regions where its velocity
differs, and the result is the same as the one found on
p.~\pageref{reflected-wave-amplitude}. (There are some technical
differences, which don't turn out to matter. This is discussed in more
detail on p.~\pageref{subsubsec:s-eqn-partial-refl}.) The ratio of the amplitude of the reflected
wave to that of the incident wave is $R = (v_2-v_1)/(v_2+v_1)$. The
probability of reflection is $R^2$. (Counterintuitively, $R^2$ is
nonzero even if $U<0$, i.e., $v_2>v_1$.) 

This seems to violate the correspondence principle. There is
no $m$ or $h$ anywhere in the result, so we seem to have the result that, even classically,
the marble in figure \figref{marble-no-reflection} can be reflected!

The solution to this paradox is that the step in figure \figref{step-potential} was taken to
be completely abrupt --- an idealized mathematical discontinuity. Suppose we make the
transition a little more gradual, as in figure \figref{step-potential-smoother}.
As shown in problem \ref{hw:maxtransmission} on p.~\pageref{hw:maxtransmission},
this reduces the amplitude with which a wave is reflected.
By smoothing out the step more and
more, we continue to reduce the probability of reflection, until finally we arrive at
a barrier shaped like a smooth ramp. More detailed calculations show that this
results in zero reflection in the limit where the width of the ramp is large
compared to the wavelength.
\end{eg}

\begin{eg}{Beta decay: a push or pull on the way out the door}
The nucleus ${}^{64}\text{Cu}$ undergoes $\beta^+$ and $\beta^-$ decay with similar probabilities
and energies. Each of these decays releases a fixed amount of energy $Q$ due to 
the difference in mass between the parent nucleus and the decay products.
This energy is shared randomly between the beta and the neutrino. In experiments,
the beta's energy is easily measured, while the neutrino flies off without interacting.
Figure \figref{cu64-betas}
shows the energy spectrum of the $\beta^+$ and $\beta^-$ in these decays.\footnote{Redrawn
from Cook and Langer, 1948.} There is a relatively
high probability for the beta and neutrino each to carry off roughly half the kinetic energy,
the reason being identical to the kind of phase-space argument discussed in sec.~\ref{subsec:phase-space},
p.~\pageref{subsec:phase-space}. Therefore in each case we get a bell-shaped curve stretching from
0 up to the energy $Q$, with $Q$ being slightly different in the two cases.

<% marg(10) %>
<%
  fig(
    'cu64-betas',
    %q{$\beta^+$ and $\beta^-$ spectra of ${}^{64}\text{Cu}$.}
  )
%>
<% end_marg %>

So we expect the two bell curves to
look almost the same except for a slight rescaling of the horizontal
axis. Yes --- but we also see markedly
different behavior at low energies. At very low energies, there is almost
no chance to see a $\beta^+$ with very low energy, but
quite a high probability for a $\beta^-$.

We could try to explain this difference in terms of the release of
electrical energy.  The $\beta^+$ is repelled by the
nucleus, so it gets an extra push on the way out the door.  A
$\beta^-$ should be held back as it exits, and so should lose some
energy. The bell curves should be shifted up
and down in energy relative to one another, as observed.

But if we try to estimate this energy shift using classical
physics, we come out with a wildly incorrect answer. This would be
a process in which the beta and neutrino are
released in a pointlike event inside the nucleus. 
The radius $r$ of the ${}^{64}\text{Cu}$ nucleus is on the order of 4 fm 
($1\ \zu{fm}=10^{-15}\ \munit$). Therefore the  energy lost or gained by the
$\beta^+$ or $\beta^-$ on the way out would be $U\sim kZe^2/r\sim10\
\text{MeV}$. The actual shift is much smaller.

To understand what's really going on, we need quantum mechanics. A beta in
the observed energy range has a
wavelength of about 2000 fm, which is hundreds of times
greater than the size of the nucleus. Therefore the beta cannot be
much better localized than that when it is emitted.  This means that
we should really use something more like $r\sim 500\ \text{fm}$ (a
quarter of a wavelength) in our calculation of the electrical energy.
This gives $U\sim0.08\ \text{MeV}$, which is about the right order of
magnitude compared to observation. 

A byproduct of this analysis is that a $\beta^+$ is always emitted
within the classically forbidden region, and then has to tunnel out
through the barrier. As in example \ref{eg:alpha-tunneling}, we have
the counterintuitive fact about quantum
mechanics that a repulsive force can \emph{hinder} the escape of a
particle.
\end{eg}

\pagebreak

<% begin_sec("Three dimensions",4) %>

For simplicity, we've been considering the Schr\"odinger equation in one dimension,
so that $\Psi$ is only a function of $x$, and has units of $\munit^{-1/2}$ rather
than  $\munit^{-3/2}$.
Since the Schr\"odinger equation is a statement of conservation of energy, and
energy is a scalar, the generalization to three dimensions isn't particularly
complicated. The total energy term $E\cdot\Psi$ and the interaction energy term
$U\cdot\Psi$ involve nothing but scalars, and don't need to be changed at all.
In the kinetic energy term, however, we're essentially basing our computation of
the kinetic energy on the squared magnitude of the momentum, $p_x^2$,
and in three dimensions this would clearly have to be generalized to
$p_x^2+p_y^2+p_z^2$. The obvious way to achieve this is to replace
the second derivative $\der^2\Psi/\der x^2$ with the sum
 $\partial^2\Psi/\partial x^2+ \partial^2\Psi/\partial y^2+ \partial^2\Psi/\partial z^2$. Here the
partial derivative symbol $\partial$, introduced on page \pageref{partial-der}, indicates that
when differentiating with respect to a particular variable, the other variables are to be considered
as constants. This operation
on the function $\Psi$ is notated $\nabla^2\Psi$, and the derivative-like
operator $\nabla^2=\partial^2/\partial x^2+ \partial^2/\partial y^2+ \partial^2/\partial z^2$ is called the Laplacian, and was introduced briefly
on p.~\pageref{laplacian-em}.\index{Laplacian}\label{laplacian-qm} It occurs elsewhere in physics.
For example, in classical electrostatics,
the voltage in a region of vacuum must be a solution of the equation $\nabla^2V=0$.
Like the second derivative, the Laplacian is essentially a measure of curvature.
Or, as shown in figure \figref{laplacian-geometrical}, we can think of it as a measure of how much the value
of a function at a certain point differs from the average of its value on nearby points.

<% marg(-300) %>

<%
  fig(
    'laplacian-geometrical',
    %q{%
      1.~The one-dimensional version of the Laplacian is the second derivative. It is positive here
          because the average of the two nearby points is greater than the value at the center.
      2.~The Laplacian of the function $A$ in example \ref{eg:laplacian-2d} is positive because
         the average of the four nearby points along the perpendicular axes is greater than the
         function's value at the center.
      3.~$\nabla^2 C=0$. The average is the same as the value at the center.
    }
  )
%>
<% end_marg %>

\enlargethispage{-\baselineskip}

\begin{eg}{Examples of the Laplacian in two dimensions}\label{eg:laplacian-2d}
\egquestion
Compute the Laplacians of the following functions in two dimensions, and interpret them:
$A=x^2+y^2$, $B=-x^2-y^2$, $C=x^2-y^2$.

\eganswer
The first derivative of function $A$ with respect to $x$ is $\partial A/\partial x=2x$. Since $y$
is treated as a constant in the computation of the partial derivative $\partial/\partial x$, the
second term goes away. The second derivative of $A$ with respect to $x$ is
$\partial^2 A/\partial x^2=2$. Similarly we have $\partial^2 A/\partial y^2=2$, so
$\nabla^2 A=4$.

All derivative operators, including $\nabla^2$, have the linear property that multiplying
the input function by a constant just multiplies the output function by the same constant.
Since $B=-A$, and we have $\nabla^2 B=-4$.

For function $C$, the $x$ term contributes a second derivative of 2, but the $y$ term
contributes $-2$, so $\nabla^2 C=0$.

The interpretation of the positive sign in $\nabla^2 A=4$ is that $A$'s graph is shaped
like a trophy cup, and the cup is concave up. 
$\nabla^2 B<0$ is because $B$ is concave down. Function $C$ is shaped like a saddle.
Since its curvature along one axis is concave up, but the curvature along the other is
down and equal in magnitude, the function is considered to have zero concavity over all.
\end{eg}

\begin{eg}{A classically allowed region with constant $U$}
In a classically allowed region with constant $U$, we expect the solutions
to the Schr\"odinger equation to be sine waves. A sine wave in three dimensions
has the form
\begin{equation*}
  \Psi = \sin\left( k_x x + k_y y + k_z z  \right)\eqquad.
\end{equation*}
When we compute $\partial^2\Psi/\partial x^2$, double differentiation of
$\sin$ gives $-\sin$, and the chain rule brings out a factor of $k_x^2$.
Applying all three second derivative operators, we get
\begin{align*}
  \nabla^2\Psi &= \left(-k_x^2-k_y^2-k_z^2\right)\sin\left( k_x x + k_y y + k_z z  \right) \\
               &=  -\left(k_x^2+k_y^2+k_z^2\right)\Psi\eqquad.
\end{align*}
The Schr\"odinger equation gives
\begin{align*}
  E\cdot\Psi &= -\frac{\hbar^2}{2m}\nabla^2\Psi + U\cdot\Psi \\
             &= -\frac{\hbar^2}{2m}\cdot -\left(k_x^2+k_y^2+k_z^2\right)\Psi + U\cdot\Psi \\
  E-U        &= \frac{\hbar^2}{2m}\left(k_x^2+k_y^2+k_z^2\right)\eqquad,
\end{align*}
which can be satisfied since we're in a classically allowed region with $E-U>0$, and the right-hand
side is manifestly positive.
\end{eg}

<% end_sec %>

<%
  fig(
    'complex-wavefunction',
    %q{%
      1. Oscillations can go back and forth, but it's also possible for them to move along a path that bites its
      own tail, like a circle. Photons act like one, electrons like the other.\\\\
      2. Back-and-forth oscillations can naturally be described by a segment taken from the real number line, and
      we visualize the corresponding type of wave as a sine wave. Oscillations around a closed path relate more
      naturally to the complex number system. The complex number system has rotation built into its
      structure, e.g., the sequence 1, $i$, $i^2$, $i^3$, \ldots rotates around the unit circle in 90-degree increments.\\\\
      3. The double slit experiment embodies the one and only mystery of quantum physics. Either type of wave
      can undergo double-slit interference.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

<% begin_sec("Use of complex numbers",nil,'complex-wavefunction') %>

\index{wavefunction!complex numbers in}\index{complex numbers!in quantum physics}
In a classically forbidden region, a particle's total
energy, $U+K$, is less than its $U$, so
its $K$ must be negative. If we want to keep believing
in the equation $K=p^2/2m$, then apparently the
momentum of the particle is the square root of a negative
number. This is a symptom of the fact that the Schr\"odinger
equation fails to describe all of nature unless the
wavefunction and various other quantities are allowed to be
complex numbers. In particular it is not possible to
describe traveling waves correctly without using complex wavefunctions.
Complex numbers were reviewed in subsection \ref{subsec:complex-numbers}, p.~\pageref{subsec:complex-numbers}.

This may seem like nonsense, since real numbers are the only
ones that are, well, real! Quantum mechanics can always be
related to the real world, however, because its structure is
such that the results of measurements always come out to be
real numbers. For example, we may describe an electron as
having non-real momentum in classically forbidden regions,
but its average momentum will always come out to be real
(the imaginary parts average out to zero), and it can never
transfer a non-real quantity of momentum to another particle.

A complete investigation of these issues is beyond the scope
of this book, and this is why we have normally limited
ourselves to standing waves, which can be described with
real-valued wavefunctions. Figure \figref{complex-wavefunction}
gives a visual depiction of the difference between real and
complex wavefunctions. The following remarks may also be helpful.

Neither of the graphs in \subfigref{complex-wavefunction}{2} should be
interpreted as a path traveled by something. This isn't anything mystical about
quantum physics. It's just an ordinary fact about waves, which we first encountered
in subsection \ref{subsec:wave-motion}, p.~\pageref{subsec:wave-motion},
where we saw the distinction between the motion
of a wave and the motion of a wave pattern. In \emph{both} examples in \subfigref{complex-wavefunction}{2},
the wave pattern is moving in a straight line to the right.

The helical graph in \subfigref{complex-wavefunction}{2}
shows a complex wavefunction whose value rotates around a circle in the
complex plane with a frequency $f$ related to its energy by $E=hf$. As it does
so, its squared magnitude $|\Psi|^2$ stays the same, so the corresponding probability stays
constant. Which direction does it rotate? This direction is purely a matter of convention,
since the distinction between the symbols $i$ and $-i$ is arbitrary --- both are equally valid
as square roots of $-1$. We can, for example, arbitrarily say that electrons with positive energies
have wavefunctions whose phases rotate counterclockwise, and as long as we follow that rule
consistently within a given calculation, everything will work. Note that it is not possible
to define anything like a right-hand rule here, because the complex plane shown in
the right-hand side of \subfigref{complex-wavefunction}{2} doesn't represent two dimensions of
physical space; unlike a screw going into a piece of wood, an electron doesn't have a direction
of rotation that depends on its direction of travel.

\begin{eg}{Superposition of complex wavefunctions}
\egquestion
The right side of figure \subfigref{complex-wavefunction}{3} is a cartoonish representation of
double-slit interference; it depicts the situation at the center, where symmetry guarantees that
the interference is constructive. Suppose that at some off-center point, the two wavefunctions
being superposed are $\Psi_1=b$ and $\Psi_2=bi$, where $b$ is a real number with units.
Compare the probability of finding the electron at this position with what it would have been
if the superposition had been purely constructive, $b+b=2b$.

\eganswer
The probability per unit volume is proportional to the square of the magnitude of the total
wavefunction, so we have
\begin{equation*}
  \frac{P_{\text{off center}}}{P_{\text{center}}} 
         = \frac{|b+bi|^2}{|b+b|^2} = \frac{1^2+1^2}{2^2+0^2} = \frac{1}{2}\eqquad.
\end{equation*}
\end{eg}

Figure \figref{rainbow} shows a method for visualizing complex wavefunctions. The idea
is to use colors to represent complex numbers, according to the arbitrary convention
defined in figure \subfigref{rainbow}{1}. Brightness indicates magnitude, and the rainbow
hue shows the argument. Because this representation can't be understood in a black and white
printed book, the figure is also reproduced on the back cover of printed copies.
To avoid any confusion, note that the use of rainbow colors does
not mean that we are representing actual visible light. In fact, we will be using these
visual conventions to represent the wavefunctions of a material particle such as an electron.
It is arbitrary that we use red for positive real numbers and blue-green for negative numbers,
and that we pick a handedness for the diagram such that going from red toward yellow means
going counterclockwise. Although physically the rainbow is a linear spectrum, we are not
representing physical colors here, and we are exploiting the fact that the human brain
tends to perceive color as a circle rather than a line, with violet and red being perceptually
similar. One of the limitations of this representation is that brightness is limited, so we
can't represent complex numbers with arbitrarily large magnitudes.
%

<%
  fig(
    'rainbow',
    %q{%
      1.~A representation of complex numbers using color and brightness.
      2.~A wave traveling toward the right.
      3.~A wave traveling toward the left.
      4.~A standing wave formed by superposition of waves 2 and 3.
      5.~A two-dimensional standing wave.
      6.~A double-slit diffraction pattern.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

Figure \subfigref{rainbow}{2} shows a traveling wave as it propagates to the right.
The standard convention in physics is that for a wave moving in a certain direction,
the phase in the forward direction is farther counterclockwise in the complex plane, and
you can verify for yourself that this is the case by comparing with the convention
defined by \subfigref{rainbow}{1}. The function being plotted here is $\Psi=e^{ikx}$,
where $k=2\pi/\lambda$ is the spatial analog of frequency, with an extra factor of $2\pi$ for
convenience. For the use of the complex exponential, see sec.~\ref{subsec:euler-formula},
p~.\pageref{subsec:euler-formula}; it simply represents a point on the unit circle in the
complex plane. The wavelength $\lambda$
is a constant and can be measured, for example, from one yellow point to the next. The wavelength
is \emph{not} different at different points on the figure, because we are using the colors merely
as a visual encoding of the complex numbers --- so, for example, a red point on the figure is
not a point where the wave has a longer wavelength than it does at a blue point.

Figure \subfigref{rainbow}{3} represents a wave traveling to the left.

Figure \subfigref{rainbow}{4} shows a standing wave created by superimposing the traveling
waves from \subfigref{rainbow}{2} and \subfigref{rainbow}{3}, $\Psi_4=(\Psi_2+\Psi_3)/2$. (The reason
for the factor of 2 is simply that otherwise some portions of $\Psi_4$ would have magnitudes too great to be
represented using the available range of brightness.) All points on this wave have real values,
represented by red and blue-green. We made the superposition real by an appropriate choice of
the phases of $\Psi_2$ and $\Psi_3$. This is always possible to do when we have a standing wave,
but it is \emph{only} possible for a standing wave, and this is the reason for all of the disclaimers
in the captions of previous figures in which I took the liberty of representing a traveling wave as
a sine-wave graph.

Figure \subfigref{rainbow}{5} shows a two-dimensional standing wave of a particle in a box,
and \subfigref{rainbow}{6} shows a double-slit interference pattern. (In the latter, I've cheated
by making the amplitude of the wave on the right-hand half of the picture much greater than it
would actually be.)

\begin{eg}{A paradox resolved}
Consider the following paradox. Suppose we have an electron that is traveling wave, and
its wavefunction looks like a wave-train consisting of 5 cycles of a sine wave. Call the distance
between the leading and trailing edges of the wave-train $L$, so that $\lambda=L/5$.
By sketching the wave, you can easily check that
there are 11 points where its value equals zero. Therefore at a particular moment in time, there are 11 points
where a detector has zero probability of detecting the electron.

But now consider how this would look in a frame of reference where the electron is moving
more slowly, at one fifth of the speed we saw in the original frame. In this frame, $L$ is
the same, but $\lambda$ is five times greater, because $\lambda=h/p$. Therefore in this frame
we see only one cycle in the wave-train. Now there are only 3 points where the probability
of detection is zero. But how can this be? All observers, regardless of their frames of reference, should agree
on whether a particular detector detects the electron.

The resolution to this paradox is that it starts from the assumption that we can depict
a traveling wave as a real-valued sine wave, which is zero in certain places. Actually, we can't.
It has to be a complex number with a rotating phase angle in the complex plane, as in figure
\subfigref{rainbow}{2}, and a
\emph{constant} magnitude.
\end{eg}

<% end_sec('complex-wavefunction') %>

<% begin_sec("Linearity of the Schr\\\"odinger equation",nil,'linearity-of-schrodinger') %>

Some mathematical relationships and operations are \emph{linear}, and some are not.
For example, $2\times(3+2)$ is the same as $2\times3+2\times2$, but $\sqrt{1+1}\ne\sqrt{1}+\sqrt{1}$.
Differentiation is a linear operation, $(f+g)'=f'+g'$. The Schr\"odinger equation is
built out of derivatives, so it is linear as well. That is, if $\Psi_1$ and $\Psi_2$ are
both solutions of the Schr\"odinger equation, then so is $\Psi_1+\Psi_2$. Linearity
normally implies linearity with respect both to addition and to multiplication by a scalar.
For example, if $\Psi$ is a solution, then so is $\Psi+\Psi+\Psi$, which is the same as $3\Psi$.

Linearity guarantees that the phase of a wavefunction makes no difference as to its validity
as a solution to the Schr\"odinger equation. If $\sin kx$ is a solution, then so is the sine wave
$-\sin kx$ with the opposite phase. This fact is logically interdependent with the fact that,
as discussed on p.~\pageref{phase-unobservable-basic}, the phase
of a wavefunction is unobservable.\index{phase in quantum mechanics!not observable}
For measuring devices and humans are material objects
that can be described by wavefunctions. So suppose, for example, that we flip the phase of
all the particles inside the entire laboratory. By linearity,
the evolution of this measurement process is still a valid  solution of the Schr\"odinger equation.

The Schr\"odinger equation is a wave equation, and its
linearity implies that the waves obey the principle of superposition.
In most cases in nature, we find that the principle of superposition for waves is at best an
approximation. For example, if the amplitude of
a tsunami is so huge that the trough of the wave reaches all the way down to the ocean floor,
exposing the rocks and sand as it passes overhead, then clearly there is no way to double the
amplitude of the wave and still get something that obeys the laws of physics. Even at less extreme
amplitudes, superposition is only an approximation for water waves, and so for example it is only
approximately true that when two sets of ripples intersect on the surface of a pond, they pass
through without ``seeing'' each other.

It is therefore natural to ask whether the apparent linearity of the Schr\"odinger equation is only
an approximation to some more precise, nonlinear theory. This is not currently believed to be the
case. If we are to make sense of Schr\"odinger's cat
(sec.~\ref{subsec:nonlocality-and-entanglement}, p.~\pageref{schrodingers-cat}),
then the experimenter who sees a live cat and the one
who sees a dead cat must remain oblivious to their other selves, like the ripples on the pond
that intersect without ``seeing'' each other. Attempts to create slightly nonlinear versions
of standard quantum mechanics have been shown to have implausible physical properties, such as
allowing the propagation of signals faster than $c$. (This is known as Gisin's theorem.\label{gisin-theorem}
The original paper, ``Weinberg's non-linear quantum mechanics and supraluminal communications,''
is surprisingly readable and nonmathematical.)\index{Gisin's theorem}

If you have had a course in linear algebra, then it is worth noting that the
linearity of the Schr\"odinger equation allows us to talk about its solutions as vectors in a vector
space. For example, if $\Psi_1$ represents an unstable nucleus that has not yet gamma decayed, and
$\Psi_2$ is its state after the decay, then any superposition $\alpha\Psi_1+\beta\Psi_2$, with real or complex
coefficients $\alpha$ and $\beta$, is
a possible wavefunction, and we can notate this as a vector, $\langle\alpha,\beta\rangle$, in a two-dimensional
vector space.

<% end_sec('linearity-of-schrodinger') %>

\vfill

\startdqs

\begin{dq}
The zero level of interaction energy $U$ is arbitrary, e.g., it's equally valid to pick the zero of
gravitational energy to be on the floor of your lab or at the ceiling. Suppose we're doing the double-slit
experiment, \subfigref{complex-wavefunction}{3}, with electrons. We define the zero-level of $U$ so that
the total energy $E=U+K$ of each electron is positive. and we observe a certain interference
pattern like the one in figure \figref{ccd-diffraction} on p.~\pageref{fig:ccd-diffraction}. What happens
if we then redefine the zero-level of $U$ so that the electrons have $E<0$?
\end{dq}

\pagebreak

\begin{dq}\label{dq:qm-cons-of-prob}
The top panel of the
figure shows a series of snapshots in the motion of two pulses on a coil spring, one negative and one positive,
as they move toward one another and superpose. The final image is very close to the moment at which
the two pulses cancel completely. 
The following discussion is simpler if we consider infinite sine waves rather than pulses.
How can the cancellation of two such mechanical waves be reconciled with conservation of energy?
What about the case of colliding electromagnetic waves? 

Quantum-mechanically, the issue isn't conservation of energy, it's conservation of probability,
i.e., if there's initially a 100\%
probability that a particle exists somewhere, we don't want the probability to be more than or
less than 100\%
at some later time. What happens when the colliding waves have
real-valued wavefunctions $\Psi$? Now consider the sketches of complex-valued wave pulses
shown in the bottom panel of the figure as they are getting ready to collide.
\end{dq}

<% marg(30) %>
<%
  fig(
    'superposition-cancellation',
    ''
  )
%>
<% end_marg %>

\begin{dq}
The figure shows a skateboarder tipping over into a swimming pool with zero
initial kinetic energy. There is no friction, the corners are smooth enough to allow the
skater to pass over the smoothly, and the vertical distances are small
enough so that negligible time is required for the vertical parts of the motion.
The pool is divided into a deep end and a shallow end. Their widths are equal.
The deep end is four times deeper. (1) Classically, compare the skater's velocity in
the left and right regions, and infer the probability of finding the skater in either of the
two halves if an observer peeks at a random moment.
(2) Quantum-mechanically, this could be a one-dimensional model of an electron shared between two atoms
in a diatomic molecule. Compare the electron's kinetic energies, momenta, and wavelengths in the
two sides. For simplicity, let's assume that there is no tunneling into the classically forbidden
regions. What is the simplest standing-wave pattern that you can draw, and what are the probabilities
of finding the electron in one side or the other? Does this obey the correspondence principle?
\end{dq}

<%
  fig(
    'quantum-pool-skater',
    '',
    {                                 
      'width'=>'fullpage'
    }
  )
%>

 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % \fullpagewidthfignocaption{hwavefnlowres} 
\vfill\pagebreak[4]

<%
  fig(
    'hwavefnlowres',
    '',
    {
      'width'=>'wide',
      'anonymous'=>true,
      'float'=>false
    }
  )
%>

<% end_sec() %>

<% end_sec %>

<% begin_sec("The atom",0,'atom') %>

You can learn a lot by taking a car engine apart, but you
will have learned a lot more if you can put it all back
together again and make it run. Half the job of reductionism
is to break nature down into its smallest parts and
understand the rules those parts obey. The second half is to
show how those parts go together, and that is our goal in
this chapter. We have seen how certain features of all atoms
can be explained on a generic basis in terms of the
properties of bound states, but this kind of argument
clearly cannot tell us any details of the behavior of an
atom or explain why one atom acts differently from another.

The biggest embarrassment for reductionists is that the job
of putting things back together job is usually much harder
than the taking them apart. Seventy years after the
fundamentals of atomic physics were solved, it is only
beginning to be possible to calculate accurately the
properties of atoms that have many electrons. Systems
consisting of many atoms are even harder. Supercomputer
manufacturers point to the folding of large \index{protein
molecules}protein molecules as a process whose calculation
is just barely feasible with their fastest machines. The
goal of this chapter is to give a gentle and visually
oriented guide to some of the simpler results about atoms.

<% begin_sec("Classifying states") %>

We'll focus our attention first on the simplest atom,
\index{hydrogen atom!classification of states}hydrogen, with
one proton and one electron. We know in advance a little of
what we should expect for the structure of this atom. Since
the electron is bound to the proton by electrical forces, it
should display a set of discrete energy states, each
corresponding to a certain standing wave pattern. We need to
understand what states there are and what their properties are.

What properties should we use to classify the states? The
most sensible approach is to used conserved quantities.
\index{hydrogen atom!energy in}Energy is one conserved
quantity, and we already know to expect each state to have a
specific energy. It turns out, however, that energy alone is
not sufficient. Different standing wave patterns of the atom
can have the same energy.

\index{hydrogen atom!momentum in}Momentum is also a
conserved quantity, but it is not particularly appropriate
for classifying the states of the electron in a hydrogen
atom. The reason is that the force between the electron and
the proton results in the continual exchange of momentum
between them. (Why wasn't this a problem for energy as well?
Kinetic energy and momentum are related by $K=p^2/2m$,
so the much more massive proton never has very much kinetic
energy. We are making an approximation by assuming all the
kinetic energy is in the electron, but it is quite a
good approximation.)

<% marg(0) %>
<%
  fig(
    'moat-rainbow',
    %q{1.~Eight wavelengths fit around this circle ($\ell=8$). This is a standing wave.
       2.~A traveling wave with $\ell=8$, depicted according to the color conventions
          defined in figure \figref{rainbow}, p.~\pageref{fig:rainbow}.
      }
  )
%>
<% end_marg %>

\index{hydrogen atom!angular momentum in}Angular momentum
does help with classification. There is no transfer of
angular momentum between the proton and the electron, since
the force between them is a center-to-center force,
producing no torque.

Like energy, angular momentum is quantized in quantum
physics. As an example, consider a quantum wave-particle
confined to a circle, like a wave in a circular moat
surrounding a castle. A sine wave in such a ``\index{angular
momentum!quantization of}\index{quantum moat}quantum moat''
cannot have any old wavelength, because an integer number of
wavelengths must fit around the circumference, $C$, of the
moat. The larger this integer is, the shorter the wavelength,
and a shorter wavelength relates to greater momentum and
angular momentum. Since this integer is related to angular
momentum, we use the symbol $\ell$ for it:
\begin{equation*}
                \lambda          =    C /   \ell
\end{equation*}
The angular momentum is 
\begin{equation*}
                L         =    rp\eqquad.  
\end{equation*}
Here, $r=C/2\pi $, and $p=h/\lambda=h\ell/C$, so
\begin{align*}
        L         &=    \frac{C}{2\pi}\cdot\frac{h\ell}{C}  \\
                         &=     \frac{h}{2\pi}\ell 
\end{align*}
In the example of the quantum moat, angular momentum is
quantized in units of $h/2\pi $. This makes $h/2\pi $ a
pretty important number, so we define the abbreviation
$\hbar=h/2\pi $. This symbol is read ``h-bar.''

In fact, this is a completely general fact in quantum
physics, not just a fact about the quantum moat:

\begin{important}[Quantization of angular momentum]
The angular momentum of a particle due to its motion through
space is quantized in units of $\hbar$.
\end{important}

<% self_check('angmompicture',<<-'SELF_CHECK'

What is the angular momentum of the wavefunction shown at
the beginning of the section?
  SELF_CHECK
  ) %>

<% begin_sec("Degeneracy",nil,'degeneracy') %>

Comparing the oversimplified figure \subfigref{moat-rainbow}{1} with the more realistic
depiction in \subfigref{moat-rainbow}{2} using complex numbers, we see that there is
a direction of rotation, which was drawn as counterclockwise in the figure. As in figures
\subfigref{rainbow}{2} and \subfigref{rainbow}{3} on p.~\pageref{fig:rainbow}, we could
have drawn the clockwise version by putting the rainbow colors in the opposite order, i.e.,
by letting the phase spin in the opposite direction in the complex plane. This feature
was hidden in \subfigref{moat-rainbow}{1}, where in order to get a depiction using real
numbers, we had to use a standing wave. A standing wave, however, can be constructed as
a superposition of two traveling waves, so the issue was still there, just hidden. We really
have \emph{two} quantum-mechanical states here, regardless of whether we use standing waves
or traveling waves. If we use standing waves, they are of the form $\sin 8\theta$ and $\cos 8\theta$,
while in terms of the traveling waves we have $e^{8i\theta}$ and $e^{-8i\theta}$. By Euler's
formula (sec.~\ref{subsec:euler-formula}, p.~\pageref{subsec:euler-formula}), either traveling
wave can be expressed as a superposition of the two standing waves, and vice versa. (Physically,
there are not four different states here but two. The situation is a bit like choosing a
Cartesian coordinate system in the plane, where we could choose one coordinate system $(x,y)$,
or some other coordinates system $(x',y')$ rotated with respect to the first one; but this does
not mean there are four coordinates needed to describe a plane.)

These two states are simplified models of states in an atom, so it's worth thinking about how we could
tell, for a real atom, whether the electron had angular momentum in one direction or the
other. One technique would be to look at absorption or emission spectra of thin gases,
as in example \ref{eg:spectra-of-thin-gases} on p.~\pageref{eg:spectra-of-thin-gases}.
But this only distinguishes states according to their energies, and since these two states
have the same kinetic energy, that would not necessarily help. In quantum mechanics, when we
have more than one state with the same energy, they are said to be \emph{degenerate}.\index{degeneracy}
In our example, the degeneracy of the $\ell=8$ state is 2. This degeneracy arises from the
symmetry of space, which does not distinguish one direction from another. Degeneracies often, but not
always, arise from symmetries. (Cf.~p.~\pageref{accidental-degeneracy}.)

If we wanted to distinguish these two degenerate states observationally, one way to do it
would be to ``lift'' the degeneracy by applying an external magnetic field.
Since an electron has an electric charge, it acts like a current loop, and
the two states behave like oppositely oriented magnetic dipoles with an additional
potential energy $-\vc{m}\cdot\vc{B}$, which lowers the energy of one state and raises the
energy of the other. The existence of the magnetic field breaks the symmetry, which was the
reason for the degeneracy.

<% end_sec('degeneracy') %>

<% end_sec() %>

<% begin_sec("Three dimensions") %>

\index{angular momentum!and the uncertainty principle}\index{angular
momentum!in three dimensions}

Our discussion of quantum-mechanical angular momentum has so far been limited to
rotation in a plane, for which we can simply
use positive and negative signs to indicate clockwise and
counterclockwise directions of rotation. A hydrogen atom,
however, is unavoidably three-dimensional.
The classical treatment of angular momentum in three-dimensions
has been presented in section
\ref{sec:amthreed}; in general, the angular momentum of a particle
is defined as the vector cross product $\vc{r}\times\vc{p}$.

There
is a basic problem here: the angular momentum of the electron in
a hydrogen atom depends on both its distance $\vc{r}$ from the proton
and its momentum $\vc{p}$, so in order to know its angular momentum
precisely it would seem we would need to know both its
position and its momentum simultaneously with good accuracy.
This, however,  seems forbidden by the Heisenberg
uncertainty principle.

Actually the uncertainty principle does place limits on what
can be known about a particle's angular momentum vector, but
it does not prevent us from knowing its magnitude as an
exact integer multiple of $\hbar$. The reason is that in three
dimensions, there are really three separate \index{Heisenberg
uncertainty principle!in three dimensions}\index{uncertainty
principle!in three dimensions}uncertainty principles:
\begin{align*}
        \Delta p_x \Delta x &\gtrsim h \\
        \Delta p_y \Delta y &\gtrsim h \\
        \Delta p_z \Delta z &\gtrsim h
\end{align*}
Now consider a particle, \figref{particle-a-m-examples}/1, that is moving along the $x$
axis at position $x$ and with momentum $p_x$. We may not be
able to know both $x$ and $p_x$ with unlimited accurately,
but we can still know the particle's angular momentum about
the origin exactly: it is zero, because the particle is
moving directly away from the origin.

<% marg(100) %>
<%
  fig(
    'particle-a-m-examples',
    %q{%
      Reconciling the uncertainty principle
      with the definition of angular momentum.
    }
  )
%>
<% end_marg %>

Suppose, on the other hand, a particle finds itself, \figref{particle-a-m-examples}/2, at
a position $x$ along the $x$ axis, and it is moving parallel
to the $y$ axis with momentum $p_y$. It has angular momentum
$xp_y$ about the $z$ axis, and again we can know its angular
momentum with unlimited accuracy, because the uncertainty
principle only relates $x$ to $p_x$ and $y$ to $p_y$. It does
not relate $x$ to $p_y$.

As shown by these examples, the uncertainty principle does
not restrict the accuracy of our knowledge of angular
momenta as severely as might be imagined. However, it does
prevent us from knowing all three components of an angular
momentum vector simultaneously. The most general statement
about this is the following theorem:

\begin{important}[The angular momentum vector in quantum physics]
The most that can be known about a (nonzero) orbital angular momentum vector
is its magnitude and one of its three vector components.
Both are quantized in units of $\hbar$.
\end{important}

<% marg() %>

<%
  fig(
    'spherical-harmonic',
    %q{%
      A wavefunction on the sphere with $|\vc{L}|=11\hbar$ and $L_z=8\hbar$, shown using the color conventions
          defined in figure \figref{rainbow}, p.~\pageref{fig:rainbow}.
    }
  )
%>
<% end_marg %>

To see why this is true, consider the example wavefunction shown in figure \figref{spherical-harmonic}.
This is the like the quantum moat of figure \figref{moat-rainbow}, p.~\pageref{fig:moat-rainbow},
but extended to one more dimension.
If we slice the sphere in any plane perpendicular to the $z$ axis, we get an 8-cycle circular rainbow exactly
like figure \figref{moat-rainbow}. This is required because $L_z=8\hbar$. But if we take a slice perpendicular
to some other axis, such as the $y$ axis, we don't get a circular rainbow as we would for a state with a
definite value of $L_y$. It is obviously not possible to get circular rainbows for slices perpendicular to more
than one axis.

For those with a taste for rigor, here is a
complete argument:\label{proof-lx-and-lz-incompatible}

\noindent Theorem: On the sphere, if a wavefunction has
definite values of both $L_z$ and $L_x$, then it is a wavefunction
that is constant everywhere, so $\vc{L}=0$.

\noindent Lemma 1: If the component of $\ell_A$ 
along a certain axis A has a definite value and is nonzero, then (a)
$\Psi=0$ at the poles, and (b) $\Psi$ is of the form $Ae^{i\ell_A\phi}$ on any circle in
a plane perpendicular to the axis. Part a holds because $\vc{L}=0$ if $r_\perp=0$.
For b, see p.~\pageref{fig:moat-rainbow}.

\noindent Lemma 2: If the component of $\vc{L}$ along a certain axis has a
definite value and is zero, then $\Psi$ is constant in any plane perpendicular to that axis.
This follows from lemma 1 in the case where $\ell_A=0$.

\noindent \emph{Case I: $\ell_z$ and $\ell_x$ are both nonzero.}
We have $\Psi=0$ at the poles along both the $x$ axis and
the $z$ axis. The $z$-axis pole is a point on the great circle
perpendicular to the $x$ axis, and vice versa, so applying 1b, $A=0$ and $\Psi$
vanishes on both of these great circles. But now if we apply 1b along
any slice perpendicular to either axis, we get $\Psi=0$ everywhere on
that slice, so $\Psi=0$ everywhere.

\noindent \emph{Case II: $\ell_z$ and $\ell_x$ are both zero.}
By lemma 2, $\Psi$ is a constant everywhere.

\noindent \emph{Case III: One component is zero and the other nonzero.}
Let $\ell_z$ be the one that is zero.
By 1a, $\Psi=0$ at the $x$-axis pole, so by 2, $\Psi=0$ on the great circle perpendicular to $z$.
But then 1b tells us that $\Psi=0$ everywhere.

<% end_sec() %>

<% begin_sec("Quantum numbers",4,'quantum-numbers') %>

<% begin_sec("Completeness",nil) %>

<%
  fig(
    'completeness-ylm',
    %q{%
      The three states inside the box are a complete set of quantum numbers for $\ell=1$.
      Other states with $\ell=1$, such as the one on the right, are not really new: they can
      be expressed as superpositions of the original three we chose.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

For a given $\ell$, consider the set of states with all the
possible values of the angular momentum's component along some fixed axis.
This set of states is \emph{complete}, meaning that they encompass all the possible states with this
$\ell$.

For example, figure \figref{completeness-ylm} shows wavefunctions with $\ell=1$ that are solutions of
the Schr\"odinger equation for a particle that is confined to the surface of a sphere. Although the
formulae for these wavefunctions are not particularly complicated,\footnote{They are 
$\Psi_{1,-1}=\sin\theta e^{-i\phi}$, $\Psi_{10}=\sqrt{2}\cos\theta$,
and $\Psi_{11}=\sin\theta e^{i\phi}$, where $\theta$ is the angle measured down from the $z$
axis, and $\phi$ is the angle running counterclockwise around the $z$ axis. These functions are
called spherical harmonics.\index{spherical harmonics}} they are not our main
focus here, so to help with getting a feel for the idea of completeness, I have simply
selected three points on the sphere at which to give numerical samples of the value of the wavefunction.
These are the top (where the sphere is intersected by the positive $z$ axis),
left ($x$), and front ($y$). (Although the wavefunctions are shown using the
color conventions defined in figure \figref{rainbow}, p.~\pageref{fig:rainbow},
these numerical samples should make the example understandable if you're looking at a black and white
copy of the book.)

Suppose we arbitrarily choose the $z$ axis as the one along which to quantize the component
of the angular momentum. With this choice, we have three possible values for $\ell_z$: $-1$, $0$,
and $1$. These three states are shown in the three boxes surrounded by the black rectangle.
This set of three states is complete. 

Consider, for example, the fourth state, shown on the
right outside the box. This state is clearly identifiable as a copy of the $\ell_z=0$ state,
rotated by 90 degrees counterclockwise, so it is the $\ell_x=0$ state. We might imagine that
this would be an entirely new prize to be added to our stamp collection. But it is actually
not a state that we didn't possess before. We can obtain it as the sum of the
$\ell_z=-1$ and $\ell_z=1$ states, divided by an appropriate normalization factor. Although
I'm avoiding making this example an exercise in manipulating formulae, it is easy to check
that the sum does work out properly at the three sample points.

<% end_sec() %>

<% begin_sec("Sets of compatible quantum numbers",nil) %>

Figure \figref{compatible-quantum-numbers} shows some examples in which we can completely describe
a wavefunction by giving a few numbers. These are referred to as ``quantum numbers.'' It is important
that the quantum numbers we use in describing a state be compatible. By analogy, ``Bond, James, 007''
would be a clear and consistent definition of the famous fictional spy, but in general this identification
scheme would not work, because although almost everyone has a first and last name, most people do not have
a license to kill with a corresponding double-oh number.

<%
  fig(
    'compatible-quantum-numbers',
    %q{Three examples of sets of compatible quantum numbers.},
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

The laser beam in the figure is a state described
according to its definite values $p_x$ and $y$, so we have the vanishing uncertainties $\Delta p_x=0$ and $\Delta y=0$.
Since the Heisenberg uncertainty principle doesn't talk about an $x$ momentum in relation to a $y$ position, this is OK.
If we had been in doubt about whether this violated the uncertainty principle, we would have been
reassured by our ability to draw the picture.

It is also possible to have \emph{incompatible} quantum numbers. The combination of $p_x$ with $x$ would be
an incompatible set of quantum numbers, because a state can't have a definite $p_x$ and also a definite $x$.
If we try to draw such a wave, we fail. $L_x$ and $L_z$ would also be an incompatible set. 

<% end_sec() %>

<% begin_sec("Complete and compatible sets of quantum numbers") %>

Let's summarize. Just as we expect everyone to have a first and last name,
we expect there to be a complete and compatible set of quantum numbers for any
given quantum-mechanical system. Completeness means that we have enough quantum
numbers to uniquely describe every possible state of the system, although we may
need to describe a state as a superposition, as with the state $\ell_x=0$ in
figure \figref{completeness-ylm} on p.~\pageref{fig:completeness-ylm}. Compatibility
means that when we specify a set of quantum numbers, we aren't making a set of
demands that can't be met. These ideas are revisited in a slightly fancier mathematical
way on p.~\pageref{qm-completeness-stated}.

<% end_sec() %>

<% end_sec('quantum-numbers') %>

<% begin_sec("The hydrogen atom") %>

 % This figure was also used at the beginning of the whole section, without caption 
 % or label. 

<%
  fig(
    'hwavefnlowres',
    %q{A cross-section of a hydrogen wavefunction.},
    {
      'width'=>'wide'
    }
  )
%>

\index{hydrogen atom}\index{hydrogen atom!quantum numbers}
Deriving all the wavefunctions of the states of the hydrogen
atom from first principles would be mathematically too
complex for this book. (The ground state is not too hard, and we
analyze it on p.~\pageref{subsubsec:exact-h-gs}.). But it's not hard to understand the
logic behind the wavefunctions in visual terms. Consider
the wavefunction from the beginning of the section, which is
reproduced in figure \figref{hwavefnlowres}.
 Although the graph looks three-dimensional,
it is really only a representation of the part of the
wavefunction lying within a two-dimensional plane. The third
(up-down) dimension of the plot represents the value of the
wavefunction at a given point, not the third dimension of
space. The plane chosen for the graph is the one perpendicular
to the angular momentum vector.

Each ring of peaks and valleys has eight wavelengths going
around in a circle, so this state has $L=8\hbar$, i.e., we label
it $\ell=8$. The wavelength is shorter near the center, and this
makes sense because when the electron is close to the
nucleus it has a lower electrical energy, a higher kinetic energy,
and a higher momentum.

Between each ring of peaks in this wavefunction is a nodal
circle, i.e., a circle on which the wavefunction is zero. The
full three-dimensional wavefunction has nodal spheres: a
series of nested spherical surfaces on which it is zero. The
number of radii at which nodes occur, including $r=\infty$,
is called $n$, and $n$ turns out to be closely related to
energy. The ground state has $n=1$ (a single node only at
$r=\infty$), and higher-energy states have higher $n$
values. There is a simple equation relating $n$ to energy,
which we will discuss in subsection \ref{subsec:hydrogen-energies}.

<% marg(40) %>
<%
  fig(
    'hydrogen-energy-levels',
    %q{%
      The energy of a state in the hydrogen 
      atom depends only on its $n$ quantum number.
    }
  )
%>
<% end_marg %>

The numbers $n$ and $\ell$, which identify the state, are called
its \index{quantum numbers}quantum numbers.
 A state of a
given $n$ and $\ell$ can be oriented in a variety of directions in
space. We might try to indicate the orientation using the
three quantum numbers
$\ell_x=L_x/\hbar$,
$\ell_y=L_y/\hbar$, and
$\ell_z=L_z/\hbar$.
But we have already seen
that it is impossible to know all three of these simultaneously.
To give the most complete possible description of a state,
we choose an arbitrary axis, say the $z$ axis, and label the
state according to $n$, $\ell$, and $\ell_z$.\footnote{See page
\pageref{quantumnotation} for a note about the two different
systems of notations that are used for quantum numbers.}

Angular momentum requires motion, and motion implies kinetic
energy. Thus it is not possible to have a given amount of
angular momentum without having a certain amount of kinetic
energy as well. Since energy relates to the $n$ quantum
number, this means that for a given $n$ value there will be
a maximum possible $\ell$. It turns out that this maximum
value of  equals $n-1$.

In general, we can list the possible combinations of
quantum numbers as follows:

\noindent\begin{tabular}{|l|}
\hline
  $n$ can equal 1, 2, 3, \ldots \\
  $\ell$ can range from 0 to $n-1$, in steps of 1\\
  $\ell_z$ can range from $-\ell$ to $\ell$, in steps of 1 \\
\hline
\end{tabular}

Applying these rules, we have the following list of states:

\noindent\begin{tabular}{|lll|l|}
\hline
$n=1$, & $\ell=0$, & $\ell_z=0$        & one state \\
$n=2$, & $\ell=0$, & $\ell_z=0$        & one state \\
$n=2$, & $\ell=1$, & $\ell_z=-1$, 0, or 1 &        three states\\
\ldots & & & \\
\hline
\end{tabular}

<% self_check('listingatomstates',<<-'SELF_CHECK'
Continue the list for $n=3$.
  SELF_CHECK
  ) %>

Because the energy only depends on $n$, we have degeneracies.
For example, the $n=2$ energy level is 4-fold degenerate (and in fact
this degeneracy will be doubled to 8 when we take into account the intrinsic
spin of the electron, sec.~\ref{subsec:electron-spin}, p.~\pageref{subsec:electron-spin}).
The degeneracy of the different $\ell_z$ states follows from symmetry, as in our
original example of degeneracy on p.~\pageref{subsubsec:degeneracy}, and is therefore
exact. The degeneracy with respect to different values of $\ell$ for the same $n$
is not at all obvious, and is in fact not exact when effects such as relativity
are taken into account. We refer to this as an ``accidental'' degeneracy.\label{accidental-degeneracy}
The very high level of degeneracy in the hydrogen atom means that when you observe it the hydrogen spectrum
in your lab course, there is a great deal of structure that is effectively
hidden from you. Historically, physicists were fooled by the apparent simplicity of the
spectrum, and more than 70 years passed between the measurement of the
spectrum and the time when the degeneracies were fully recognized and understood.

% Angstron 1853, Uhlenbeck-Goudsmit 1925

Figure \figref{hydrogen-three-states} on page \pageref{fig:hydrogen-three-states}
shows the lowest-energy states
of the hydrogen atom. The left-hand column of graphs
displays the wavefunctions in the $x-y$ plane, and the
right-hand column shows the probability distribution in a
three-dimensional representation.

<%
  fig(
    'hydrogen-three-states',
    %q{%
      The three states of the hydrogen atom having
      the lowest energies.
    },
    {
      'width'=>'wide'
    }
  )
%>

\startdqs

\begin{dq}
The quantum number $n$ is defined as the number of radii
at which the wavefunction is zero, including $r=\infty$.
Relate this to the features of figure \figref{hydrogen-three-states}.
\end{dq}

\begin{dq}
Based on the definition of $n$, why can't there be any
such thing as an $n=0$ state?
\end{dq}

\begin{dq}
Relate the features of the wavefunction plots in figure \figref{hydrogen-three-states}
to the corresponding features of the probability distribution pictures.
\end{dq}

\begin{dq}
How can you tell from the wavefunction plots in figure \figref{hydrogen-three-states}
which ones have which angular momenta?
\end{dq}

\begin{dq}
Criticize the following incorrect statement: ``The $\ell=8$
wavefunction in figure \figref{hwavefnlowres} has a shorter wavelength
in the center because in the center the electron is in a
higher energy level.''
\end{dq}

\begin{dq}
Discuss the implications of the fact that the probability
cloud in of the $n=2$, $\ell=1$ state is split into two parts.
\end{dq}

<% end_sec() %>

<% begin_sec("Energies of states in hydrogen",nil,'hydrogen-energies') %>

__incl(text/h-energy)

<% end_sec() %>

<% begin_sec("Electron spin",nil,'electron-spin') %>
\index{spin}

It's disconcerting to the novice ping-pong player to
encounter for the first time a more skilled player who can
put spin on the ball. Even though you can't see that the
ball is spinning, you can tell something is going on by the
way it interacts with other objects in its environment. In
the same way, we can tell from the way electrons interact
with other things that they have an intrinsic spin of their
own. Experiments show that even when an \index{electron!spin
of}\index{spin!of electron}electron is not moving through
space, it still has angular momentum amounting to $\hbar/2$.
An important historical experiment of this type, the Stern-Gerlach
experiment, is described in detail in section \ref{sec:stern-gerlach}.

<% marg(m4_ifelse(__sn,1,[:30:],[:0:])) %>

<%
  fig(
    'spin-vs-orbital',
    %q{%
      The top has angular momentum both
       because of the motion of its center of mass
       through space and due to its internal rotation. Electron
       spin is roughly analogous to the intrinsic spin of the top.
    }
  )
%>
<% end_marg %>

This may seem paradoxical because the quantum moat, for
instance, gave only angular momenta that were integer
multiples of $\hbar$, not half-units, and I claimed that angular
momentum was always quantized in units of $\hbar$, not just in the
case of the quantum moat. That whole discussion, however,
assumed that the angular momentum would come from the motion
of a particle through space. The  $\hbar/2$ angular momentum of the
electron is simply a property of the particle, like its
charge or its mass. It has nothing to do with whether the
electron is moving or not, and it does not come from any
internal motion within the electron. Nobody has ever
succeeded in finding any internal structure inside the
electron, and even if there was internal structure, it would
be mathematically impossible for it to result in a half-unit
of angular momentum.

We simply have to accept this $\hbar/2$ angular momentum, called the
``spin'' of the electron --- Mother Nature rubs our noses in it as
an observed fact.
\index{spin!proton's}\index{proton!spin of}
\index{spin!neutron's}\index{neutron!spin of}
\index{spin!photon's}\index{photon!spin of}
Protons and neutrons have
the same $\hbar/2$ spin, while photons have an intrinsic spin of $\hbar$.
In general, half-integer spins are typical of material particles.
Integral values are found for the particles that carry forces: photons,
which embody the electric and magnetic fields of force, as well as the
more exotic messengers of the nuclear and gravitational forces.
The photon is particularly important: it has spin 1.

As was the case with ordinary angular momentum, we can
describe spin angular momentum in terms of its magnitude,
and its component along a given axis.
We write $s$ and $s_z$ for these quantities, expressed in
units of $\hbar$, so an
electron has $s=1/2$ and $s_z=+1/2$ or $-1/2$.

__incl(text/spin-examples)

<% begin_sec("States in hydrogen, with spin",nil,'states-in-h-with-spin') %>

Taking electron spin into account, we need a total of four
quantum numbers to label a state of an electron in the
hydrogen atom: $n$, $\ell$, $\ell_z$, and $s_z$. (We omit $s$ because it
always has the same value.) The symbols $\ell$  and $\ell_z$  include only
the angular momentum the electron has because it is moving
through space, not its spin angular momentum. The availability
of two possible spin states of the electron leads to a
doubling of the numbers of states:

\noindent\begin{tabular}{|llll|l|}
\hline
  $n=1$, & $\ell=0$, & $\ell_z=0$, & $s_z=+1/2$ or $-1/2$ &        two states \\
  $n=2$, & $\ell=0$, & $\ell_z=0$, & $s_z=+1/2$ or $-1/2$ &        two states\\
  $n=2$, & $\ell=1$, & $\ell_z=-1$, 0, or 1, & $s_z=+1/2$ or $-1/2$ &        six states\\
        \ldots & & & &\\
\hline
\end{tabular}

<% end_sec('states-in-h-with-spin') %>


<% begin_sec("A note about notation") %>
\label{quantumnotation}
There are unfortunately two inconsistent systems of notation
for the quantum numbers we've been discussing. The notation
I've been using is the one that is used in nuclear
physics, but there is a different one that is used in
atomic physics.

\begin{tabular}{|l|l|}
  \hline
  nuclear physics        & atomic physics \\
  \hline
  $n$                        & same \\
  $\ell$                & same \\
  $\ell_x$                & no notation \\
  $\ell_y$                & no notation \\
  $\ell_z$                & $m$ \\
  $s=1/2$                & no notation (sometimes $\sigma$)\\
  $s_x$                        & no notation \\
  $s_y$                        & no notation \\
  $s_z$                        & $s$ \\
  \hline
\end{tabular}

\noindent{}The nuclear physics notation is more logical (not giving
special status to the $z$ axis) and more memorable ($\ell_z$
rather than the obscure $m$), which is why I use it consistently
in this book, even though nearly all the applications we'll
consider are atomic ones.

We are further encumbered with
the following historically derived
letter labels, which deserve to be
eliminated in favor of the simpler numerical ones:

\begin{tabular}{|cccc|}
\hline
$\ell=0$        & $\ell=1$ & $\ell=2$ & $\ell=3$\\
s               & p        & d        & f \\
\hline
\end{tabular}

\begin{tabular}{|ccccccc|}
\hline
$n=1$   & $n=2$ & $n=3$ & $n=4$ & $n=5$ & $n=6$ & $n=7$ \\
K & L & M & N & O & P & Q\\
\hline
\end{tabular}

\noindent{}The spdf labels are used in both nuclear\footnote{After f,
the series continues in alphabetical order. In nuclei
that are spinning rapidly enough that they are almost breaking apart,
individual protons and neutrons can
be stirred up to $\ell$ values as high as 7, which is j.}
 and atomic physics, while
the KLMNOPQ letters are used only to refer to states of electrons.

And finally, there is a piece of notation that is good and useful,
but which I simply haven't mentioned yet. The vector $\vc{j}=\vc{\ell}+\vc{s}$
stands for the total angular momentum of a particle in units of $\hbar$, including
both orbital and spin parts. This quantum number turns out to be
very useful in nuclear physics, because nuclear forces tend to
exchange orbital and spin angular momentum, so a given energy level
often contains a mixture of $\ell$ and $s$ values, while remaining
fairly pure in terms of $j$.

<% end_sec() %>

<% end_sec() %>

<% begin_sec("Atoms with more than one electron") %>

\index{atoms!with many electrons}
What about other atoms besides hydrogen? It would seem that
things would get much more complex with the addition of a
second electron. A hydrogen atom only has one particle that
moves around much, since the nucleus is so heavy and nearly
immobile. \index{atoms!helium}\index{helium}Helium, with
two, would be a mess. Instead of a wavefunction whose square
tells us the probability of finding a single electron at any
given location in space, a helium atom would need to have a
wavefunction whose square would tell us the probability of
finding two electrons at any given combination of points.
Ouch! In addition, we would have the extra complication of
the electrical interaction between the two electrons, rather
than being able to imagine everything in terms of an
electron moving in a static field of force created
by the nucleus alone.

Despite all this, it turns out that we can get a surprisingly
good description of many-electron atoms simply by assuming
the electrons can occupy the same standing-wave patterns
that exist in a hydrogen atom. The ground state of helium,
for example, would have both electrons in states that are
very similar to the $n=1$ states of hydrogen.  The
second-lowest-energy state of helium would have one electron
in an $n=1$ state, and the other in an $n=2$ states. The
relatively complex spectra of elements heavier than hydrogen
can be understood as arising from the great number of
possible combinations of states for the electrons.

A surprising thing happens, however, with \index{atoms!lithium}lithium,
the three-electron atom. We would expect the ground state of
this atom to be one in which all three electrons settle down
into $n=1$ states. What really happens is that two electrons
go into $n=1$ states, but the third stays up in an $n=2$
state. This is a consequence of a new principle of physics:
\begin{important}[The Pauli Exclusion Principle]
\index{exclusion principle}\index{Pauli exclusion principle}\label{exclusion-principle-nonrigorous}
Two electrons can never occupy the same state.\footnote{This principle is stated with more mathematical precision on p.~\pageref{exclusion-principle-rigorous}.}
\end{important}

There are two $n=1$ states, one with $s_z=+1/2$ and one with
$s_z=-1/2$, but there is no third $n=1$ state for lithium's
third electron to occupy, so it is forced to go into an $n=2$ state.

It can be proved mathematically that the Pauli exclusion
principle applies to any type of particle that has
half-integer spin. Thus two neutrons can never occupy the
same state, and likewise for two protons. Photons, however,
are immune to the exclusion principle because their spin is an integer.

<% begin_sec("Deriving the periodic table",nil,'deriving-periodic-table') %>
\index{periodic table}
We can now account for the structure of the periodic table,
which seemed so mysterious even to its inventor Mendeleev.
The first row consists of atoms with electrons only
in the $n=1$ states:

\noindent\begin{tabular}{rp{98mm}}
                H &        1 electron in an $n = 1$ state  \\
                He &        2 electrons in the two $n = 1$ states  
\end{tabular}

The next row is built by filling the $n=2$ energy levels:

\noindent\begin{tabular}{rp{98mm}}
        Li &        2 electrons in $n=1$ states, 1 electron in an $n=2$ state\\
        Be &        2 electrons in $n=1$ states, 2 electrons in $n=2$ states\\
        \ldots\\
        O &        2 electrons in $n=1$ states, 6 electrons in $n=2$ states\\
        F &        2 electrons in $n=1$ states, 7 electrons in $n=2$ states\\
        Ne &        2 electrons in $n=1$ states, 8 electrons in $n=2$ states
\end{tabular}

In the third row we start in on the $n=3$ levels:

\noindent\begin{tabular}{rp{98mm}}
        Na        & 2 electrons in $n=1$ states, 8 electrons in $n=2$ 
                        states, 1 electron in an $n=3$ state \\
        ...
\end{tabular}

<% marg(0) %>
<%
  fig(
    'small-periodic-table',
    %q{The beginning of the periodic table.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'hindenburg',
    %q{Hydrogen is highly reactive.}
  )
%>
<% end_marg %>

We can now see a logical link between the filling of the
energy levels and the structure of the periodic table.
Column 0, for example, consists of atoms with the right
number of electrons to fill all the available states up to a
certain value of $n$. Column I contains atoms like lithium
that have just one electron more than that.

\label{ionicbonds}
This shows that the columns relate to the filling of energy
levels, but why does that have anything to do with
chemistry? Why, for example, are the elements in columns I
and VII dangerously reactive? Consider, for example, the
element \index{atoms!sodium}\index{sodium}sodium (Na), which
is so reactive that it may burst into flames when exposed to
air. The electron in the $n=3$ state has an unusually high
energy. If we let a sodium atom come in contact with an
oxygen atom, energy can be released by transferring the
$n=3$ electron from the sodium to one of the vacant
lower-energy $n=2$ states in the oxygen. This energy is
transformed into heat. Any atom in column I is highly
reactive for the same reason: it can release energy by
giving away the electron that has an unusually high energy.

Column VII is spectacularly reactive for the opposite
reason: these atoms have a single vacancy in a low-energy
state, so energy is released when these atoms steal an
electron from another atom.

It might seem as though these arguments would only explain
reactions of atoms that are in different rows of the
periodic table, because only in these reactions can a
transferred electron move from a higher-$n$ state to a
lower-$n$ state. This is incorrect. An $n=2$ electron in
fluorine (F), for example, would have a different energy
than an $n=2$ electron in lithium (Li), due to the different
number of protons and electrons with which it is interacting.
Roughly speaking, the $n=2$ electron in fluorine is more
tightly bound (lower in energy) because of the larger number
of protons attracting it. The effect of the increased number
of attracting protons is only partly counteracted by the
increase in the number of repelling electrons, because the
forces exerted on an electron by the other electrons are in
many different directions and cancel out partially.

 % ============================================================= 
 % ============================================================= 
 % ============================================================= 
 % ============================================================= 
 % ============================================================= 

<% end_sec() %>

<% end_sec() %>

<% end_sec() %>

<% begin_hw_sec %>

<% begin_hw('wholelife') %>__incl(hw/wholelife)<% end_hw() %>

<% begin_hw('snakeeyes') %>__incl(hw/snakeeyes)<% end_hw() %>

<% begin_hw('deleted') %>This problem has been deleted.<% end_hw() %>

<% begin_hw('deleted') %>This problem has been deleted.<% end_hw() %>

\enlargethispage{\baselineskip}

<% begin_hw('heightdistribution') %>__incl(hw/heightdistribution)<% end_hw() %>

<% marg(15) %>
<%
  fig(
    'hw-unknownisotopes',
    %q{Problem \ref{hw:unknownisotopes}.}
  )
%>
<% end_marg %>

<% begin_hw('unknownisotopes') %>__incl(hw/unknownisotopes)<% end_hw() %>

<% begin_hw('craps') %>__incl(hw/craps)<% end_hw() %>

\enlargethispage{\baselineskip}

<% begin_hw('blindfold-target') %>__incl(hw/blindfold-target)<% end_hw() %>

 % 

<% begin_hw('truncated-half-life') %>__incl(hw/truncated-half-life)<% end_hw() %>

<% begin_hw('maxwellian') %>__incl(hw/maxwellian)<% end_hw() %>

\pagebreak

<% begin_hw('lava') %>__incl(hw/lava)<% end_hw() %>

 % 

<% begin_hw('mirrorphotons') %>__incl(hw/mirrorphotons)<% end_hw() %>

 % 

<% begin_hw('pelightsensor') %>__incl(hw/pelightsensor)<% end_hw() %>

 %  

\enlargethispage{\baselineskip}

<% begin_hw('cancer') %>__incl(hw/cancer)<% end_hw() %>

\pagebreak

<% marg(0) %>
<%
  fig(
    'hw-compare-four-photons',
    %q{Problem \ref{hw:compare-four-photons}.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'wavespeedingup',
    %q{Problem \ref{hw:wavespeedingup}.}
  )
%>
<% end_marg %>

<% begin_hw('compare-four-photons') %>__incl(hw/compare-four-photons)<% end_hw() %>

\enlargethispage{\baselineskip}

<% begin_hw('wavespeedingup') %>__incl(hw/wavespeedingup)<% end_hw() %>

 % 

<% begin_hw('projector') %>__incl(hw/projector)<% end_hw() %>

 % 

<% begin_hw('pe') %>__incl(hw/pe)<% end_hw() %>

\pagebreak

<% begin_hw('tv') %>__incl(hw/tv)<% end_hw() %>

 % 

<% begin_hw('lead') %>__incl(hw/lead)<% end_hw() %>

 % 

<% begin_hw('particleinabox') %>__incl(hw/particleinabox)<% end_hw() %>

 % 

<% begin_hw('chip-qm') %>__incl(hw/chip-qm)<% end_hw() %>

 % 

<% begin_hw('quantumho') %>__incl(hw/quantumho)<% end_hw() %>

 % 

<% begin_hw('hydrogen-scale') %>__incl(hw/hydrogen-scale)<% end_hw() %>

 % 

<% marg() %>

<%
  fig(
    'hw-h-transitions',
    %q{Problem \ref{hw:hydrogenlevels}.}
  )
%>
<% end_marg %>

<% begin_hw('hydrogenlevels') %>__incl(hw/hydrogenlevels)<% end_hw() %>

<% begin_hw('hydrogenphoton') %>__incl(hw/hydrogenphoton)<% end_hw() %>

<% begin_hw('basketball') %>__incl(hw/basketball)<% end_hw() %>

<% begin_hw('hydrogennonrelativistic') %>__incl(hw/hydrogennonrelativistic)<% end_hw() %>

<% begin_hw('energysum') %>__incl(hw/energysum)<% end_hw() %>

<% begin_hw('electroninproton') %>__incl(hw/electroninproton)<% end_hw() %>

 % 

<% begin_hw('hydrogenlike') %>__incl(hw/hydrogenlike)<% end_hw() %>

 % 

<% begin_hw('muonic-spectrum') %>__incl(hw/muonic-spectrum)<% end_hw() %>

\pagebreak

<% begin_hw('compton') %>__incl(hw/compton)<% end_hw() %>

 % 

<% begin_hw('compton-any-angle') %>__incl(hw/compton-any-angle)<% end_hw() %>

 % 

<% begin_hw('wkb',2) %>__incl(hw/wkb)<% end_hw() %>

<% begin_hw('h-atom-normalization') %>__incl(hw/h-atom-normalization)<% end_hw() %>

<% begin_hw('three-d-forbidden') %>__incl(hw/three-d-forbidden)<% end_hw() %>

<% begin_hw('three-d-box') %>__incl(hw/three-d-box)<% end_hw() %>

\pagebreak

<% begin_hw('tritium-decay') %>__incl(hw/tritium-decay)<% end_hw() %>

<% begin_hw('photon-mass') %>__incl(hw/photon-mass)<% end_hw() %>

<% begin_hw('hydrogen-ratio') %>__incl(hw/hydrogen-ratio)<% end_hw() %>

<% begin_hw('compare-photons') %>__incl(hw/compare-photons)<% end_hw() %>

\pagebreak

<% begin_hw('generalize-hydrogen') %>__incl(hw/generalize-hydrogen)<% end_hw() %>

<% marg(150) %>
<%
  fig(
    'hw-generalize-hydrogen',
    %q{Problem \ref{hw:generalize-hydrogen}.}
  )
%>
<% end_marg %>

<% begin_hw('neutron-as-proton-plus-electron') %>__incl(hw/neutron-as-proton-plus-electron)<% end_hw() %>

\pagebreak

<% begin_hw('easy-electron-normalization') %>__incl(hw/easy-electron-normalization)<% end_hw() %>

<% begin_hw('easy-partials') %>__incl(hw/easy-partials)<% end_hw() %>

<% begin_hw('am-radio-photon-density') %>__incl(hw/am-radio-photon-density)<% end_hw() %>

<% begin_hw('square-wavefunction') %>__incl(hw/square-wavefunction)<% end_hw() %>

<% begin_hw('bogus-laplacian') %>__incl(hw/bogus-laplacian)<% end_hw() %>

\pagebreak

<% begin_hw('easy-laplacian') %>__incl(hw/easy-laplacian)<% end_hw() %>

<% end_hw_sec %>

\begin{exsection}
 % ==================================================================== 
 % ==================================================================== 
 % ====================================================================  

\extitle{A}{Quantum Versus Classical Randomness}

\noindent 1. Imagine the classical version of the particle in a one-dimensional box. Suppose you insert the particle in the
 box and give it a known, predetermined energy, but a random initial position and a
 random direction of motion. You then pick a random later moment in time to see where it is.
 Sketch the resulting probability distribution by shading on top of a line segment. Does the probability distribution depend on energy?

\noindent 2. Do similar sketches for the first few energy levels of the quantum mechanical particle in a box, and compare with 1.  

\noindent 3. Do the same thing as in 1, but for a classical hydrogen atom in two dimensions, which acts just like a miniature solar
 system. Assume you're always starting out with the same fixed values of energy and angular momentum, but a position
 and direction of motion that are otherwise random. Do this for $L=0$, and compare with a real $L=0$ probability distribution for the hydrogen atom.

\noindent 4. Repeat 3 for a nonzero value of $L$, say L=$\hbar$.

\noindent 5. Summarize: Are the classical probability distributions 
accurate? What qualitative features are possessed by the classical diagrams but not by the quantum mechanical ones, or vice-versa?

\pagebreak

\newcommand{\abox}{\fbox{\rule{0.8cm}{0pt}\rule[-0.5ex]{0pt}{2.7ex}}\:}
\newcommand{\aboxfilledin}[1]{\fbox{#1\rule[-0.5ex]{0pt}{2.7ex}}\:}

\extitle{B}{Choice of quantum numbers}

We could choose to identify a human by their first and last names, or by their social security number.
We're free to choose any set of labels, as long as they're compatible.

\noindent 1. Warm-up, nothing to do with quantum mechanics:

{\centering \anonymousinlinefig{ex-rotate-90-degrees} \par}

Express the $x',y'$ coordinates in terms of the $x,y$ coordinates:
\begin{align*}
  x' &= \abox x + \abox y \\
  y' &= \abox x + \abox y
\end{align*}

\noindent 2. Consider these four $\ell=1$ wavefunctions on the ``quantum moat:''
\begin{align*}
  \Psi_\circlearrowleft = \abox e^{i\theta} \qquad\qquad & \Psi_\circlearrowright = \abox e^{-i\theta} \\
  \Psi_\text{s}  = \abox\sin\theta          \qquad\qquad & \Psi_\text{c} = \abox\cos\theta
\end{align*}
Determine the normalization factor for your group's wave.

\noindent 3. We now want to discuss the standing waves in terms of the traveling waves:
\begin{align*}
  \Psi_\text{c} &= \abox\Psi_\circlearrowleft + \abox\Psi_\circlearrowright \\
  \Psi_\text{s} &= \abox\Psi_\circlearrowleft + \abox\Psi_\circlearrowright .
\end{align*}
Determine the coefficients assigned to your group, and check your equation at the
value of $\theta$ assigned to your group: $\theta=0$, $\pi/2$, $\pi$, or $3\pi/2$.

\noindent 4. An electron is initially in state $\Psi_\text{c}$, and Jane then measures
its angular momentum. Discuss what happens to the electron's wavefunction and to Jane's,
as in sec.~\ref{subsec:nonlocality-and-entanglement}, p.~\pageref{schrodingers-cat}, on
entanglement.

\noindent 5. Discuss the probability interpretation and normalization.

\pagebreak

\extitle{C}{Rotation around different axes}

This exercise refers to the example at the beginning of section 
\ref{subsec:quantum-numbers} on p.~\pageref{subsec:quantum-numbers}, which analyzes the
figure below:

{\centering \anonymousinlinefig{../../../share/quantum/figs/completeness-ylm-ex} \par}

To simplify the writing:
\begin{itemize}
\item $\Psi_{-1}$ means the state with $\ell_z=-1$, $\Psi_0$ has $\ell_z=0$, etc.
\item States with definite values of $\ell_x$ are notated as $\Psi_{\ell_x=0}$, etc.
\end{itemize}

\noindent 1. The wavefunctions are shown 
with values of the wavefunction written at the north pole and
at two points on the equator. Fill in the south poles.
Why do the results make sense physically for the $\ell_z=1$ and $-1$ wavefunctions?

\noindent 2.
By rotating the pictures 90 degrees counterclockwise, we can make states of definite $\ell_x$.
We now want to express the states of definite $\ell_x$ in terms of the states of definite $\ell_z$.

\anonymousinlinefig{../../../share/quantum/figs/completeness-ylm-ex-3a} \qquad
    $\Psi_{\ell_x=0} =  \aboxfilledin{$1/\sqrt2$}\Psi_{-1}
                       +\aboxfilledin{$0\quad$}\Psi_0
                       +\aboxfilledin{$1/\sqrt2$}\Psi_1 \qquad \text{Done on p.~\pageref{subsec:quantum-numbers}.} $

\noindent\anonymousinlinefig{../../../share/quantum/figs/completeness-ylm-ex-3b} \qquad
    $\Psi_{\ell_x=1} =  \abox\Psi_{-1}
                       +\abox\Psi_0
                       +\abox\Psi_1 \qquad \text{Demonstrated by the instructor.} $

\noindent\anonymousinlinefig{../../../share/quantum/figs/completeness-ylm-ex-3c} \qquad
    $\Psi_{\ell_x=-1} =  \abox\Psi_{-1}
                       +\abox\Psi_0
                       +\abox\Psi_1 \qquad \text{Done by the students.}$

\noindent 3.
Fred takes a molecule known to have $\ell=1$, and measures its $\ell_x$. (This can be
done by passing it through a magnetic field, as described in more detail in section
\ref{sec:stern-gerlach}.) If the molecule is not prepared in any particular orientation,
then the result is random, and can be $\ell_x=-1$, $0$, or $1$. (The probabilities are
all $1/3$, although this is not obvious.) Suppose he measures $\ell_x=0$, so that
\emph{after} measurement, he is sure that the wavefunction is $\Psi_{\ell_x=0}$.
(Fred may now be superimposed with other versions of himself who saw $\ell_x=-1$ or
1, but we stop keeping track of them now.)

Now suppose that Fred follows up with a second measurement, on the same molecule,
but this time he orients the magnetic field so that he's measuring $\ell_z$.
What are the probabilities of the three possible results? Check normalization.

\pagebreak

\extitle{D}{The Einstein-Podolsky-Rosen paradox}

A nucleus having zero angular momentum undergoes symmetric fission into two
fragments, each with $\ell=1$. By conservation of momentum, they fly off back
to back, and by conservation of angular momentum their angular momenta are also
opposite. Let's say that except for this correlation, the two angular momentum vectors
are randomly oriented.

\noindent 1. Warm-up: Suppose Alice measures the $\ell_x$ of particle A,
and Bob measures $\ell_x$ of fragment B. Make a table of the probabilities
of the outcomes.

\begin{tabular}{ll|lll}
 & & \multicolumn{3}{c}{particle B} \\
 & & $\ell_x=-1$ & $\ell_x=0$ & $\ell_x=1$ \\
\hline
           & $\ell_x=-1$ \\
particle A & $\ell_x=0$ \\
           & $\ell_x=1$ 
\end{tabular}

\noindent 2. 
In a 1935 paper that ended up being one of the most frequently cited physics papers of all time,
Einstein and his collaborators considered a scenario similar to the following.
Suppose now that Alice measures $\ell_x$, but Bob measures $\ell_z$. It shouldn't
matter who goes first, but let's say that Alice does. Using the results of exercise C,
compute the probabilities in the row assigned to your group. Take into account the factor
of $1/3$, because in this table, as in the first one, we're talking about the probability
of a certain result for A \emph{and} a certain result for B.

\begin{tabular}{l|lll}
 & \multicolumn{3}{c}{Bob's probabilities} \\
 & $\ell_z=-1$ & $\ell_z=0$ & $\ell_z=1$ \\
\hline
1/3 of the time, Alice gets $\ell_x=-1\:$ $\implies$  \\
1/3 of the time, Alice gets $\ell_x=0$ \quad $\implies$  \\
1/3 of the time, Alice gets $\ell_x=1$ \quad $\implies$  \\
\hline
total probabilities for Bob
\end{tabular}

\noindent 3. Can Alice send information to Bob by deciding whether or not to measure
her particle's $\ell_x$?

\end{exsection}

<% end_chapter() %>
